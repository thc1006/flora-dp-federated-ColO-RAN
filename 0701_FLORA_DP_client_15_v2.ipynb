{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thc1006/flora-dp-federated-ColO-RAN/blob/main/0701_FLORA_DP_client_15_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWyXJ3F_H9G-",
        "outputId": "547f6ba2-ae5e-4bd6-9120-058f8b8ddc0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m254.4/254.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… Cell 1: ç’°å¢ƒèˆ‡å‡½å¼åº«æº–å‚™å°±ç·’ã€‚\n",
            "PyTorch/Opacus ç‰ˆæœ¬: 2.6.0+cu124 / 1.5.4\n",
            "CUDA æ˜¯å¦å¯ç”¨: True\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 1: ç’°å¢ƒè¨­å®šèˆ‡å‡½å¼åº«åŒ¯å…¥ï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
        "!pip install --upgrade opacus -q\n",
        "\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np, pandas as pd, random, copy, json, os, time, warnings, math, re, contextlib\n",
        "from collections import deque\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from dataclasses import dataclass, asdict\n",
        "from sklearn.cluster import KMeans\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.validators import ModuleValidator\n",
        "from opacus.data_loader import DPDataLoader\n",
        "\n",
        "# --- ç’°å¢ƒè¨­å®š ---\n",
        "try: torch._dynamo.disable()\n",
        "except Exception: pass\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\".*overflow encountered.*\", category=RuntimeWarning)\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "print(\"âœ… Cell 1: ç’°å¢ƒèˆ‡å‡½å¼åº«æº–å‚™å°±ç·’ã€‚\")\n",
        "import opacus\n",
        "print(f\"PyTorch/Opacus ç‰ˆæœ¬: {torch.__version__} / {opacus.__version__}\")\n",
        "print(f\"CUDA æ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp11BP-2IANH",
        "outputId": "dd9ce0ac-28a9-4981-b339-85004878c0f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 2: TrainingConfigï¼ˆè™›æ“¬å®¢æˆ¶ç«¯å„ªåŒ–ç‰ˆï¼‰å®šç¾©å®Œæˆã€‚\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 2: ğŸ“ å¯¦é©—åƒæ•¸è¨­å®šï¼ˆè™›æ“¬å®¢æˆ¶ç«¯å„ªåŒ–ç‰ˆï¼‰\n",
        "from dataclasses import dataclass, field  # ã€ä¿®æ­£ã€‘æ·»åŠ  field å°å…¥\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    experiment_name: str\n",
        "    output_dir: str\n",
        "    mode: str = \"ClusteredFL\"\n",
        "    random_seed: int = 42\n",
        "    comm_rounds: int = 20\n",
        "\n",
        "    # ã€ä¿®æ”¹5ã€‘è™›æ“¬å®¢æˆ¶ç«¯æ“´å¢é…ç½®\n",
        "    base_client_pairs: tuple = ((1, 2), (3, 7), (5, 6))  # åŸå§‹3å°åŸºç«™\n",
        "    virtual_expansion_factor: int = 3  # æ¯å€‹åŸºç«™çµ„åˆç”Ÿæˆ3å€‹è™›æ“¬å®¢æˆ¶ç«¯\n",
        "    num_virtual_clients: int = 9       # 3Ã—3 = 9å€‹è™›æ“¬å®¢æˆ¶ç«¯\n",
        "    num_real_clients: int = 3          # åŸå§‹çœŸå¯¦å®¢æˆ¶ç«¯\n",
        "    total_clients: int = 12            # ç¸½è¨ˆ12å€‹å®¢æˆ¶ç«¯\n",
        "    num_clients: int = 12              # å¯¦éš›åƒèˆ‡çš„å®¢æˆ¶ç«¯æ•¸é‡\n",
        "    num_clients_to_select: int = 8     # æ¯è¼ªé¸æ“‡åƒèˆ‡çš„å®¢æˆ¶ç«¯æ•¸é‡\n",
        "\n",
        "    # è™›æ“¬å®¢æˆ¶ç«¯ç”Ÿæˆåƒæ•¸\n",
        "    temporal_split_method: str = \"sliding_window\"  # æ™‚é–“çª—å£åˆ†å‰²\n",
        "    noise_injection_std: float = 0.03  # æ·»åŠ å·®åˆ†éš±ç§å‹å¥½çš„å™ªè²\n",
        "    feature_augmentation: bool = True   # ç‰¹å¾µå¢å¼·\n",
        "    cross_validation_split: bool = True # äº¤å‰é©—è­‰åˆ†å‰²\n",
        "\n",
        "    # ã€ä¿®æ”¹4ã€‘æ”¾å¤§æœ¬åœ°æ‰¹æ¬¡èˆ‡å›åˆ\n",
        "    local_episodes_per_round: int = 4  # 6 â†’ 12\n",
        "    steps_per_episode: int = 500        # 300 â†’ 600\n",
        "    batch_size: int = 256              # 64 â†’ 256\n",
        "    gamma: float = 0.99\n",
        "\n",
        "    # å­¸ç¿’åƒæ•¸\n",
        "    lr: float = 1e-4\n",
        "    target_update_freq: int = 15\n",
        "\n",
        "    # RL æ¢ç´¢åƒæ•¸\n",
        "    epsilon_start: float = 1.0\n",
        "    epsilon_decay: float = 0.9995\n",
        "    epsilon_min: float = 0.05\n",
        "\n",
        "    # è¨˜æ†¶èˆ‡å›æ”¾ï¼ˆæ“´å¢ä»¥é…åˆå¤§æ‰¹æ¬¡ï¼‰\n",
        "    memory_capacity: int = 50000       # 50000 â†’ 100000\n",
        "    replay_start_size: int = 1000       # 1000 â†’ 2000\n",
        "    replay_frequency: int = 2\n",
        "    replay_batches_per_call: int = 3    # 2 â†’ 3\n",
        "\n",
        "    # ã€ä¿®æ”¹6ã€‘èª¿é«˜FedProxæ­£å‰‡åŒ–å¼·åº¦\n",
        "    fedprox_mu: float = 0.15           # 0.01 â†’ 0.15\n",
        "    num_clusters: int = 3              # é…åˆå®¢æˆ¶ç«¯æ•¸é‡èª¿æ•´\n",
        "    cluster_update_freq: int = 8       # 15 â†’ 8ï¼Œæ›´é »ç¹æ›´æ–°\n",
        "\n",
        "    # ã€ä¿®æ”¹1&4ã€‘å·®åˆ†éš±ç§åƒæ•¸å„ªåŒ–\n",
        "    enable_dp: bool = True\n",
        "    dp_target_epsilon: float = 8.0\n",
        "    dp_target_delta: float = 1e-5\n",
        "    dp_max_grad_norm: float = 1.0\n",
        "    dp_noise_multiplier: float = 0.5   # 0.7 â†’ 0.5ï¼ˆé…åˆå¤§æ‰¹æ¬¡ï¼‰\n",
        "    dp_sampling_probability: float = 0.1  # 0.05 â†’ 0.1\n",
        "    dp_virtual_batch_size: int = 256   # 64 â†’ 256\n",
        "    dp_microbatch_size: int = 1\n",
        "\n",
        "    # ã€ä¿®æ”¹2ã€‘é‡è¨­æ©Ÿåˆ¶åƒæ•¸\n",
        "    dp_reset_threshold_multiplier: float = 1.5  # è¶…é1.5Ã—target_epsilonæ™‚é‡è¨­\n",
        "    enable_dp_reset: bool = True\n",
        "\n",
        "    # åŠŸèƒ½é–‹é—œ\n",
        "    enable_heterogeneity: bool = True\n",
        "    enable_compression: bool = True\n",
        "\n",
        "    # ç³»çµ±è¨­å®š\n",
        "    straggler_ratio: float = 0.1\n",
        "    dropout_ratio: float = 0.05\n",
        "    compression_type: str = \"quantize_fp16\"\n",
        "    use_pfl_finetune: bool = True\n",
        "    local_finetune_episodes: int = 15\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    reset_to_random_start: bool = True\n",
        "\n",
        "    # å…¼å®¹æ€§åƒæ•¸ï¼ˆä¿æŒå‘å¾Œå…¼å®¹ï¼‰\n",
        "    client_pairs: tuple = field(init=False)  # å°‡åœ¨ __post_init__ ä¸­è¨­å®š\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # å‹•æ…‹ç”Ÿæˆå®¢æˆ¶ç«¯é…å°ï¼ˆç”¨æ–¼å…¼å®¹æ€§ï¼‰\n",
        "        extended_client_pairs = []\n",
        "        for i in range(self.total_clients):\n",
        "            embb_id = i + 1\n",
        "            urllc_id = i + 2\n",
        "            extended_client_pairs.append((embb_id, urllc_id))\n",
        "        self.client_pairs = tuple(extended_client_pairs)\n",
        "\n",
        "        # GPU ç’°å¢ƒæª¢æ¸¬\n",
        "        if torch.cuda.is_available():\n",
        "            if \"L4\" in torch.cuda.get_device_name(0):\n",
        "                print(f\"ğŸš€ L4 GPUæª¢æ¸¬åˆ°ï¼Œå•Ÿç”¨å¤§æ‰¹æ¬¡å„ªåŒ–é…ç½®\")\n",
        "\n",
        "        # å·®åˆ†éš±ç§æ¨¡å¼æª¢æ¸¬\n",
        "        if self.mode == 'Centralized':\n",
        "            self.enable_dp = False\n",
        "\n",
        "        # é…ç½®ä¿¡æ¯é¡¯ç¤º\n",
        "        print(f\"ğŸ”„ è™›æ“¬å®¢æˆ¶ç«¯é…ç½®:\")\n",
        "        print(f\"   - çœŸå¯¦åŸºç«™å°: {len(self.base_client_pairs)} ({self.base_client_pairs})\")\n",
        "        print(f\"   - æ“´å¢å› å­: {self.virtual_expansion_factor}\")\n",
        "        print(f\"   - ç¸½å®¢æˆ¶ç«¯æ•¸: {self.total_clients} (çœŸå¯¦: {self.num_real_clients}, è™›æ“¬: {self.num_virtual_clients})\")\n",
        "        print(f\"   - æ¯è¼ªåƒèˆ‡: {self.num_clients_to_select}\")\n",
        "\n",
        "        if self.enable_dp and self.mode != 'Centralized':\n",
        "            print(f\"ğŸ›¡ï¸ å·®åˆ†éš±ç§å·²å•Ÿç”¨ï¼ˆGDP/PRV Accountant + Poisson Samplingï¼‰\")\n",
        "            print(f\"   - ç›®æ¨™éš±ç§é ç®—: Îµ={self.dp_target_epsilon}\")\n",
        "            print(f\"   - æ‰¹æ¬¡å¤§å°: {self.batch_size}\")\n",
        "            print(f\"   - å™ªè²ä¹˜æ•¸: {self.dp_noise_multiplier}\")\n",
        "            print(f\"   - é‡è¨­æ©Ÿåˆ¶: {'å•Ÿç”¨' if self.enable_dp_reset else 'ç¦ç”¨'}\")\n",
        "        else:\n",
        "            print(f\"ğŸ›¡ï¸ å·®åˆ†éš±ç§ï¼šç¦ç”¨ï¼ˆæ¨¡å¼: {self.mode}ï¼‰\")\n",
        "\n",
        "    def save(self):\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        path = os.path.join(self.output_dir, f'{self.experiment_name}_config.json')\n",
        "        config_dict = {k: (list(v) if isinstance(v, tuple) else v) for k, v in self.__dict__.items()}\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(config_dict, f, indent=4)\n",
        "        print(f\"âœ… é…ç½®å·²ä¿å­˜è‡³: {path}\")\n",
        "\n",
        "print(\"âœ… Cell 2: TrainingConfigï¼ˆè™›æ“¬å®¢æˆ¶ç«¯å„ªåŒ–ç‰ˆï¼‰å®šç¾©å®Œæˆã€‚\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PK6SehiIDD0",
        "outputId": "f9db7702-c237-4546-ff68-5f8975e62ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 3: DataManagerï¼ˆä¿®æ­£ç‰ˆï¼‰å®šç¾©å®Œæˆã€‚\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 3: ğŸ§© æ•¸æ“šèˆ‡ç’°å¢ƒæº–å‚™ï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
        "class DataManager:\n",
        "    def __init__(self, data_path, client_pairs_config):\n",
        "        print(f\"\\n[DataManager] æ­£åœ¨å¾ {data_path} è®€å–æ•¸æ“š...\")\n",
        "        self.df_kpi = pd.read_parquet(data_path)\n",
        "        self.client_pairs_config = client_pairs_config\n",
        "        self._sanitize_column_names(); self._preflight_check()\n",
        "\n",
        "    def _sanitize_column_names(self):\n",
        "        sanitized_columns = [re.sub(r'[\\[\\]\\(\\)%\\s\\.-]+', '_', col.strip().lower()).strip('_')\n",
        "                           for col in self.df_kpi.columns]\n",
        "        self.df_kpi.columns = sanitized_columns\n",
        "\n",
        "    def _preflight_check(self):\n",
        "        print(\"\\n\" + \"=\"*20 + \" DataManager å•Ÿå‹•å‰é æª¢æŸ¥ \" + \"=\"*20)\n",
        "        cols = self.df_kpi.columns.tolist()\n",
        "        tput_cand = ['throughput_dl_mbps', 'tx_brate_downlink_mbps']\n",
        "        lat_cand = ['buffer_occupancy_dl_bytes', 'dl_buffer_bytes']\n",
        "\n",
        "        self.tput_col = next((c for c in tput_cand if c in cols), None)\n",
        "        self.lat_col = next((c for c in lat_cand if c in cols), None)\n",
        "\n",
        "        print(f\"âœ… æ¸…ç†å¾Œçš„æ¬„ä½åˆ—è¡¨ (å…± {len(cols)} å€‹):\")\n",
        "        print(f\"   - ååé‡æ¬„ä½æˆåŠŸåŒ¹é…: '{self.tput_col}'\" if self.tput_col\n",
        "              else \"   - ååé‡æ¬„ä½åŒ¹é…å¤±æ•—ï¼\")\n",
        "        print(f\"   - å»¶é²/ç·©è¡å€æ¬„ä½æˆåŠŸåŒ¹é…: '{self.lat_col}'\" if self.lat_col\n",
        "              else \"   - å»¶é²/ç·©è¡å€æ¬„ä½åŒ¹é…å¤±æ•—ï¼\")\n",
        "\n",
        "        # ä¿®æ­£ï¼šæª¢æŸ¥BSç¯€é»çš„å¯¦éš›å¯ç”¨æ€§\n",
        "        available_bs = sorted(self.df_kpi['bs_id'].unique())\n",
        "        print(f\"   - å¯ç”¨BSç¯€é»: {available_bs}\")\n",
        "\n",
        "        # é©—è­‰å®¢æˆ¶ç«¯é…å°çš„æœ‰æ•ˆæ€§\n",
        "        for i, (embb_bs, urllc_bs) in enumerate(self.client_pairs_config):\n",
        "            if embb_bs not in available_bs or urllc_bs not in available_bs:\n",
        "                raise ValueError(f\"å®¢æˆ¶ç«¯ {i} çš„BSé…å° ({embb_bs}, {urllc_bs}) ä¸­åŒ…å«ä¸å­˜åœ¨çš„BSç¯€é»\")\n",
        "        print(\"   - å®¢æˆ¶ç«¯BSé…å°é©—è­‰é€šé\")\n",
        "        print(\"=\"*65 + \"\\n\")\n",
        "\n",
        "        if not (self.tput_col and self.lat_col):\n",
        "            raise ValueError(\"é æª¢æŸ¥å¤±æ•—: æ‰¾ä¸åˆ°å¿…è¦çš„æ•¸æ“šæ¬„ä½ã€‚\")\n",
        "\n",
        "    def _get_clean_df(self, gnb_id, slice_id):\n",
        "        \"\"\"ä¿®æ­£ç‰ˆï¼šå¢åŠ æ›´åš´æ ¼çš„æ•¸æ“šéæ¿¾å’Œé©—è­‰\"\"\"\n",
        "        df, bs_col, sl_col = self.df_kpi, 'bs_id', 'slice_id'\n",
        "\n",
        "        # ç¢ºä¿æ•¸æ“šé¡å‹ä¸€è‡´æ€§\n",
        "        mask = (df[bs_col].astype(int) == int(gnb_id)) & (df[sl_col].astype(int) == int(slice_id))\n",
        "        subset = df.loc[mask, ['timestamp', self.tput_col, self.lat_col]].copy()\n",
        "\n",
        "        # ä¿®æ­£ï¼šæ›´åš´æ ¼çš„æ•¸æ“šæ¸…ç†\n",
        "        subset = subset.rename(columns={self.tput_col: 'throughput', self.lat_col: 'latency'})\n",
        "        subset = subset.dropna()\n",
        "\n",
        "        # ç§»é™¤ç•°å¸¸å€¼ï¼ˆè¶…å‡ºåˆç†ç¯„åœçš„æ•¸æ“šé»ï¼‰\n",
        "        if not subset.empty:\n",
        "            subset = subset[\n",
        "                (subset['throughput'] >= 0) & (subset['throughput'] <= 1000) &  # ååé‡ç¯„åœ\n",
        "                (subset['latency'] >= 0) & (subset['latency'] <= 1e9)  # å»¶é²ç¯„åœ\n",
        "            ]\n",
        "\n",
        "        return subset\n",
        "\n",
        "    def get_client_trajectories(self):\n",
        "        \"\"\"ä¿®æ­£ç‰ˆï¼šç¢ºä¿å®¢æˆ¶ç«¯æ•¸æ“šå®Œå…¨ç¨ç«‹\"\"\"\n",
        "        client_trajectories = {}\n",
        "        print(\"[DataManager] æ­£åœ¨ç‚ºæ¯å€‹å®¢æˆ¶ç«¯ç”Ÿæˆæ•¸æ“šè»Œè·¡...\")\n",
        "\n",
        "        for i, (embb_id, urllc_id) in enumerate(tqdm(self.client_pairs_config, desc=\"è™•ç†å®¢æˆ¶ç«¯æ•¸æ“š\")):\n",
        "            try:\n",
        "                # ç¢ºä¿æ¯å€‹å®¢æˆ¶ç«¯ä½¿ç”¨ä¸åŒçš„BSï¼Œç¶­è­·æ•¸æ“šç¨ç«‹æ€§\n",
        "                df_embb = self._get_clean_df(embb_id, 0)  # eMBBåˆ‡ç‰‡\n",
        "                df_urllc = self._get_clean_df(urllc_id, 2)  # URLLCåˆ‡ç‰‡\n",
        "\n",
        "                if df_embb.empty or df_urllc.empty:\n",
        "                    print(f\"ğŸŸ¡ è­¦å‘Š: å®¢æˆ¶ç«¯ {i} (BS {embb_id}/{urllc_id}) ç¯©é¸å¾Œç„¡æœ‰æ•ˆæ•¸æ“šã€‚\")\n",
        "                    client_trajectories[i] = np.array([])\n",
        "                    continue\n",
        "\n",
        "                # ä¿®æ­£ï¼šä½¿ç”¨æ›´ä¿å®ˆçš„æ™‚é–“å®¹å¿åº¦ï¼Œé¿å…æ•¸æ“šæ´©æ¼\n",
        "                merged_df = pd.merge_asof(\n",
        "                    df_embb.sort_values('timestamp'),\n",
        "                    df_urllc.sort_values('timestamp'),\n",
        "                    on='timestamp',\n",
        "                    direction='backward',  # ç¢ºä¿åªä½¿ç”¨éå»çš„ä¿¡æ¯\n",
        "                    tolerance=pd.Timedelta('100ms'),  # æ¸›å°‘å®¹å¿åº¦\n",
        "                    suffixes=('_embb', '_urllc')\n",
        "                ).dropna()\n",
        "\n",
        "                if merged_df.empty:\n",
        "                    print(f\"ğŸŸ¡ è­¦å‘Š: å®¢æˆ¶ç«¯ {i} åˆä½µå¾Œç„¡æœ‰æ•ˆæ•¸æ“šã€‚\")\n",
        "                    client_trajectories[i] = np.array([])\n",
        "                    continue\n",
        "\n",
        "                # ç¢ºä¿æ•¸æ“šåºåˆ—çš„æ™‚é–“é †åºæ€§\n",
        "                merged_df = merged_df.sort_values('timestamp').reset_index(drop=True)\n",
        "                trajectory = merged_df[['throughput_embb', 'latency_embb',\n",
        "                                      'throughput_urllc', 'latency_urllc']].to_numpy(dtype=np.float32)\n",
        "\n",
        "                client_trajectories[i] = trajectory\n",
        "                print(f\"   - å®¢æˆ¶ç«¯ {i}: {len(trajectory)} å€‹æ™‚é–“æ­¥\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ éŒ¯èª¤: è™•ç†å®¢æˆ¶ç«¯ {i} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}\")\n",
        "                client_trajectories[i] = np.array([])\n",
        "\n",
        "        num_valid = sum(1 for traj in client_trajectories.values() if traj.size > 0)\n",
        "        print(f\"\\n[DataManager] æ•¸æ“šè™•ç†å®Œæˆï¼æˆåŠŸç‚º {num_valid} / {len(self.client_pairs_config)} å€‹å®¢æˆ¶ç«¯å‰µå»ºäº†ç’°å¢ƒã€‚\")\n",
        "        return client_trajectories\n",
        "\n",
        "print(\"âœ… Cell 3: DataManagerï¼ˆä¿®æ­£ç‰ˆï¼‰å®šç¾©å®Œæˆã€‚\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GhzPgA9QILLk",
        "outputId": "24eabb2e-d638-40b9-80f2-b1855c54964d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 4: RLç’°å¢ƒèˆ‡æ•¸æ“šè™•ç†ï¼ˆT4 CPUé™åˆ¶å„ªåŒ–ç‰ˆï¼‰å®šç¾©å®Œæˆã€‚\n",
            "ğŸ”¥ T4å°ˆç”¨å„ªåŒ–é …ç›®:\n",
            "   1. CPUè³‡æºæ„ŸçŸ¥çš„ num_workers è¨­å®šï¼ˆ2 CPU â†’ num_workers=1ï¼‰\n",
            "   2. å¤šå±¤å›é€€æ©Ÿåˆ¶ç¢ºä¿ç©©å®šæ€§\n",
            "   3. ä¿å®ˆçš„è¨˜æ†¶é«”å’Œç·šç¨‹ç®¡ç†\n",
            "   4. T4å‹å¥½çš„GPUé ç†±ç­–ç•¥\n",
            "   5. CPUé™åˆ¶æ„ŸçŸ¥çš„CUDAé…ç½®\n",
            "   6. å¢å¼·çš„éŒ¯èª¤è™•ç†æ©Ÿåˆ¶\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 4: âš¡ RLç’°å¢ƒèˆ‡æ•¸æ“šè™•ç†ï¼ˆT4 CPUé™åˆ¶å„ªåŒ–ç‰ˆï¼‰\n",
        "import gc\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import contextlib\n",
        "from collections import deque\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "class PairedEnv:\n",
        "    \"\"\"é…å°ç’°å¢ƒé¡åˆ¥ - ä¿æŒä¸è®Šä½†åŠ å…¥GPUå„ªåŒ–æ¨™è¨˜\"\"\"\n",
        "    def __init__(self, trajectory, config: TrainingConfig):\n",
        "        self.trajectory, self.config = trajectory, config\n",
        "        self.state_size = trajectory.shape[1] if trajectory.size > 0 else 4\n",
        "        self.action_size = 3\n",
        "        self.cursor = 0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        if self.trajectory.size == 0:\n",
        "            return np.zeros(self.state_size, dtype=np.float32)\n",
        "        max_start = max(0, len(self.trajectory) - self.config.steps_per_episode)\n",
        "        if self.config.reset_to_random_start and max_start > 0:\n",
        "            self.cursor = np.random.randint(0, max_start)\n",
        "        else:\n",
        "            self.cursor = 0\n",
        "        return self.trajectory[self.cursor]\n",
        "\n",
        "    def step(self, action_id: int):\n",
        "        if self.trajectory.size == 0 or self.cursor >= len(self.trajectory) - 1:\n",
        "            state = self.trajectory[-1] if self.trajectory.size > 0 else np.zeros(self.state_size, dtype=np.float32)\n",
        "            return state, 0.0, True, {}\n",
        "        self.cursor += 1\n",
        "        done = self.cursor >= len(self.trajectory) - 1\n",
        "        state = self.trajectory[self.cursor]\n",
        "        reward = self._compute_reward_with_action(state, action_id)\n",
        "        return state, reward, done, {}\n",
        "\n",
        "    def _compute_reward_with_action(self, state: np.ndarray, action_id: int) -> float:\n",
        "        tput_embb, lat_embb, tput_urllc, lat_urllc = state\n",
        "        if action_id == 0: w_tput, w_lat = (0.7, 0.3)\n",
        "        elif action_id == 2: w_tput, w_lat = (0.3, 0.7)\n",
        "        else: w_tput, w_lat = (0.5, 0.5)\n",
        "        tput_reward = w_tput * (np.log1p(tput_embb) + 0.5 * np.log1p(tput_urllc))\n",
        "        lat_penalty = w_lat * (np.tanh(lat_urllc * 1e-6) + 0.3 * np.tanh(lat_embb * 1e-6))\n",
        "        reward_val = tput_reward - lat_penalty\n",
        "        return float(np.nan_to_num(reward_val, nan=0.0, posinf=10.0, neginf=-10.0))\n",
        "\n",
        "class RLDataset(Dataset):\n",
        "    \"\"\"RLæ•¸æ“šé›†é¡åˆ¥ - GPUå„ªåŒ–ç‰ˆæœ¬\"\"\"\n",
        "    def __init__(self, memory_list):\n",
        "        self.data = memory_list[:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        state, action, reward, next_state, done = self.data[idx]\n",
        "        return (\n",
        "            torch.from_numpy(state).float(),\n",
        "            torch.tensor(action).long(),\n",
        "            torch.tensor(reward).float(),\n",
        "            torch.from_numpy(next_state).float(),\n",
        "            torch.tensor(done).bool()\n",
        "        )\n",
        "\n",
        "def get_gpu_optimized_data_loader(agent_memory: deque, batch_size: int, device: str):\n",
        "    \"\"\"\n",
        "    ğŸ”¥ GPUæ€§èƒ½å„ªåŒ–çš„æ•¸æ“šåŠ è¼‰å™¨ï¼ˆT4 CPUé™åˆ¶å„ªåŒ–ç‰ˆï¼‰\n",
        "\n",
        "    ä¸»è¦å„ªåŒ–ï¼š\n",
        "    1. é‡å°T4æœ‰é™CPUè³‡æºçš„æ™ºèƒ½ num_workers è¨­å®š\n",
        "    2. ä¿å®ˆçš„ prefetch_factor é¿å…è¨˜æ†¶é«”å£“åŠ›\n",
        "    3. pin_memory åŠ é€Ÿ CPU-GPU å‚³è¼¸\n",
        "    4. å¢å¼·çš„éŒ¯èª¤è™•ç†å’Œå›é€€æ©Ÿåˆ¶\n",
        "    \"\"\"\n",
        "    if len(agent_memory) < batch_size:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        dataset = RLDataset(list(agent_memory))\n",
        "\n",
        "        # ğŸ”¥ é—œéµå„ªåŒ–1ï¼šT4 CPUé™åˆ¶æ„ŸçŸ¥çš„ num_workers è¨­å®š\n",
        "        cpu_count = os.cpu_count() or 4\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_name = torch.cuda.get_device_name(0)\n",
        "\n",
        "            # ã€ä¿®æ­£ã€‘é‡å°T4 GPUçš„CPUé™åˆ¶å„ªåŒ–\n",
        "            if \"L4\" in gpu_name:\n",
        "                if cpu_count <= 2:\n",
        "                    # T4ç’°å¢ƒé€šå¸¸åªæœ‰2å€‹CPUï¼Œä½¿ç”¨ä¿å®ˆè¨­å®š\n",
        "                    num_workers = 1              # åªä½¿ç”¨1å€‹workerï¼Œé¿å…ç«¶çˆ­\n",
        "                    prefetch_factor = 2          # é©ä¸­çš„é è¼‰å…¥\n",
        "                    print(f\"ğŸ”§ T4 CPUé™åˆ¶æ¨¡å¼: æª¢æ¸¬åˆ° {cpu_count} CPUï¼Œä½¿ç”¨ num_workers=1\")\n",
        "                elif cpu_count <= 4:\n",
        "                    # ç¨å¤šCPUçš„T4ç’°å¢ƒ\n",
        "                    num_workers = 2              # ä½¿ç”¨2å€‹worker\n",
        "                    prefetch_factor = 3\n",
        "                    print(f\"ğŸ”§ T4 æ¨™æº–æ¨¡å¼: æª¢æ¸¬åˆ° {cpu_count} CPUï¼Œä½¿ç”¨ num_workers=2\")\n",
        "                else:\n",
        "                    # å……è¶³CPUçš„T4ç’°å¢ƒ\n",
        "                    num_workers = min(cpu_count // 2, 4)\n",
        "                    prefetch_factor = 4\n",
        "                    print(f\"ğŸ”§ T4 é«˜æ€§èƒ½æ¨¡å¼: æª¢æ¸¬åˆ° {cpu_count} CPUï¼Œä½¿ç”¨ num_workers={num_workers}\")\n",
        "            else:\n",
        "                # å…¶ä»–GPUï¼šæ›´ä¿å®ˆçš„è¨­å®š\n",
        "                if cpu_count <= 2:\n",
        "                    num_workers = 1\n",
        "                    prefetch_factor = 2\n",
        "                else:\n",
        "                    num_workers = min(cpu_count // 2, 3)\n",
        "                    prefetch_factor = 2\n",
        "        else:\n",
        "            # CPUç’°å¢ƒï¼šæ¥µä¿å®ˆè¨­å®š\n",
        "            if cpu_count <= 2:\n",
        "                num_workers = 0              # å–®ç·šç¨‹æ¨¡å¼\n",
        "                prefetch_factor = None\n",
        "            else:\n",
        "                num_workers = 1\n",
        "                prefetch_factor = 2\n",
        "\n",
        "        # ã€ä¿®æ­£ã€‘é¿å…åœ¨CPUæ•¸é‡æ¥µå°‘æ™‚ä½¿ç”¨persistent_workers\n",
        "        use_persistent_workers = num_workers > 0 and cpu_count > 2\n",
        "\n",
        "        # ğŸ”¥ é—œéµå„ªåŒ–2ï¼šT4å„ªåŒ–çš„ DataLoader è¨­å®š\n",
        "        dataloader_kwargs = {\n",
        "            'dataset': dataset,\n",
        "            'batch_size': batch_size,\n",
        "            'shuffle': True,\n",
        "            'num_workers': num_workers,\n",
        "            'pin_memory': True,\n",
        "            'drop_last': True,\n",
        "        }\n",
        "\n",
        "        # åªåœ¨æœ‰è¶³å¤ è³‡æºæ™‚æ·»åŠ é€²éšåŠŸèƒ½\n",
        "        if num_workers > 0:\n",
        "            dataloader_kwargs['persistent_workers'] = use_persistent_workers\n",
        "            if prefetch_factor is not None:\n",
        "                dataloader_kwargs['prefetch_factor'] = prefetch_factor\n",
        "\n",
        "            # ã€ä¿®æ­£ã€‘åœ¨CPUè³‡æºæœ‰é™æ™‚é¿å…ä½¿ç”¨spawn\n",
        "            if cpu_count > 2:\n",
        "                dataloader_kwargs['multiprocessing_context'] = 'spawn'\n",
        "\n",
        "        return DataLoader(**dataloader_kwargs)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ğŸš¨ GPUå„ªåŒ–DataLoaderå‰µå»ºå¤±æ•—: {e}\")\n",
        "        print(f\"   å›é€€åˆ°åŸºæœ¬è¨­å®š...\")\n",
        "\n",
        "        # ã€ä¿®æ­£ã€‘å¤šå±¤å›é€€æ©Ÿåˆ¶\n",
        "        try:\n",
        "            dataset = RLDataset(list(agent_memory))\n",
        "\n",
        "            # ç¬¬ä¸€å±¤å›é€€ï¼šç°¡åŒ–è¨­å®š\n",
        "            return DataLoader(\n",
        "                dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                num_workers=1 if cpu_count > 2 else 0,  # T4å®‰å…¨è¨­å®š\n",
        "                pin_memory=True,\n",
        "                drop_last=True\n",
        "            )\n",
        "        except Exception as e2:\n",
        "            print(f\"ğŸš¨ ç¬¬ä¸€å±¤å›é€€å¤±æ•—: {e2}\")\n",
        "            print(f\"   ä½¿ç”¨çµ‚æ¥µå®‰å…¨æ¨¡å¼...\")\n",
        "\n",
        "            try:\n",
        "                # ç¬¬äºŒå±¤å›é€€ï¼šæœ€å®‰å…¨è¨­å®š\n",
        "                return DataLoader(\n",
        "                    dataset,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True,\n",
        "                    num_workers=0,      # å®Œå…¨å–®ç·šç¨‹\n",
        "                    pin_memory=False,   # é—œé–‰pin_memory\n",
        "                    drop_last=True\n",
        "                )\n",
        "            except Exception as e3:\n",
        "                print(f\"ğŸš¨ çµ‚æ¥µå›é€€ä¹Ÿå¤±æ•—: {e3}\")\n",
        "                return None\n",
        "\n",
        "def setup_gpu_environment():\n",
        "    \"\"\"\n",
        "    ğŸš€ çµ±ä¸€çš„GPUç’°å¢ƒè¨­å®šå‡½æ•¸ï¼ŒT4 CPUé™åˆ¶å„ªåŒ–ç‰ˆ\n",
        "\n",
        "    é‡å°T4ç’°å¢ƒçš„ç‰¹æ®Šå„ªåŒ–ï¼š\n",
        "    1. CPUè³‡æºæ„ŸçŸ¥çš„é…ç½®\n",
        "    2. è¨˜æ†¶é«”ä½¿ç”¨å„ªåŒ–\n",
        "    3. TF32 å’Œæ··åˆç²¾åº¦æ”¯æ´\n",
        "    4. ä¿å®ˆçš„CUDAé…ç½®\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        compute_capability = torch.cuda.get_device_properties(0).major\n",
        "        cpu_count = os.cpu_count() or 4\n",
        "\n",
        "        print(f\"ğŸ® GPU æª¢æ¸¬: {gpu_name}\")\n",
        "        print(f\"ğŸ“Š ç¸½è¨˜æ†¶é«”: {total_memory:.1f} GB\")\n",
        "        print(f\"ğŸ”§ è¨ˆç®—èƒ½åŠ›: {compute_capability}.x\")\n",
        "        print(f\"ğŸ’» å¯ç”¨CPUæ•¸: {cpu_count}\")\n",
        "\n",
        "        if \"L4\" in gpu_name:\n",
        "            # ğŸ”¥ T4 GPU é‡å°CPUé™åˆ¶çš„å„ªåŒ–è¨­å®š\n",
        "            if cpu_count <= 2:\n",
        "                # CPUè³‡æºåš´é‡å—é™çš„T4ç’°å¢ƒ\n",
        "                torch.cuda.set_per_process_memory_fraction(0.85)  # è¼ƒä¿å®ˆçš„è¨˜æ†¶é«”ä½¿ç”¨\n",
        "                print(f\"ğŸ”§ T4 CPUé™åˆ¶è¨­å®šï¼šä½¿ç”¨85%è¨˜æ†¶é«” ({total_memory:.1f} GB)\")\n",
        "\n",
        "                # æ¸›å°‘ä¸¦è¡Œé‹ç®—å£“åŠ›\n",
        "                torch.set_num_threads(1)  # é™åˆ¶PyTorchç·šç¨‹æ•¸\n",
        "                os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "                os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "                # ä¿å®ˆçš„CUDAé…ç½®\n",
        "                os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024,expandable_segments:True\"\n",
        "\n",
        "            else:\n",
        "                # æ¨™æº–T4è¨­å®š\n",
        "                torch.cuda.set_per_process_memory_fraction(0.90)\n",
        "                print(f\"ğŸš€ T4 æ¨™æº–è¨­å®šï¼šä½¿ç”¨90%è¨˜æ†¶é«” ({total_memory:.1f} GB)\")\n",
        "\n",
        "                # æ¨™æº–å„ªåŒ–\n",
        "                os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:2048,expandable_segments:True,roundup_power2_divisions:16\"\n",
        "\n",
        "            # T4é€šç”¨å„ªåŒ–\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "            torch.backends.cuda.matmul.allow_tf32 = True\n",
        "            torch.backends.cudnn.allow_tf32 = True\n",
        "            torch.backends.cudnn.deterministic = False\n",
        "\n",
        "        elif compute_capability >= 7:  # V100, A100ç­‰é«˜ç«¯GPU\n",
        "            torch.cuda.set_per_process_memory_fraction(0.90)\n",
        "            print(f\"ğŸ¯ é«˜ç«¯GPUè¨­å®šï¼šä½¿ç”¨90%è¨˜æ†¶é«” ({total_memory:.1f} GB)\")\n",
        "\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "            torch.backends.cuda.matmul.allow_tf32 = True\n",
        "            torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "            os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024,expandable_segments:True\"\n",
        "\n",
        "        else:\n",
        "            # å…¶ä»–GPUä½¿ç”¨è¼ƒä¿å®ˆçš„è¨­å®š\n",
        "            torch.cuda.set_per_process_memory_fraction(0.80)\n",
        "            print(f\"ğŸ® æ¨™æº–GPUè¨­å®šï¼šä½¿ç”¨80%è¨˜æ†¶é«” ({total_memory:.1f} GB)\")\n",
        "\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "            os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512,expandable_segments:True\"\n",
        "\n",
        "        # ğŸ”¥ GPUé ç†± - é‡å°T4å„ªåŒ–\n",
        "        print(f\"ğŸ”¥ é–‹å§‹GPUé ç†±...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # ã€ä¿®æ­£ã€‘T4å‹å¥½çš„é ç†±ç­–ç•¥\n",
        "            if \"L4\" in gpu_name and cpu_count <= 2:\n",
        "                # CPUå—é™æ™‚ä½¿ç”¨è¼ƒå°çš„é ç†±å¼µé‡\n",
        "                dummy_tensor = torch.randn(1024, 1024, device='cuda', dtype=torch.float32)\n",
        "                for _ in range(2):  # æ¸›å°‘é ç†±æ¬¡æ•¸\n",
        "                    _ = dummy_tensor @ dummy_tensor\n",
        "            else:\n",
        "                # æ¨™æº–é ç†±\n",
        "                dummy_tensor = torch.randn(2048, 2048, device='cuda', dtype=torch.float32)\n",
        "                for _ in range(3):\n",
        "                    _ = dummy_tensor @ dummy_tensor\n",
        "\n",
        "            # é ç†±æ··åˆç²¾åº¦\n",
        "            if torch.cuda.is_available() and compute_capability >= 7:\n",
        "                with autocast():\n",
        "                    dummy_half = dummy_tensor.half()\n",
        "                    _ = dummy_half @ dummy_half\n",
        "\n",
        "            del dummy_tensor\n",
        "            if 'dummy_half' in locals():\n",
        "                del dummy_half\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "            warmup_time = (time.time() - start_time) * 1000\n",
        "            print(f\"   âœ… GPUé ç†±å®Œæˆ ({warmup_time:.1f}ms)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   âš ï¸ GPUé ç†±å¤±æ•—: {e}\")\n",
        "\n",
        "        # æ¸…ç†è¨˜æ†¶é«”\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # ğŸ”¥ è¨˜æ†¶é«”ä½¿ç”¨çµ±è¨ˆ\n",
        "        memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        print(f\"ğŸ“Š è¨˜æ†¶é«”ç‹€æ…‹: å·²åˆ†é… {memory_allocated:.2f}GB, å·²ä¿ç•™ {memory_reserved:.2f}GB\")\n",
        "\n",
        "        print(f\"ğŸ§¹ GPUç’°å¢ƒè¨­å®šå®Œæˆï¼ˆç¸½è€—æ™‚: {(time.time() - start_time)*1000:.1f}msï¼‰\")\n",
        "\n",
        "    else:\n",
        "        print(\"âš ï¸ æœªæª¢æ¸¬åˆ°GPUï¼Œå°‡ä½¿ç”¨CPUæ¨¡å¼é‹è¡Œï¼ˆæ€§èƒ½å°‡æ˜é¡¯ä¸‹é™ï¼‰ã€‚\")\n",
        "\n",
        "def create_gpu_scaler():\n",
        "    \"\"\"å‰µå»ºGPUæ··åˆç²¾åº¦ç¸®æ”¾å™¨\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return GradScaler()\n",
        "    return None\n",
        "\n",
        "def optimize_batch_processing(states, actions, rewards, next_states, dones, device):\n",
        "    \"\"\"\n",
        "    ğŸ”¥ æ‰¹æ¬¡è™•ç†å„ªåŒ–å‡½æ•¸ï¼ˆT4å„ªåŒ–ç‰ˆï¼‰\n",
        "\n",
        "    å„ªåŒ–é …ç›®ï¼š\n",
        "    1. éé˜»å¡è¨˜æ†¶é«”å‚³è¼¸\n",
        "    2. è¨˜æ†¶é«”é åˆ†é…\n",
        "    3. T4å‹å¥½çš„æ•¸æ“šå‹åˆ¥å„ªåŒ–\n",
        "    \"\"\"\n",
        "    # ğŸ”¥ éé˜»å¡å‚³è¼¸åˆ°GPU\n",
        "    states_gpu = states.to(device, non_blocking=True)\n",
        "    actions_gpu = actions.to(device, non_blocking=True)\n",
        "    rewards_gpu = rewards.to(device, non_blocking=True)\n",
        "    next_states_gpu = next_states.to(device, non_blocking=True)\n",
        "    dones_gpu = dones.to(device, non_blocking=True)\n",
        "\n",
        "    return states_gpu, actions_gpu, rewards_gpu, next_states_gpu, dones_gpu\n",
        "\n",
        "# ğŸ”¥ å…¨å±€æ€§èƒ½ç›£æ§è®Šæ•¸\n",
        "_gpu_utilization_history = []\n",
        "_memory_usage_history = []\n",
        "\n",
        "def monitor_gpu_performance():\n",
        "    \"\"\"ç›£æ§GPUæ€§èƒ½ï¼ˆå¯é¸ï¼‰\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        _memory_usage_history.append({\n",
        "            'allocated': memory_allocated,\n",
        "            'reserved': memory_reserved,\n",
        "            'timestamp': time.time()\n",
        "        })\n",
        "\n",
        "        # ä¿æŒæ­·å²è¨˜éŒ„åœ¨åˆç†ç¯„åœå…§\n",
        "        if len(_memory_usage_history) > 100:\n",
        "            _memory_usage_history.pop(0)\n",
        "\n",
        "def get_performance_summary():\n",
        "    \"\"\"ç²å–æ€§èƒ½æ‘˜è¦\"\"\"\n",
        "    if not _memory_usage_history:\n",
        "        return \"ç„¡æ€§èƒ½æ•¸æ“š\"\n",
        "\n",
        "    recent = _memory_usage_history[-10:]  # æœ€è¿‘10æ¬¡è¨˜éŒ„\n",
        "    avg_allocated = sum(r['allocated'] for r in recent) / len(recent)\n",
        "    avg_reserved = sum(r['reserved'] for r in recent) / len(recent)\n",
        "\n",
        "    return f\"å¹³å‡GPUè¨˜æ†¶é«”ä½¿ç”¨: åˆ†é… {avg_allocated:.2f}GB, ä¿ç•™ {avg_reserved:.2f}GB\"\n",
        "\n",
        "print(\"âœ… Cell 4: RLç’°å¢ƒèˆ‡æ•¸æ“šè™•ç†ï¼ˆT4 CPUé™åˆ¶å„ªåŒ–ç‰ˆï¼‰å®šç¾©å®Œæˆã€‚\")\n",
        "print(\"ğŸ”¥ T4å°ˆç”¨å„ªåŒ–é …ç›®:\")\n",
        "print(\"   1. CPUè³‡æºæ„ŸçŸ¥çš„ num_workers è¨­å®šï¼ˆ2 CPU â†’ num_workers=1ï¼‰\")\n",
        "print(\"   2. å¤šå±¤å›é€€æ©Ÿåˆ¶ç¢ºä¿ç©©å®šæ€§\")\n",
        "print(\"   3. ä¿å®ˆçš„è¨˜æ†¶é«”å’Œç·šç¨‹ç®¡ç†\")\n",
        "print(\"   4. T4å‹å¥½çš„GPUé ç†±ç­–ç•¥\")\n",
        "print(\"   5. CPUé™åˆ¶æ„ŸçŸ¥çš„CUDAé…ç½®\")\n",
        "print(\"   6. å¢å¼·çš„éŒ¯èª¤è™•ç†æ©Ÿåˆ¶\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hJCSz0KING1",
        "outputId": "834e62b7-117f-4962-b3b9-5e58c08430ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 5: RLAgentï¼ˆå¢å¼·å®¹éŒ¯ç‰ˆï¼‰å®šç¾©å®Œæˆã€‚\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 5: ğŸ›¡ï¸ æ ¸å¿ƒå­¸ç¿’ä»£ç†ï¼ˆå¢å¼·å®¹éŒ¯ç‰ˆï¼‰\n",
        "import gc\n",
        "import time\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.validators import ModuleValidator\n",
        "\n",
        "class RLAgent:\n",
        "    def __init__(self, state_size: int, action_size: int, config: TrainingConfig, client_id: int,\n",
        "                 dataset_size: int, is_eval_agent: bool = False):\n",
        "        self.state_size, self.action_size, self.config = state_size, action_size, config\n",
        "        self.client_id, self.dataset_size = client_id, dataset_size\n",
        "        self.device = torch.device(config.device)\n",
        "        self.mu = self.config.fedprox_mu\n",
        "        self.gamma, self.epsilon = config.gamma, config.epsilon_start\n",
        "        self.memory = deque(maxlen=config.memory_capacity)\n",
        "        self.global_params = None\n",
        "        self.is_eval_agent = is_eval_agent\n",
        "        self.privacy_engine = None\n",
        "        self.dp_steps = 0\n",
        "        self.current_epsilon = 0.0\n",
        "        self.current_best_alpha = None\n",
        "        self.consecutive_errors = 0\n",
        "        self.max_consecutive_errors = 5\n",
        "\n",
        "        # ã€å¢å¼·éŒ¯èª¤è™•ç†ã€‘éš±ç§è¨ˆç®—å¤±æ•—è¿½è¹¤\n",
        "        self.privacy_calc_failures = 0\n",
        "        self.last_valid_epsilon = 0.0\n",
        "        self.dp_reset_count = 0\n",
        "        self.last_reset_round = -1\n",
        "        self.original_optimizer_class = optim.Adam\n",
        "\n",
        "        self.model = self._build_dp_model()\n",
        "        self.target_model = self._build_dp_model()\n",
        "        self.update_target_model()\n",
        "        self.target_model.eval()\n",
        "\n",
        "        self.optimizer = self.original_optimizer_class(self.model.parameters(), lr=config.lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        if self.config.enable_dp and not self.is_eval_agent and self.config.mode != 'Centralized':\n",
        "            self._initialize_dp_engine()\n",
        "        else:\n",
        "            print(f\"[C-{self.client_id}] ğŸ›¡ï¸ æ¨™æº–æ¨¡å¼ï¼ˆç„¡å·®åˆ†éš±ç§ï¼‰\")\n",
        "\n",
        "    def _build_dp_model(self):\n",
        "        model = nn.Sequential(\n",
        "            nn.Linear(self.state_size, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 64), nn.ReLU(),\n",
        "            nn.Linear(64, self.action_size)\n",
        "        ).to(self.device)\n",
        "        if self.config.enable_dp and not self.is_eval_agent:\n",
        "            if not ModuleValidator.is_valid(model):\n",
        "                model = ModuleValidator.fix(model)\n",
        "        return model\n",
        "\n",
        "    def _initialize_dp_engine(self):\n",
        "        \"\"\"ã€ç©©å®šæ€§å¢å¼·ç‰ˆã€‘å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–\"\"\"\n",
        "        print(f\"[C-{self.client_id}] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\")\n",
        "\n",
        "        # åˆå§‹åŒ–å¤±æ•—è¨ˆæ•¸å™¨\n",
        "        self.privacy_calc_failures = 0\n",
        "        self.last_valid_epsilon = 0.0\n",
        "\n",
        "        try:\n",
        "            # æ¸…ç†èˆŠå¼•æ“\n",
        "            if hasattr(self, 'privacy_engine') and self.privacy_engine is not None:\n",
        "                del self.privacy_engine\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "            # é‡å»ºæ¨¡å‹å’Œå„ªåŒ–å™¨\n",
        "            self.model = self._build_dp_model()\n",
        "            self.optimizer = self.original_optimizer_class(self.model.parameters(), lr=self.config.lr)\n",
        "\n",
        "            # å‰µå»ºè™›æ“¬æ•¸æ“šé›†\n",
        "            dummy_data = []\n",
        "            effective_dataset_size = max(self.dataset_size, self.config.batch_size * 10)\n",
        "\n",
        "            for _ in range(effective_dataset_size):\n",
        "                state = np.random.randn(self.state_size).astype(np.float32)\n",
        "                action = int(np.random.randint(0, 3))\n",
        "                reward = float(np.random.randn())\n",
        "                next_state = np.random.randn(self.state_size).astype(np.float32)\n",
        "                done = bool(np.random.choice([True, False]))\n",
        "                dummy_data.append((state, action, reward, next_state, done))\n",
        "\n",
        "            dummy_dataset = RLDataset(dummy_data)\n",
        "            dummy_loader = DataLoader(\n",
        "                dummy_dataset, batch_size=self.config.batch_size, num_workers=0, shuffle=True\n",
        "            )\n",
        "\n",
        "            # ã€å¢å¼·ã€‘å¤šé‡ accountant å›é€€æ©Ÿåˆ¶\n",
        "            accountant_options = [\n",
        "                (\"gdp\", \"GDP\"),\n",
        "                (\"prv\", \"PRV\"),\n",
        "                (\"rdp\", \"RDP\"),\n",
        "                (None, \"Default\")\n",
        "            ]\n",
        "\n",
        "            for accountant_type, type_name in accountant_options:\n",
        "                try:\n",
        "                    if accountant_type:\n",
        "                        self.privacy_engine = PrivacyEngine(accountant=accountant_type)\n",
        "                    else:\n",
        "                        self.privacy_engine = PrivacyEngine()\n",
        "\n",
        "                    print(f\"   - å˜—è©¦ Accountant: {type_name}\")\n",
        "                    break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"   - {type_name} Accountant å¤±æ•—: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if not self.privacy_engine:\n",
        "                raise RuntimeError(\"æ‰€æœ‰ Accountant é¡å‹éƒ½å¤±æ•—\")\n",
        "\n",
        "            # å‹•æ…‹ sample_rate è¨ˆç®—\n",
        "            sample_rate = min(self.config.batch_size / len(dummy_data), 1.0)\n",
        "            print(f\"   - Sample Rate: {sample_rate:.6f}\")\n",
        "\n",
        "            self.model, self.optimizer, dummy_loader = self.privacy_engine.make_private(\n",
        "                module=self.model,\n",
        "                optimizer=self.optimizer,\n",
        "                data_loader=dummy_loader,\n",
        "                noise_multiplier=self.config.dp_noise_multiplier,\n",
        "                max_grad_norm=self.config.dp_max_grad_norm,\n",
        "                poisson_sampling=True\n",
        "            )\n",
        "\n",
        "            # é‡è¨­è¨ˆæ•¸å™¨\n",
        "            self.dp_steps = 0\n",
        "            self.current_epsilon = 0.0\n",
        "            self.current_best_alpha = None\n",
        "\n",
        "            print(f\"   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\")\n",
        "            print(f\"   - å™ªè²ä¹˜æ•¸: {self.config.dp_noise_multiplier}\")\n",
        "            print(f\"   - æ¢¯åº¦è£å‰ª: {self.config.dp_max_grad_norm}\")\n",
        "            print(f\"   - Poissonæ¡æ¨£: True\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   - âŒ å·®åˆ†éš±ç§åˆå§‹åŒ–å¤±æ•—: {e}\")\n",
        "            print(f\"   - ğŸ”„ åˆ‡æ›åˆ°éå·®åˆ†éš±ç§æ¨¡å¼\")\n",
        "            self.privacy_engine = None\n",
        "            self.privacy_calc_failures = 999  # æ¨™è¨˜ç‚ºæ°¸ä¹…å¤±æ•—ç‹€æ…‹\n",
        "\n",
        "    def reset_dp_engine(self, round_num: int):\n",
        "        \"\"\"ã€å®Œå…¨ä¿®æ­£2ã€‘æ­£ç¢ºçš„DPå¼•æ“é‡è¨­æ–¹æ³•\"\"\"\n",
        "        if not self.config.enable_dp_reset or not self.privacy_engine:\n",
        "            return False\n",
        "\n",
        "        print(f\"[C-{self.client_id}] ğŸ”„ é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound {round_num}ï¼‰...\")\n",
        "        try:\n",
        "            # è¨˜éŒ„é‡è¨­è³‡è¨Š\n",
        "            self.dp_reset_count += 1\n",
        "            self.last_reset_round = round_num\n",
        "            old_epsilon = self.current_epsilon\n",
        "\n",
        "            # é‡æ–°åˆå§‹åŒ–æ•´å€‹engine\n",
        "            self._initialize_dp_engine()\n",
        "\n",
        "            print(f\"   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬{self.dp_reset_count}æ¬¡ï¼‰\")\n",
        "            print(f\"   - èˆŠÎµ: {old_epsilon:.4f} â†’ æ–°Îµ: {self.current_epsilon:.4f}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"   - âŒ é‡è¨­å¤±æ•—: {e}\")\n",
        "            return False\n",
        "\n",
        "    def get_privacy_cost(self):\n",
        "        \"\"\"ã€å¢å¼·ç‰ˆã€‘éš±ç§æˆæœ¬è¨ˆç®—ï¼Œå¤šé‡éŒ¯èª¤è™•ç†æ©Ÿåˆ¶\"\"\"\n",
        "        if not self.privacy_engine:\n",
        "            return 0.0\n",
        "\n",
        "        # å¦‚æœä¹‹å‰è¨ˆç®—å¤±æ•—å¤ªå¤šæ¬¡ï¼Œç›´æ¥è¿”å›ç·©å­˜å€¼\n",
        "        if hasattr(self, 'privacy_calc_failures') and self.privacy_calc_failures > 10:\n",
        "            return getattr(self, 'last_valid_epsilon', 0.0)\n",
        "\n",
        "        try:\n",
        "            # ã€æ–¹æ³•1ã€‘å˜—è©¦æ¨™æº– get_epsilon èª¿ç”¨\n",
        "            result = self.privacy_engine.get_epsilon(delta=self.config.dp_target_delta)\n",
        "\n",
        "            # è™•ç†ä¸åŒè¿”å›æ ¼å¼\n",
        "            if isinstance(result, tuple):\n",
        "                if len(result) == 2:\n",
        "                    epsilon, best_alpha = result\n",
        "                    self.current_best_alpha = best_alpha\n",
        "                else:\n",
        "                    epsilon = result[0]\n",
        "                    self.current_best_alpha = None\n",
        "            elif isinstance(result, (int, float)):\n",
        "                epsilon = result\n",
        "                self.current_best_alpha = None\n",
        "            else:\n",
        "                raise ValueError(f\"æœªçŸ¥çš„epsilonè¿”å›æ ¼å¼: {type(result)}\")\n",
        "\n",
        "            # é©—è­‰ epsilon å€¼æœ‰æ•ˆæ€§\n",
        "            if np.isinf(epsilon) or np.isnan(epsilon) or epsilon < 0:\n",
        "                raise ValueError(f\"ç„¡æ•ˆepsilonå€¼: {epsilon}\")\n",
        "\n",
        "            # æˆåŠŸè¨ˆç®—ï¼Œé‡ç½®å¤±æ•—è¨ˆæ•¸\n",
        "            if hasattr(self, 'privacy_calc_failures'):\n",
        "                self.privacy_calc_failures = 0\n",
        "\n",
        "            self.current_epsilon = float(epsilon)\n",
        "            self.last_valid_epsilon = self.current_epsilon  # ç·©å­˜æœ‰æ•ˆå€¼\n",
        "            return self.current_epsilon\n",
        "\n",
        "        except Exception as primary_error:\n",
        "            # è¨˜éŒ„å¤±æ•—æ¬¡æ•¸\n",
        "            if not hasattr(self, 'privacy_calc_failures'):\n",
        "                self.privacy_calc_failures = 0\n",
        "            self.privacy_calc_failures += 1\n",
        "\n",
        "            # ã€æ–¹æ³•2ã€‘å˜—è©¦ä½¿ç”¨æ›¿ä»£è¨ˆç®—æ–¹æ³•\n",
        "            try:\n",
        "                # ä½¿ç”¨æ­¥æ•¸ä¼°ç®—éš±ç§æˆæœ¬ï¼ˆç²—ç•¥ä¼°è¨ˆï¼‰\n",
        "                if hasattr(self, 'dp_steps') and self.dp_steps > 0:\n",
        "                    # åŸºæ–¼å·²åŸ·è¡Œæ­¥æ•¸çš„ç°¡å–®ä¼°ç®—\n",
        "                    estimated_epsilon = self.dp_steps * 0.01  # æ¯æ­¥æ¶ˆè€—0.01 epsilonï¼ˆä¿å®ˆä¼°è¨ˆï¼‰\n",
        "                    estimated_epsilon = min(estimated_epsilon, self.config.dp_target_epsilon)\n",
        "\n",
        "                    if self.privacy_calc_failures <= 3:  # å‰å¹¾æ¬¡å¤±æ•—æ™‚é¡¯ç¤ºè©³ç´°ä¿¡æ¯\n",
        "                        print(f\"[C-{self.client_id}] âš ï¸ éš±ç§æˆæœ¬è¨ˆç®—å¤±æ•—: {primary_error}\")\n",
        "                        print(f\"   ğŸ”„ ä½¿ç”¨ä¼°ç®—å€¼: Îµ â‰ˆ {estimated_epsilon:.4f} (åŸºæ–¼ {self.dp_steps} æ­¥)\")\n",
        "\n",
        "                    self.current_epsilon = estimated_epsilon\n",
        "                    return estimated_epsilon\n",
        "\n",
        "            except Exception as fallback_error:\n",
        "                if self.privacy_calc_failures <= 3:\n",
        "                    print(f\"[C-{self.client_id}] âš ï¸ å‚™ç”¨éš±ç§è¨ˆç®—ä¹Ÿå¤±æ•—: {fallback_error}\")\n",
        "\n",
        "            # ã€æ–¹æ³•3ã€‘æœ€çµ‚å›é€€ï¼šä½¿ç”¨ç·©å­˜å€¼æˆ–è¿”å›0\n",
        "            if hasattr(self, 'last_valid_epsilon'):\n",
        "                cached_value = self.last_valid_epsilon\n",
        "                if self.privacy_calc_failures <= 3:\n",
        "                    print(f\"[C-{self.client_id}] ğŸ”„ ä½¿ç”¨ç·©å­˜éš±ç§å€¼: Îµ = {cached_value:.4f}\")\n",
        "                return cached_value\n",
        "\n",
        "            # å®Œå…¨å¤±æ•—çš„æƒ…æ³\n",
        "            if self.privacy_calc_failures <= 3:  # åªåœ¨å‰å¹¾æ¬¡å¤±æ•—æ™‚é¡¯ç¤º\n",
        "                print(f\"[C-{self.client_id}] âš ï¸ éš±ç§æˆæœ¬è¨ˆç®—å®Œå…¨å¤±æ•—: {primary_error}\")\n",
        "                print(f\"   ğŸ”„ è¿”å› 0.0ï¼Œè¨“ç·´ç¹¼çºŒï¼ˆå¤±æ•—æ¬¡æ•¸: {self.privacy_calc_failures}ï¼‰\")\n",
        "            elif self.privacy_calc_failures == 11:  # å¤±æ•—æ¬¡æ•¸éå¤šæ™‚çš„ä¸€æ¬¡æ€§æé†’\n",
        "                print(f\"[C-{self.client_id}] âš ï¸ éš±ç§è¨ˆç®—æŒçºŒå¤±æ•—ï¼Œå·²åˆ‡æ›è‡³éœé»˜æ¨¡å¼\")\n",
        "\n",
        "            return 0.0\n",
        "\n",
        "    def get_privacy_detailed_info(self):\n",
        "        \"\"\"ã€æ–°å¢ã€‘ç²å–è©³ç´°éš±ç§è³‡è¨Šä¾›æ—¥èªŒè¨˜éŒ„\"\"\"\n",
        "        return {\n",
        "            'client_id': self.client_id,\n",
        "            'epsilon': self.current_epsilon,\n",
        "            'best_alpha': self.current_best_alpha,\n",
        "            'dp_steps': self.dp_steps,\n",
        "            'reset_count': self.dp_reset_count,\n",
        "            'last_reset_round': self.last_reset_round,\n",
        "            'calc_failures': getattr(self, 'privacy_calc_failures', 0),\n",
        "            'last_valid_epsilon': getattr(self, 'last_valid_epsilon', 0.0)\n",
        "        }\n",
        "\n",
        "    def replay(self, num_batches: int):\n",
        "        if len(self.memory) < self.config.batch_size:\n",
        "            return 0.0\n",
        "\n",
        "        data_loader = get_gpu_optimized_data_loader(\n",
        "            self.memory, self.config.batch_size, self.device\n",
        "        )\n",
        "        if data_loader is None:\n",
        "            return 0.0\n",
        "\n",
        "        total_loss, batches_processed = 0.0, 0\n",
        "        self.model.train()\n",
        "\n",
        "        try:\n",
        "            for i, batch in enumerate(data_loader):\n",
        "                if i >= num_batches:\n",
        "                    break\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                states, actions, rewards, next_states, dones = [item.to(self.device, non_blocking=True) for item in batch]\n",
        "\n",
        "                current_q = self.model(states).gather(1, actions.view(-1, 1))\n",
        "                with torch.no_grad():\n",
        "                    max_next_q = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
        "                    target_q = rewards.view(-1, 1) + (self.gamma * max_next_q * (~dones.view(-1, 1)))\n",
        "\n",
        "                loss = self.criterion(current_q, target_q)\n",
        "\n",
        "                # ã€æ•ˆèƒ½å„ªåŒ–6ã€‘å¢å¼·çš„FedProxæ­£å‰‡åŒ–ï¼ˆÎ¼=0.15ï¼‰\n",
        "                if self.config.mode in ['FedProx', 'ClusteredFL'] and self.mu > 0 and self.global_params:\n",
        "                    proximal_term = 0.0\n",
        "                    model_params = self.model._module.parameters() if hasattr(self.model, '_module') else self.model.parameters()\n",
        "                    for local_param, global_param in zip(model_params, self.global_params):\n",
        "                        proximal_term += torch.sum((local_param - global_param.to(self.device))**2)\n",
        "                    loss += (self.mu / 2) * proximal_term\n",
        "\n",
        "                if not torch.isfinite(loss):\n",
        "                    continue\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                batches_processed += 1\n",
        "                if self.privacy_engine:\n",
        "                    self.dp_steps += 1\n",
        "\n",
        "            return total_loss / batches_processed if batches_processed > 0 else 0.0\n",
        "        except Exception as e:\n",
        "            print(f\"[C-{self.client_id}] ğŸš¨ å›æ”¾éŒ¯èª¤: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    # å…¶é¤˜æ–¹æ³•ä¿æŒä¸è®Š...\n",
        "    def remember(self, *args):\n",
        "        self.memory.append(args)\n",
        "\n",
        "    def set_global_params(self, state_dict):\n",
        "        with torch.no_grad():\n",
        "            self.global_params = [p.clone().detach().cpu() for p in state_dict.values()]\n",
        "\n",
        "    def act(self, state):\n",
        "        if not self.is_eval_agent and np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(torch.from_numpy(state).float().unsqueeze(0).to(self.device))\n",
        "        return q_values.argmax().item()\n",
        "\n",
        "    def get_clean_state_dict(self):\n",
        "        return self.model._module.state_dict() if self.privacy_engine and hasattr(self.model, '_module') else self.model.state_dict()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.get_clean_state_dict())\n",
        "\n",
        "    def get_model_weights_flat(self):\n",
        "        with torch.no_grad():\n",
        "            params = self.model._module.parameters() if self.privacy_engine and hasattr(self.model, '_module') else self.model.parameters()\n",
        "            return torch.cat([p.view(-1) for p in params]).cpu().numpy()\n",
        "\n",
        "    def get_model_for_upload(self):\n",
        "        state_dict = self.get_clean_state_dict()\n",
        "        return {k: v.half() for k, v in state_dict.items()} if self.config.enable_compression else state_dict\n",
        "\n",
        "print(\"âœ… Cell 5: RLAgentï¼ˆå¢å¼·å®¹éŒ¯ç‰ˆï¼‰å®šç¾©å®Œæˆã€‚\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9WEQIAOrE2a",
        "outputId": "9a1e5599-2a26-4d0a-ac19-03bc0cf2060f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 5.5: FLServer é¡åˆ¥å®šç¾©å®Œæˆã€‚\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 5.5: ğŸŒ è¯é‚¦å­¸ç¿’æœå‹™å™¨é¡åˆ¥ï¼ˆFLServerï¼‰\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import copy\n",
        "\n",
        "class FLServer:\n",
        "    \"\"\"\n",
        "    è¯é‚¦å­¸ç¿’æœå‹™å™¨é¡åˆ¥\n",
        "    è² è²¬æ¨¡å‹èšåˆã€å®¢æˆ¶ç«¯èšé¡å’Œæ¨¡å‹åˆ†ç™¼\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: TrainingConfig):\n",
        "        self.config = config\n",
        "        self.num_clusters = config.num_clusters\n",
        "        self.client_to_cluster = {}\n",
        "        self.cluster_models = {}\n",
        "        self.aggregation_weights = {}\n",
        "\n",
        "        # åˆå§‹åŒ–èšé¡æ¨¡å‹\n",
        "        for i in range(self.num_clusters):\n",
        "            self.cluster_models[i] = None\n",
        "\n",
        "        print(f\"[FLServer] åˆå§‹åŒ–å®Œæˆ - èšé¡æ•¸: {self.num_clusters}\")\n",
        "\n",
        "    def distribute_model(self, participating_agents: Dict, global_model_state: Dict):\n",
        "        \"\"\"\n",
        "        å°‡æ¨¡å‹åˆ†ç™¼çµ¦åƒèˆ‡çš„å®¢æˆ¶ç«¯\n",
        "        \"\"\"\n",
        "        for client_id, agent in participating_agents.items():\n",
        "            if self.config.mode == 'ClusteredFL':\n",
        "                # èšé¡è¯é‚¦å­¸ç¿’ï¼šä½¿ç”¨å°æ‡‰èšé¡çš„æ¨¡å‹\n",
        "                cluster_id = self.client_to_cluster.get(client_id, 0)\n",
        "                if cluster_id in self.cluster_models and self.cluster_models[cluster_id] is not None:\n",
        "                    model_to_send = self.cluster_models[cluster_id]\n",
        "                else:\n",
        "                    model_to_send = global_model_state\n",
        "            else:\n",
        "                # å…¶ä»–æ¨¡å¼ï¼šä½¿ç”¨å…¨åŸŸæ¨¡å‹\n",
        "                model_to_send = global_model_state\n",
        "\n",
        "            # è™•ç†å£“ç¸®æ¨¡å‹\n",
        "            if self.config.enable_compression:\n",
        "                model_to_send = {k: v.float() if hasattr(v, 'float') else v\n",
        "                               for k, v in model_to_send.items()}\n",
        "\n",
        "            # è¨­ç½®å…¨åŸŸåƒæ•¸å’Œè¼‰å…¥æ¨¡å‹\n",
        "            agent.set_global_params(model_to_send)\n",
        "            agent.model.load_state_dict(model_to_send)\n",
        "\n",
        "    def aggregate_weighted(self, client_updates: List[Tuple[Dict, int]]) -> Dict:\n",
        "        \"\"\"\n",
        "        åŠ æ¬Šèšåˆå®¢æˆ¶ç«¯æ¨¡å‹æ›´æ–°\n",
        "\n",
        "        Args:\n",
        "            client_updates: [(model_state_dict, num_samples), ...]\n",
        "\n",
        "        Returns:\n",
        "            aggregated_model_state: èšåˆå¾Œçš„æ¨¡å‹ç‹€æ…‹\n",
        "        \"\"\"\n",
        "        if not client_updates:\n",
        "            return {}\n",
        "\n",
        "        # è¨ˆç®—ç¸½æ¨£æœ¬æ•¸\n",
        "        total_samples = sum(num_samples for _, num_samples in client_updates)\n",
        "        if total_samples == 0:\n",
        "            return client_updates[0][0]  # è¿”å›ç¬¬ä¸€å€‹æ¨¡å‹\n",
        "\n",
        "        # åˆå§‹åŒ–èšåˆæ¨¡å‹\n",
        "        aggregated_model = {}\n",
        "        first_model = client_updates[0][0]\n",
        "\n",
        "        for key in first_model.keys():\n",
        "            aggregated_model[key] = torch.zeros_like(first_model[key])\n",
        "\n",
        "        # åŠ æ¬Šèšåˆ\n",
        "        for model_state, num_samples in client_updates:\n",
        "            weight = num_samples / total_samples\n",
        "            for key in aggregated_model.keys():\n",
        "                if key in model_state:\n",
        "                    aggregated_model[key] += weight * model_state[key].to(aggregated_model[key].device)\n",
        "\n",
        "        return aggregated_model\n",
        "\n",
        "    def update_clusters(self, client_agents: Dict, current_round: int):\n",
        "        \"\"\"\n",
        "        æ›´æ–°å®¢æˆ¶ç«¯èšé¡\n",
        "        \"\"\"\n",
        "        if len(client_agents) < self.num_clusters:\n",
        "            # å®¢æˆ¶ç«¯æ•¸é‡å°‘æ–¼èšé¡æ•¸ï¼Œç°¡å–®åˆ†é…\n",
        "            for i, client_id in enumerate(client_agents.keys()):\n",
        "                self.client_to_cluster[client_id] = i % self.num_clusters\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # æå–å®¢æˆ¶ç«¯æ¨¡å‹ç‰¹å¾µé€²è¡Œèšé¡\n",
        "            client_features = []\n",
        "            client_ids = []\n",
        "\n",
        "            for client_id, agent in client_agents.items():\n",
        "                # ç²å–æ¨¡å‹æ¬Šé‡å‘é‡\n",
        "                weights = agent.get_model_weights_flat()\n",
        "                if weights is not None and len(weights) > 0:\n",
        "                    client_features.append(weights)\n",
        "                    client_ids.append(client_id)\n",
        "\n",
        "            if len(client_features) < self.num_clusters:\n",
        "                # ç‰¹å¾µä¸è¶³ï¼Œä½¿ç”¨ç°¡å–®åˆ†é…\n",
        "                for i, client_id in enumerate(client_ids):\n",
        "                    self.client_to_cluster[client_id] = i % self.num_clusters\n",
        "                return\n",
        "\n",
        "            # ä½¿ç”¨ K-means é€²è¡Œèšé¡\n",
        "            client_features_array = np.vstack(client_features)\n",
        "\n",
        "            # æ¨™æº–åŒ–ç‰¹å¾µ\n",
        "            from sklearn.preprocessing import StandardScaler\n",
        "            scaler = StandardScaler()\n",
        "            normalized_features = scaler.fit_transform(client_features_array)\n",
        "\n",
        "            # K-means èšé¡\n",
        "            kmeans = KMeans(n_clusters=self.num_clusters, random_state=42, n_init=10)\n",
        "            cluster_labels = kmeans.fit_predict(normalized_features)\n",
        "\n",
        "            # æ›´æ–°å®¢æˆ¶ç«¯åˆ°èšé¡çš„æ˜ å°„\n",
        "            for client_id, cluster_label in zip(client_ids, cluster_labels):\n",
        "                self.client_to_cluster[client_id] = int(cluster_label)\n",
        "\n",
        "            print(f\"[FLServer] Round {current_round}: èšé¡æ›´æ–°å®Œæˆ\")\n",
        "            for cluster_id in range(self.num_clusters):\n",
        "                clients_in_cluster = [cid for cid, cid_cluster in self.client_to_cluster.items()\n",
        "                                    if cid_cluster == cluster_id]\n",
        "                print(f\"  èšé¡ {cluster_id}: {clients_in_cluster}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[FLServer] èšé¡æ›´æ–°å¤±æ•—: {e}\")\n",
        "            # å›é€€åˆ°ç°¡å–®åˆ†é…\n",
        "            for i, client_id in enumerate(client_agents.keys()):\n",
        "                self.client_to_cluster[client_id] = i % self.num_clusters\n",
        "\n",
        "    def get_cluster_info(self) -> Dict:\n",
        "        \"\"\"\n",
        "        ç²å–èšé¡ä¿¡æ¯\n",
        "        \"\"\"\n",
        "        cluster_info = {}\n",
        "        for cluster_id in range(self.num_clusters):\n",
        "            clients_in_cluster = [cid for cid, cid_cluster in self.client_to_cluster.items()\n",
        "                                if cid_cluster == cluster_id]\n",
        "            cluster_info[cluster_id] = {\n",
        "                'clients': clients_in_cluster,\n",
        "                'size': len(clients_in_cluster),\n",
        "                'has_model': cluster_id in self.cluster_models and self.cluster_models[cluster_id] is not None\n",
        "            }\n",
        "        return cluster_info\n",
        "\n",
        "print(\"âœ… Cell 5.5: FLServer é¡åˆ¥å®šç¾©å®Œæˆã€‚\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtMd7sQ5IOyr",
        "outputId": "c6084eba-d9ea-469f-d75b-2437c5cd1a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 6: FLServer é¡åˆ¥å®šç¾©å®Œæˆï¼ˆå®Œæ•´ä¿®æ­£ç‰ˆ - DPæ ¼å¼è½‰æ›ï¼‰ã€‚\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 6: ğŸŒ è¯é‚¦å­¸ç¿’æœå‹™å™¨é¡åˆ¥ï¼ˆFLServerï¼‰- å®Œæ•´ä¿®æ­£ç‰ˆ\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import copy\n",
        "\n",
        "class FLServer:\n",
        "    \"\"\"\n",
        "    è¯é‚¦å­¸ç¿’æœå‹™å™¨é¡åˆ¥ï¼ˆå®Œæ•´ä¿®æ­£ç‰ˆï¼‰\n",
        "    è² è²¬æ¨¡å‹èšåˆã€å®¢æˆ¶ç«¯èšé¡å’Œæ¨¡å‹åˆ†ç™¼\n",
        "    ç‰¹åˆ¥è™•ç†å·®åˆ†éš±ç§æ¨¡å‹çš„ state_dict æ ¼å¼è½‰æ›\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: TrainingConfig):\n",
        "        self.config = config\n",
        "        self.num_clusters = config.num_clusters\n",
        "        self.client_to_cluster = {}\n",
        "        self.cluster_models = {}\n",
        "        self.aggregation_weights = {}\n",
        "\n",
        "        # åˆå§‹åŒ–èšé¡æ¨¡å‹\n",
        "        for i in range(self.num_clusters):\n",
        "            self.cluster_models[i] = None\n",
        "\n",
        "        print(f\"[FLServer] åˆå§‹åŒ–å®Œæˆ - èšé¡æ•¸: {self.num_clusters}\")\n",
        "\n",
        "    def _convert_state_dict_for_dp_model(self, state_dict: Dict, target_is_dp: bool) -> Dict:\n",
        "        \"\"\"\n",
        "        ã€é—œéµä¿®æ­£ã€‘è‡ªå‹•è½‰æ›state_dictæ ¼å¼ä»¥åŒ¹é…DP/éDPæ¨¡å‹\n",
        "\n",
        "        Args:\n",
        "            state_dict: æºæ¨¡å‹çš„state_dict\n",
        "            target_is_dp: ç›®æ¨™æ¨¡å‹æ˜¯å¦ç‚ºDPåŒ…è£æ¨¡å‹ï¼ˆGradSampleModuleï¼‰\n",
        "\n",
        "        Returns:\n",
        "            è½‰æ›å¾Œçš„state_dict\n",
        "        \"\"\"\n",
        "        if not state_dict:\n",
        "            return state_dict\n",
        "\n",
        "        converted_dict = {}\n",
        "\n",
        "        for key, value in state_dict.items():\n",
        "            if target_is_dp and not key.startswith('_module.'):\n",
        "                # ç›®æ¨™æ˜¯DPæ¨¡å‹ï¼Œæºæ˜¯æ¨™æº–æ ¼å¼ â†’ æ·»åŠ _moduleå‰ç¶´\n",
        "                new_key = f\"_module.{key}\"\n",
        "                converted_dict[new_key] = value\n",
        "            elif not target_is_dp and key.startswith('_module.'):\n",
        "                # ç›®æ¨™æ˜¯æ¨™æº–æ¨¡å‹ï¼Œæºæ˜¯DPæ ¼å¼ â†’ ç§»é™¤_moduleå‰ç¶´\n",
        "                new_key = key.replace('_module.', '')\n",
        "                converted_dict[new_key] = value\n",
        "            else:\n",
        "                # æ ¼å¼å·²åŒ¹é…ï¼Œç›´æ¥è¤‡è£½\n",
        "                converted_dict[key] = value\n",
        "\n",
        "        return converted_dict\n",
        "\n",
        "    def _is_dp_model(self, model) -> bool:\n",
        "        \"\"\"\n",
        "        æª¢æ¸¬æ¨¡å‹æ˜¯å¦ç‚ºå·®åˆ†éš±ç§åŒ…è£æ¨¡å‹\n",
        "        \"\"\"\n",
        "        # æ–¹æ³•1: æª¢æŸ¥æ˜¯å¦æœ‰ _module å±¬æ€§\n",
        "        if hasattr(model, '_module'):\n",
        "            return True\n",
        "\n",
        "        # æ–¹æ³•2: æª¢æŸ¥ state_dict çš„éµå\n",
        "        try:\n",
        "            state_dict = model.state_dict()\n",
        "            return any(k.startswith('_module.') for k in state_dict.keys())\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def distribute_model(self, participating_agents: Dict, global_model_state: Dict):\n",
        "        \"\"\"\n",
        "        ã€å®Œå…¨ä¿®æ­£ç‰ˆã€‘å°‡æ¨¡å‹åˆ†ç™¼çµ¦åƒèˆ‡çš„å®¢æˆ¶ç«¯ï¼Œè‡ªå‹•è™•ç†DPæ¨¡å‹æ ¼å¼è½‰æ›\n",
        "        \"\"\"\n",
        "        if not global_model_state:\n",
        "            print(f\"[FLServer] âš ï¸ å…¨åŸŸæ¨¡å‹ç‹€æ…‹ç‚ºç©ºï¼Œè·³éåˆ†ç™¼\")\n",
        "            return\n",
        "\n",
        "        # çµ±è¨ˆå’Œèª¿è©¦ä¿¡æ¯\n",
        "        total_clients = len(participating_agents)\n",
        "        successful_distributions = 0\n",
        "        failed_distributions = 0\n",
        "\n",
        "        for client_id, agent in participating_agents.items():\n",
        "            try:\n",
        "                # é¸æ“‡è¦åˆ†ç™¼çš„æ¨¡å‹\n",
        "                if self.config.mode == 'ClusteredFL':\n",
        "                    # èšé¡è¯é‚¦å­¸ç¿’ï¼šä½¿ç”¨å°æ‡‰èšé¡çš„æ¨¡å‹\n",
        "                    cluster_id = self.client_to_cluster.get(client_id, 0)\n",
        "                    if cluster_id in self.cluster_models and self.cluster_models[cluster_id] is not None:\n",
        "                        model_to_send = self.cluster_models[cluster_id]\n",
        "                    else:\n",
        "                        model_to_send = global_model_state\n",
        "                else:\n",
        "                    # å…¶ä»–æ¨¡å¼ï¼šä½¿ç”¨å…¨åŸŸæ¨¡å‹\n",
        "                    model_to_send = global_model_state\n",
        "\n",
        "                if not model_to_send:\n",
        "                    print(f\"[FLServer] âš ï¸ å®¢æˆ¶ç«¯ {client_id}: ç„¡å¯åˆ†ç™¼æ¨¡å‹\")\n",
        "                    failed_distributions += 1\n",
        "                    continue\n",
        "\n",
        "                # ã€é—œéµä¿®æ­£ã€‘è‡ªå‹•æª¢æ¸¬å®¢æˆ¶ç«¯æ¨¡å‹é¡å‹ä¸¦è½‰æ›æ ¼å¼\n",
        "                agent_is_dp = self._is_dp_model(agent.model)\n",
        "\n",
        "                # è‡ªå‹•è½‰æ›state_dictæ ¼å¼\n",
        "                converted_model = self._convert_state_dict_for_dp_model(\n",
        "                    model_to_send, target_is_dp=agent_is_dp\n",
        "                )\n",
        "\n",
        "                # è™•ç†æ¨¡å‹å£“ç¸®\n",
        "                if self.config.enable_compression:\n",
        "                    converted_model = {k: v.half() if hasattr(v, 'half') else v\n",
        "                                     for k, v in converted_model.items()}\n",
        "\n",
        "                # è¨­ç½®å…¨åŸŸåƒæ•¸ï¼ˆç”¨æ–¼FedProxç­‰ç®—æ³•ï¼‰\n",
        "                try:\n",
        "                    agent.set_global_params(converted_model)\n",
        "                except Exception as e:\n",
        "                    print(f\"[FLServer] âš ï¸ å®¢æˆ¶ç«¯ {client_id}: set_global_paramså¤±æ•—: {e}\")\n",
        "\n",
        "                # è¼‰å…¥æ¨¡å‹ç‹€æ…‹\n",
        "                agent.model.load_state_dict(converted_model, strict=True)\n",
        "                successful_distributions += 1\n",
        "\n",
        "                # èª¿è©¦ä¿¡æ¯ï¼ˆåªé¡¯ç¤ºç¬¬ä¸€å€‹å®¢æˆ¶ç«¯çš„è©³ç´°ä¿¡æ¯ï¼‰\n",
        "                if client_id == list(participating_agents.keys())[0]:\n",
        "                    print(f\"[FLServer] å®¢æˆ¶ç«¯ {client_id}: æ¨¡å‹åˆ†ç™¼æˆåŠŸ\")\n",
        "                    print(f\"  - DPæ¨¡å‹: {agent_is_dp}\")\n",
        "                    print(f\"  - åŸå§‹éµæ•¸: {len(model_to_send)}\")\n",
        "                    print(f\"  - è½‰æ›å¾Œéµæ•¸: {len(converted_model)}\")\n",
        "                    if model_to_send and converted_model:\n",
        "                        orig_sample_key = list(model_to_send.keys())[0]\n",
        "                        conv_sample_key = list(converted_model.keys())[0]\n",
        "                        print(f\"  - éµåç¯„ä¾‹: {orig_sample_key} â†’ {conv_sample_key}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                failed_distributions += 1\n",
        "                print(f\"[FLServer] âŒ å®¢æˆ¶ç«¯ {client_id} æ¨¡å‹åˆ†ç™¼å¤±æ•—: {e}\")\n",
        "                print(f\"  - éŒ¯èª¤é¡å‹: {type(e).__name__}\")\n",
        "\n",
        "                # ã€å¼·åŒ–å›é€€æ©Ÿåˆ¶ã€‘å˜—è©¦å¤šç¨®è¼‰å…¥æ–¹å¼\n",
        "                try:\n",
        "                    # å›é€€1: ä½¿ç”¨éåš´æ ¼æ¨¡å¼è¼‰å…¥åŸå§‹æ¨¡å‹\n",
        "                    agent.model.load_state_dict(model_to_send, strict=False)\n",
        "                    successful_distributions += 1\n",
        "                    print(f\"  - ğŸ”„ å›é€€æˆåŠŸï¼šéåš´æ ¼æ¨¡å¼è¼‰å…¥\")\n",
        "                except Exception as e2:\n",
        "                    try:\n",
        "                        # å›é€€2: å˜—è©¦åå‘è½‰æ›\n",
        "                        agent_is_dp = self._is_dp_model(agent.model)\n",
        "                        alt_converted_model = self._convert_state_dict_for_dp_model(\n",
        "                            model_to_send, target_is_dp=not agent_is_dp\n",
        "                        )\n",
        "                        agent.model.load_state_dict(alt_converted_model, strict=False)\n",
        "                        successful_distributions += 1\n",
        "                        print(f\"  - ğŸ”„ åå‘è½‰æ›å›é€€æˆåŠŸ\")\n",
        "                    except Exception as e3:\n",
        "                        print(f\"  - âŒ æ‰€æœ‰å›é€€æ–¹å¼éƒ½å¤±æ•—: {e3}\")\n",
        "\n",
        "        # åˆ†ç™¼çµ±è¨ˆ\n",
        "        print(f\"[FLServer] æ¨¡å‹åˆ†ç™¼å®Œæˆ: æˆåŠŸ {successful_distributions}/{total_clients}\")\n",
        "        if failed_distributions > 0:\n",
        "            print(f\"  - âš ï¸ å¤±æ•—æ•¸é‡: {failed_distributions}\")\n",
        "\n",
        "    def aggregate_weighted(self, client_updates: List[Tuple[Dict, int]]) -> Dict:\n",
        "        \"\"\"\n",
        "        ã€ä¿®æ­£ç‰ˆã€‘åŠ æ¬Šèšåˆå®¢æˆ¶ç«¯æ¨¡å‹æ›´æ–°ï¼Œç¢ºä¿è¿”å›æ¨™æº–æ ¼å¼\n",
        "        \"\"\"\n",
        "        if not client_updates:\n",
        "            return {}\n",
        "\n",
        "        # è¨ˆç®—ç¸½æ¨£æœ¬æ•¸\n",
        "        total_samples = sum(num_samples for _, num_samples in client_updates)\n",
        "        if total_samples == 0:\n",
        "            return client_updates[0][0] if client_updates else {}\n",
        "\n",
        "        # ç²å–ç¬¬ä¸€å€‹æ¨¡å‹çš„çµæ§‹\n",
        "        first_model = client_updates[0][0]\n",
        "        if not first_model:\n",
        "            return {}\n",
        "\n",
        "        # ã€ä¿®æ­£ã€‘ç¢ºä¿ä½¿ç”¨æ¨™æº–æ ¼å¼çš„éµåé€²è¡Œèšåˆ\n",
        "        standard_first_model = self._convert_state_dict_for_dp_model(first_model, target_is_dp=False)\n",
        "\n",
        "        # åˆå§‹åŒ–èšåˆæ¨¡å‹\n",
        "        aggregated_model = {}\n",
        "        for key in standard_first_model.keys():\n",
        "            aggregated_model[key] = torch.zeros_like(standard_first_model[key])\n",
        "\n",
        "        # åŠ æ¬Šèšåˆ\n",
        "        for model_state, num_samples in client_updates:\n",
        "            if not model_state:\n",
        "                continue\n",
        "\n",
        "            weight = num_samples / total_samples\n",
        "\n",
        "            # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼\n",
        "            standard_model_state = self._convert_state_dict_for_dp_model(model_state, target_is_dp=False)\n",
        "\n",
        "            for key in aggregated_model.keys():\n",
        "                if key in standard_model_state:\n",
        "                    # ç¢ºä¿è¨­å‚™åŒ¹é…\n",
        "                    param_tensor = standard_model_state[key]\n",
        "                    if param_tensor.device != aggregated_model[key].device:\n",
        "                        param_tensor = param_tensor.to(aggregated_model[key].device)\n",
        "\n",
        "                    aggregated_model[key] += weight * param_tensor\n",
        "\n",
        "        return aggregated_model\n",
        "\n",
        "    def update_clusters(self, client_agents: Dict, current_round: int):\n",
        "        \"\"\"\n",
        "        æ›´æ–°å®¢æˆ¶ç«¯èšé¡ï¼ˆClusteredFLæ¨¡å¼ä½¿ç”¨ï¼‰\n",
        "        \"\"\"\n",
        "        if len(client_agents) < self.num_clusters:\n",
        "            # å®¢æˆ¶ç«¯æ•¸é‡å°‘æ–¼èšé¡æ•¸ï¼Œç°¡å–®åˆ†é…\n",
        "            for i, client_id in enumerate(client_agents.keys()):\n",
        "                self.client_to_cluster[client_id] = i % self.num_clusters\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # æå–å®¢æˆ¶ç«¯æ¨¡å‹ç‰¹å¾µé€²è¡Œèšé¡\n",
        "            client_features = []\n",
        "            client_ids = []\n",
        "\n",
        "            for client_id, agent in client_agents.items():\n",
        "                try:\n",
        "                    # ç²å–æ¨¡å‹æ¬Šé‡å‘é‡\n",
        "                    weights = agent.get_model_weights_flat()\n",
        "                    if weights is not None and len(weights) > 0:\n",
        "                        client_features.append(weights)\n",
        "                        client_ids.append(client_id)\n",
        "                except Exception as e:\n",
        "                    print(f\"[FLServer] âš ï¸ å®¢æˆ¶ç«¯ {client_id} ç‰¹å¾µæå–å¤±æ•—: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if len(client_features) < self.num_clusters:\n",
        "                # ç‰¹å¾µä¸è¶³ï¼Œä½¿ç”¨ç°¡å–®åˆ†é…\n",
        "                for i, client_id in enumerate(client_ids):\n",
        "                    self.client_to_cluster[client_id] = i % self.num_clusters\n",
        "                return\n",
        "\n",
        "            # ä½¿ç”¨ K-means é€²è¡Œèšé¡\n",
        "            client_features_array = np.vstack(client_features)\n",
        "\n",
        "            # æ¨™æº–åŒ–ç‰¹å¾µ\n",
        "            from sklearn.preprocessing import StandardScaler\n",
        "            scaler = StandardScaler()\n",
        "            normalized_features = scaler.fit_transform(client_features_array)\n",
        "\n",
        "            # K-means èšé¡\n",
        "            kmeans = KMeans(n_clusters=self.num_clusters, random_state=42, n_init=10)\n",
        "            cluster_labels = kmeans.fit_predict(normalized_features)\n",
        "\n",
        "            # æ›´æ–°å®¢æˆ¶ç«¯åˆ°èšé¡çš„æ˜ å°„\n",
        "            for client_id, cluster_label in zip(client_ids, cluster_labels):\n",
        "                self.client_to_cluster[client_id] = int(cluster_label)\n",
        "\n",
        "            print(f\"[FLServer] Round {current_round}: èšé¡æ›´æ–°å®Œæˆ\")\n",
        "            for cluster_id in range(self.num_clusters):\n",
        "                clients_in_cluster = [cid for cid, cid_cluster in self.client_to_cluster.items()\n",
        "                                    if cid_cluster == cluster_id]\n",
        "                print(f\"  èšé¡ {cluster_id}: {clients_in_cluster}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[FLServer] èšé¡æ›´æ–°å¤±æ•—: {e}\")\n",
        "            # å›é€€åˆ°ç°¡å–®åˆ†é…\n",
        "            for i, client_id in enumerate(client_agents.keys()):\n",
        "                self.client_to_cluster[client_id] = i % self.num_clusters\n",
        "\n",
        "    def get_cluster_info(self) -> Dict:\n",
        "        \"\"\"\n",
        "        ç²å–èšé¡ä¿¡æ¯\n",
        "        \"\"\"\n",
        "        cluster_info = {}\n",
        "        for cluster_id in range(self.num_clusters):\n",
        "            clients_in_cluster = [cid for cid, cid_cluster in self.client_to_cluster.items()\n",
        "                                if cid_cluster == cluster_id]\n",
        "            cluster_info[cluster_id] = {\n",
        "                'clients': clients_in_cluster,\n",
        "                'size': len(clients_in_cluster),\n",
        "                'has_model': cluster_id in self.cluster_models and self.cluster_models[cluster_id] is not None\n",
        "            }\n",
        "        return cluster_info\n",
        "\n",
        "print(\"âœ… Cell 6: FLServer é¡åˆ¥å®šç¾©å®Œæˆï¼ˆå®Œæ•´ä¿®æ­£ç‰ˆ - DPæ ¼å¼è½‰æ›ï¼‰ã€‚\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8XuIPrqxMVj",
        "outputId": "01e8f38a-4e3c-4091-d087-288d128caf53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 6.5: VirtualClientGeneratorï¼ˆå­¸è¡“åˆç†ç‰ˆï¼‰å®šç¾©å®Œæˆã€‚\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 6.5: ğŸ”„ è™›æ“¬å®¢æˆ¶ç«¯ç”Ÿæˆå™¨ï¼ˆå­¸è¡“åˆç†ç‰ˆï¼‰\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from typing import Dict, List, Tuple\n",
        "import copy\n",
        "\n",
        "class VirtualClientGenerator:\n",
        "    \"\"\"\n",
        "    å­¸è¡“ä¸Šåˆç†çš„è™›æ“¬å®¢æˆ¶ç«¯ç”Ÿæˆå™¨\n",
        "    åŸºæ–¼æ™‚é–“åˆ†å‰²å’Œç‰¹å¾µå¢å¼·æŠ€è¡“ï¼Œé¿å…æ•¸æ“šæ´©æ¼\n",
        "\n",
        "    å­¸è¡“ä¾æ“š:\n",
        "    - FedTDD: æ™‚é–“åºåˆ—è¯é‚¦å­¸ç¿’ä¸­çš„æ™‚é–“å°é½ŠæŠ€è¡“\n",
        "    - FFTS: ç•°è³ªæ€§æ™‚é–“åºåˆ—çš„è¯é‚¦åŸºç¤æ¨¡å‹æ–¹æ³•\n",
        "    - SplitAVG: åŸºæ–¼åˆ†å‰²çš„è¯é‚¦å­¸ç¿’ç•°è³ªæ€§è™•ç†\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: TrainingConfig):\n",
        "        self.config = config\n",
        "        self.scalers = {}\n",
        "        self.generation_stats = {\n",
        "            'real_clients': 0,\n",
        "            'virtual_clients': 0,\n",
        "            'failed_generations': 0,\n",
        "            'total_data_points': 0\n",
        "        }\n",
        "\n",
        "    def generate_virtual_clients(self, original_trajectories: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        å¾åŸå§‹è»Œè·¡ç”Ÿæˆè™›æ“¬å®¢æˆ¶ç«¯\n",
        "\n",
        "        Returns:\n",
        "            Dict: åŒ…å«çœŸå¯¦å’Œè™›æ“¬å®¢æˆ¶ç«¯çš„è»Œè·¡å­—å…¸\n",
        "        \"\"\"\n",
        "        virtual_trajectories = {}\n",
        "\n",
        "        print(f\"\\nğŸ”„ é–‹å§‹ç”Ÿæˆè™›æ“¬å®¢æˆ¶ç«¯...\")\n",
        "        print(f\"   - åŸå§‹å®¢æˆ¶ç«¯æ•¸: {len(original_trajectories)}\")\n",
        "        print(f\"   - ç›®æ¨™ç¸½å®¢æˆ¶ç«¯æ•¸: {self.config.total_clients}\")\n",
        "\n",
        "        # 1. è™•ç†åŸå§‹å®¢æˆ¶ç«¯ï¼ˆçœŸå¯¦åŸºç«™å°ï¼‰\n",
        "        for i, (embb_bs, urllc_bs) in enumerate(self.config.base_client_pairs):\n",
        "            client_id = i\n",
        "            if client_id in original_trajectories:\n",
        "                virtual_trajectories[client_id] = original_trajectories[client_id]\n",
        "                self.generation_stats['real_clients'] += 1\n",
        "                self.generation_stats['total_data_points'] += len(original_trajectories[client_id])\n",
        "                print(f\"[çœŸå¯¦å®¢æˆ¶ç«¯ {client_id}] åŸºç«™å°: ({embb_bs}, {urllc_bs}) - æ•¸æ“šé»: {len(original_trajectories[client_id])}\")\n",
        "\n",
        "        # 2. ç”Ÿæˆè™›æ“¬å®¢æˆ¶ç«¯\n",
        "        virtual_client_id = len(self.config.base_client_pairs)\n",
        "\n",
        "        for base_id, (embb_bs, urllc_bs) in enumerate(self.config.base_client_pairs):\n",
        "            if base_id not in original_trajectories:\n",
        "                print(f\"âš ï¸ è·³éåŸºç«™å° ({embb_bs}, {urllc_bs}) - ç„¡åŸå§‹æ•¸æ“š\")\n",
        "                continue\n",
        "\n",
        "            base_trajectory = original_trajectories[base_id]\n",
        "\n",
        "            # ç‚ºæ¯å€‹åŸºç«™å°ç”Ÿæˆå¤šå€‹è™›æ“¬å®¢æˆ¶ç«¯\n",
        "            for virtual_idx in range(self.config.virtual_expansion_factor):\n",
        "                virtual_traj = self._create_virtual_trajectory(\n",
        "                    base_trajectory, base_id, virtual_idx\n",
        "                )\n",
        "\n",
        "                if virtual_traj is not None and virtual_traj.size > 0:\n",
        "                    virtual_trajectories[virtual_client_id] = virtual_traj\n",
        "                    self.generation_stats['virtual_clients'] += 1\n",
        "                    self.generation_stats['total_data_points'] += len(virtual_traj)\n",
        "                    print(f\"[è™›æ“¬å®¢æˆ¶ç«¯ {virtual_client_id}] åŸºæ–¼åŸºç«™å° ({embb_bs}, {urllc_bs}) - è®Šé«” {virtual_idx+1} - æ•¸æ“šé»: {len(virtual_traj)}\")\n",
        "                    virtual_client_id += 1\n",
        "                else:\n",
        "                    self.generation_stats['failed_generations'] += 1\n",
        "                    print(f\"âŒ è™›æ“¬å®¢æˆ¶ç«¯ç”Ÿæˆå¤±æ•—: åŸºç«™å° ({embb_bs}, {urllc_bs}) - è®Šé«” {virtual_idx+1}\")\n",
        "\n",
        "        # 3. ç”Ÿæˆçµ±è¨ˆå ±å‘Š\n",
        "        print(f\"\\nâœ… è™›æ“¬å®¢æˆ¶ç«¯ç”Ÿæˆå®Œæˆ:\")\n",
        "        print(f\"   - çœŸå¯¦å®¢æˆ¶ç«¯: {self.generation_stats['real_clients']}\")\n",
        "        print(f\"   - è™›æ“¬å®¢æˆ¶ç«¯: {self.generation_stats['virtual_clients']}\")\n",
        "        print(f\"   - å¤±æ•—ç”Ÿæˆ: {self.generation_stats['failed_generations']}\")\n",
        "        print(f\"   - ç¸½å®¢æˆ¶ç«¯: {len(virtual_trajectories)}\")\n",
        "        print(f\"   - ç¸½æ•¸æ“šé»: {self.generation_stats['total_data_points']}\")\n",
        "\n",
        "        return virtual_trajectories\n",
        "\n",
        "    def _create_virtual_trajectory(self, base_trajectory: np.ndarray,\n",
        "                                   base_id: int, virtual_idx: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        å‰µå»ºè™›æ“¬è»Œè·¡çš„æ ¸å¿ƒæ–¹æ³•\n",
        "\n",
        "        æ¡ç”¨å¤šç¨®å­¸è¡“é©—è­‰çš„æŠ€è¡“:\n",
        "        1. æ™‚é–“æ»‘å‹•çª—å£åˆ†å‰² (é¿å…æ•¸æ“šé‡è¤‡)\n",
        "        2. å·®åˆ†éš±ç§å‹å¥½çš„å™ªè²æ³¨å…¥\n",
        "        3. ç‰¹å¾µè®Šæ›å’Œå¢å¼·\n",
        "        4. çµ±è¨ˆä¸è®Šæ€§ä¿æŒ\n",
        "        \"\"\"\n",
        "        if base_trajectory.size == 0:\n",
        "            return np.array([])\n",
        "\n",
        "        try:\n",
        "            # æ–¹æ³•é¸æ“‡ï¼šæ ¹æ“šé…ç½®é¸æ“‡ç”Ÿæˆæ–¹æ³•\n",
        "            if self.config.temporal_split_method == \"sliding_window\":\n",
        "                virtual_traj = self._temporal_sliding_window(\n",
        "                    base_trajectory, virtual_idx\n",
        "                )\n",
        "            elif self.config.temporal_split_method == \"cross_validation\":\n",
        "                virtual_traj = self._cross_validation_split(\n",
        "                    base_trajectory, virtual_idx\n",
        "                )\n",
        "            else:\n",
        "                virtual_traj = self._feature_subspace_projection(\n",
        "                    base_trajectory, virtual_idx\n",
        "                )\n",
        "\n",
        "            # ç¢ºä¿åŸºæœ¬æ•¸æ“šå®Œæ•´æ€§\n",
        "            if virtual_traj.size == 0:\n",
        "                return np.array([])\n",
        "\n",
        "            # æ·»åŠ å·®åˆ†éš±ç§å‹å¥½çš„å™ªè²\n",
        "            if self.config.noise_injection_std > 0:\n",
        "                virtual_traj = self._add_privacy_preserving_noise(\n",
        "                    virtual_traj, base_id, virtual_idx\n",
        "                )\n",
        "\n",
        "            # ç‰¹å¾µå¢å¼·\n",
        "            if self.config.feature_augmentation:\n",
        "                virtual_traj = self._feature_augmentation(\n",
        "                    virtual_traj, base_id, virtual_idx\n",
        "                )\n",
        "\n",
        "            # æœ€çµ‚é©—è­‰\n",
        "            if not self._validate_trajectory(virtual_traj):\n",
        "                print(f\"âš ï¸ è™›æ“¬è»Œè·¡é©—è­‰å¤±æ•— (base_id={base_id}, virtual_idx={virtual_idx})\")\n",
        "                return np.array([])\n",
        "\n",
        "            return virtual_traj\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ è™›æ“¬å®¢æˆ¶ç«¯ç”Ÿæˆå¤±æ•— (base_id={base_id}, virtual_idx={virtual_idx}): {e}\")\n",
        "            return np.array([])\n",
        "\n",
        "    def _temporal_sliding_window(self, trajectory: np.ndarray, virtual_idx: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        æ™‚é–“æ»‘å‹•çª—å£åˆ†å‰² - å­¸è¡“ä¾æ“š: FedTDD\n",
        "\n",
        "        å°‡æ™‚é–“åºåˆ—æŒ‰ç…§ä¸åŒçª—å£åˆ†å‰²ï¼Œç¢ºä¿æ•¸æ“šéé‡ç–Š\n",
        "        \"\"\"\n",
        "        total_length = len(trajectory)\n",
        "        # ç¢ºä¿æ¯å€‹è™›æ“¬å®¢æˆ¶ç«¯éƒ½æœ‰è¶³å¤ çš„æ•¸æ“š\n",
        "        window_size = total_length // (self.config.virtual_expansion_factor + 1)\n",
        "\n",
        "        # æœ€å°æ•¸æ“šé‡ä¿è­‰\n",
        "        min_window_size = max(100, self.config.batch_size * 5)\n",
        "        if window_size < min_window_size:\n",
        "            # å¦‚æœçª—å£å¤ªå°ï¼Œä½¿ç”¨é‡ç–Šçª—å£ä½†æ·»åŠ æ›´å¤šå™ªè²\n",
        "            window_size = min_window_size\n",
        "            overlap = 0.3  # 30% é‡ç–Š\n",
        "            stride = int(window_size * (1 - overlap))\n",
        "            start_idx = virtual_idx * stride\n",
        "        else:\n",
        "            # éé‡ç–Šçª—å£\n",
        "            start_idx = virtual_idx * window_size\n",
        "\n",
        "        end_idx = start_idx + window_size\n",
        "\n",
        "        # é‚Šç•Œæª¢æŸ¥\n",
        "        if start_idx >= total_length:\n",
        "            # å¾ªç’°ä½¿ç”¨æ•¸æ“š\n",
        "            start_idx = start_idx % total_length\n",
        "            end_idx = start_idx + min(window_size, total_length - start_idx)\n",
        "\n",
        "        if end_idx > total_length:\n",
        "            end_idx = total_length\n",
        "\n",
        "        if start_idx >= end_idx:\n",
        "            # ä½¿ç”¨å°¾éƒ¨æ•¸æ“š\n",
        "            return trajectory[-min_window_size:].copy()\n",
        "\n",
        "        return trajectory[start_idx:end_idx].copy()\n",
        "\n",
        "    def _cross_validation_split(self, trajectory: np.ndarray, virtual_idx: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        äº¤å‰é©—è­‰åˆ†å‰² - å­¸è¡“ä¾æ“š: SLVR\n",
        "\n",
        "        æ¨¡æ“¬K-foldäº¤å‰é©—è­‰çš„æ•¸æ“šåˆ†å‰²æ–¹å¼\n",
        "        \"\"\"\n",
        "        total_length = len(trajectory)\n",
        "        k_folds = self.config.virtual_expansion_factor + 1\n",
        "        fold_size = total_length // k_folds\n",
        "\n",
        "        # ç¢ºä¿æ¯å€‹foldæœ‰æœ€å°æ•¸æ“šé‡\n",
        "        min_fold_size = max(100, self.config.batch_size * 3)\n",
        "        if fold_size < min_fold_size:\n",
        "            # ä½¿ç”¨éš¨æ©ŸæŠ½æ¨£ä»£æ›¿åš´æ ¼åˆ†å‰²\n",
        "            return self._random_stratified_sampling(trajectory, virtual_idx)\n",
        "\n",
        "        # ä½¿ç”¨ä¸åŒçš„foldä½œç‚ºè™›æ“¬å®¢æˆ¶ç«¯æ•¸æ“š\n",
        "        indices = np.arange(total_length)\n",
        "        np.random.seed(42 + virtual_idx)  # ç¢ºä¿å¯é‡ç¾æ€§\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        start_idx = virtual_idx * fold_size\n",
        "        end_idx = start_idx + fold_size\n",
        "\n",
        "        if end_idx > len(indices):\n",
        "            end_idx = len(indices)\n",
        "\n",
        "        selected_indices = indices[start_idx:end_idx]\n",
        "        selected_indices.sort()  # ä¿æŒæ™‚é–“é †åº\n",
        "\n",
        "        return trajectory[selected_indices].copy()\n",
        "\n",
        "    def _random_stratified_sampling(self, trajectory: np.ndarray, virtual_idx: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        éš¨æ©Ÿåˆ†å±¤æŠ½æ¨£ - ç•¶æ•¸æ“šé‡ä¸è¶³æ™‚çš„å‚™ç”¨æ–¹æ³•\n",
        "        \"\"\"\n",
        "        total_length = len(trajectory)\n",
        "        sample_ratio = 0.7  # æŠ½å–70%çš„æ•¸æ“š\n",
        "        sample_size = int(total_length * sample_ratio)\n",
        "\n",
        "        np.random.seed(100 + virtual_idx)\n",
        "        selected_indices = np.random.choice(\n",
        "            total_length, size=sample_size, replace=False\n",
        "        )\n",
        "        selected_indices.sort()\n",
        "\n",
        "        return trajectory[selected_indices].copy()\n",
        "\n",
        "    def _feature_subspace_projection(self, trajectory: np.ndarray, virtual_idx: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        ç‰¹å¾µå­ç©ºé–“æŠ•å½± - å­¸è¡“ä¾æ“š: SplitAVG\n",
        "\n",
        "        åœ¨ä¸åŒç‰¹å¾µå­ç©ºé–“ä¸­æŠ•å½±æ•¸æ“šï¼Œæ¨¡æ“¬ä¸åŒè¦–è§’\n",
        "        \"\"\"\n",
        "        if trajectory.ndim < 2:\n",
        "            return trajectory\n",
        "\n",
        "        num_features = trajectory.shape[1]\n",
        "        if num_features < 4:\n",
        "            return trajectory\n",
        "\n",
        "        # é¸æ“‡ä¸åŒçš„ç‰¹å¾µå­é›†\n",
        "        feature_indices = np.arange(num_features)\n",
        "        np.random.seed(100 + virtual_idx)\n",
        "\n",
        "        # ä¿ç•™80%çš„ç‰¹å¾µï¼Œä½†é¸æ“‡ä¸åŒçš„çµ„åˆ\n",
        "        num_selected = max(int(num_features * 0.8), num_features - 2)\n",
        "        selected_features = np.random.choice(\n",
        "            feature_indices, size=num_selected, replace=False\n",
        "        )\n",
        "        selected_features.sort()\n",
        "\n",
        "        return trajectory[:, selected_features].copy()\n",
        "\n",
        "    def _add_privacy_preserving_noise(self, trajectory: np.ndarray,\n",
        "                                      base_id: int, virtual_idx: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        æ·»åŠ å·®åˆ†éš±ç§å‹å¥½çš„é«˜æ–¯å™ªè²\n",
        "        å­¸è¡“ä¾æ“š: DPè¯é‚¦å­¸ç¿’\n",
        "        \"\"\"\n",
        "        np.random.seed(200 + base_id * 10 + virtual_idx)\n",
        "\n",
        "        # æ ¹æ“šæ•¸æ“šå°ºåº¦è‡ªé©æ‡‰å™ªè²\n",
        "        if trajectory.ndim == 1:\n",
        "            data_std = np.std(trajectory)\n",
        "            noise_scale = self.config.noise_injection_std * data_std\n",
        "            noise = np.random.normal(0, noise_scale, trajectory.shape)\n",
        "        else:\n",
        "            data_std = np.std(trajectory, axis=0)\n",
        "            noise_scale = self.config.noise_injection_std * data_std\n",
        "            noise = np.random.normal(0, noise_scale, trajectory.shape)\n",
        "\n",
        "        # ç¢ºä¿å™ªè²ä¸æœƒç ´å£æ•¸æ“šçš„åŸºæœ¬ç‰¹æ€§\n",
        "        noise_magnitude = np.abs(noise).max()\n",
        "        data_magnitude = np.abs(trajectory).max()\n",
        "\n",
        "        if noise_magnitude > 0.1 * data_magnitude:  # é™åˆ¶å™ªè²å¹…åº¦\n",
        "            noise = noise * (0.1 * data_magnitude / noise_magnitude)\n",
        "\n",
        "        return trajectory + noise\n",
        "\n",
        "    def _feature_augmentation(self, trajectory: np.ndarray,\n",
        "                              base_id: int, virtual_idx: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        ç‰¹å¾µå¢å¼· - å­¸è¡“ä¾æ“š: FedRDN\n",
        "\n",
        "        é€šéçµ±è¨ˆç‰¹å¾µè®Šæ›å¢å¼·æ•¸æ“šå¤šæ¨£æ€§\n",
        "        \"\"\"\n",
        "        if trajectory.size == 0:\n",
        "            return trajectory\n",
        "\n",
        "        augmented_traj = trajectory.copy()\n",
        "\n",
        "        # 1. æ™‚é–“å¹³ç§»ï¼ˆæ¨¡æ“¬ä¸åŒèµ·å§‹æ™‚é–“ï¼‰\n",
        "        shift_amount = virtual_idx * 3  # é©åº¦çš„æ™‚é–“å¹³ç§»\n",
        "        if len(augmented_traj) > shift_amount:\n",
        "            augmented_traj = np.roll(augmented_traj, shift_amount, axis=0)\n",
        "\n",
        "        # 2. è¼•å¾®çš„å°ºåº¦è®Šæ›ï¼ˆæ¨¡æ“¬ä¸åŒç³»çµ±æ¢ä»¶ï¼‰\n",
        "        scale_factor = 1.0 + (virtual_idx - 1) * 0.015  # Â±1.5%çš„è®ŠåŒ–\n",
        "        augmented_traj = augmented_traj * scale_factor\n",
        "\n",
        "        # 3. æ·»åŠ å­£ç¯€æ€§è®ŠåŒ–ï¼ˆæ¨¡æ“¬ä¸åŒæ™‚é–“æ®µçš„ç‰¹æ€§ï¼‰\n",
        "        if len(augmented_traj) > 10:\n",
        "            seasonal_period = max(10, len(augmented_traj) // 5)\n",
        "            seasonal_component = 0.02 * np.sin(\n",
        "                2 * np.pi * np.arange(len(augmented_traj)) / seasonal_period + virtual_idx\n",
        "            )\n",
        "            if augmented_traj.ndim == 1:\n",
        "                augmented_traj += seasonal_component\n",
        "            else:\n",
        "                augmented_traj[:, 0] += seasonal_component\n",
        "\n",
        "        return augmented_traj\n",
        "\n",
        "    def _validate_trajectory(self, trajectory: np.ndarray) -> bool:\n",
        "        \"\"\"\n",
        "        é©—è­‰ç”Ÿæˆçš„è™›æ“¬è»Œè·¡æ˜¯å¦æœ‰æ•ˆ\n",
        "        \"\"\"\n",
        "        if trajectory.size == 0:\n",
        "            return False\n",
        "\n",
        "        # æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§\n",
        "        if np.any(np.isnan(trajectory)) or np.any(np.isinf(trajectory)):\n",
        "            return False\n",
        "\n",
        "        # æª¢æŸ¥æœ€å°æ•¸æ“šé‡\n",
        "        min_required_length = max(50, self.config.batch_size * 2)\n",
        "        if len(trajectory) < min_required_length:\n",
        "            return False\n",
        "\n",
        "        # æª¢æŸ¥æ•¸æ“šè®Šç•°æ€§\n",
        "        if trajectory.ndim == 1:\n",
        "            if np.std(trajectory) < 1e-6:  # æ•¸æ“šéæ–¼å–®èª¿\n",
        "                return False\n",
        "        else:\n",
        "            if np.any(np.std(trajectory, axis=0) < 1e-6):\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def get_virtual_client_info(self) -> Dict:\n",
        "        \"\"\"è¿”å›è™›æ“¬å®¢æˆ¶ç«¯ç”Ÿæˆçš„è©³ç´°è³‡è¨Š\"\"\"\n",
        "        return {\n",
        "            'base_clients': self.generation_stats['real_clients'],\n",
        "            'virtual_expansion_factor': self.config.virtual_expansion_factor,\n",
        "            'virtual_clients_generated': self.generation_stats['virtual_clients'],\n",
        "            'failed_generations': self.generation_stats['failed_generations'],\n",
        "            'total_clients': self.generation_stats['real_clients'] + self.generation_stats['virtual_clients'],\n",
        "            'total_data_points': self.generation_stats['total_data_points'],\n",
        "            'split_method': self.config.temporal_split_method,\n",
        "            'noise_std': self.config.noise_injection_std,\n",
        "            'feature_augmentation': self.config.feature_augmentation,\n",
        "            'generation_success_rate': self.generation_stats['virtual_clients'] / (self.generation_stats['virtual_clients'] + self.generation_stats['failed_generations']) if (self.generation_stats['virtual_clients'] + self.generation_stats['failed_generations']) > 0 else 0\n",
        "        }\n",
        "\n",
        "print(\"âœ… Cell 6.5: VirtualClientGeneratorï¼ˆå­¸è¡“åˆç†ç‰ˆï¼‰å®šç¾©å®Œæˆã€‚\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0yvrVAOIQM5",
        "outputId": "74ba5005-057b-4ac4-f778-92b742cb0b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 7: ExperimentRunnerï¼ˆæ­»å¾ªç’°ä¿®æ­£ç‰ˆï¼‰å®šç¾©å®Œæˆã€‚\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 7: ğŸš€ ExperimentRunnerï¼ˆæ­»å¾ªç’°ä¿®æ­£ç‰ˆï¼‰\n",
        "import scipy.stats as stats\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "class ExperimentRunner:\n",
        "    def __init__(self, config: TrainingConfig, data_manager: DataManager, all_trajectories, client_pairs):\n",
        "        self.config, self.data_manager, self.server = config, data_manager, FLServer(config)\n",
        "        self.training_history, self.evaluation_results, self.privacy_costs = [], [], []\n",
        "\n",
        "        # ã€ä¿®æ­£æ­»å¾ªç’°ã€‘éš±ç§é ç®—ç®¡ç†å¢å¼·\n",
        "        self.total_privacy_budget = config.dp_target_epsilon if config.enable_dp else 0.0\n",
        "        self.consumed_privacy_budget = 0.0\n",
        "        self.privacy_budget_exceeded = False\n",
        "        self.dp_reset_history = []\n",
        "        self.round_privacy_costs = []\n",
        "        self.detailed_privacy_logs = []\n",
        "\n",
        "        # ã€æ–°å¢ã€‘æ­»å¾ªç’°é˜²è­·æ©Ÿåˆ¶\n",
        "        self.max_resets_per_round = 1  # æ¯è¼ªæœ€å¤šé‡è¨­3æ¬¡\n",
        "        self.current_round_resets = 0\n",
        "        self.consecutive_reset_rounds = 0\n",
        "        self.max_consecutive_resets = 2  # æœ€å¤šé€£çºŒ5è¼ªé‡è¨­\n",
        "\n",
        "        self._set_seeds()\n",
        "        # å…¶ä»–åˆå§‹åŒ–é‚è¼¯ä¿æŒä¸è®Š...\n",
        "        print(\"\\n[ExperimentRunner] æ­£åœ¨åˆå§‹åŒ–å®¢æˆ¶ç«¯ç’°å¢ƒèˆ‡ä»£ç†...\")\n",
        "\n",
        "        if self.config.enable_dp and self.config.mode != 'Centralized':\n",
        "            print(f\"ğŸ›¡ï¸ å·®åˆ†éš±ç§æ¨¡å¼å•Ÿç”¨ï¼ˆæ­»å¾ªç’°é˜²è­·ç‰ˆï¼‰ï¼š\")\n",
        "            print(f\"   - ç¸½éš±ç§é ç®—ä¸Šé™: Îµ={self.total_privacy_budget}\")\n",
        "            print(f\"   - é‡è¨­é–¾å€¼: Îµ={self.total_privacy_budget * self.config.dp_reset_threshold_multiplier}\")\n",
        "            print(f\"   - æ­»å¾ªç’°é˜²è­·: æ¯è¼ªæœ€å¤š{self.max_resets_per_round}æ¬¡é‡è¨­\")\n",
        "            print(f\"   - å®¢æˆ¶ç«¯æ•¸é‡: {self.config.num_clients}\")\n",
        "\n",
        "        # åˆå§‹åŒ–å®¢æˆ¶ç«¯ç’°å¢ƒï¼ˆä¿æŒä¸è®Šï¼‰\n",
        "        self.client_envs = {cid: PairedEnv(traj, config) for cid, traj in all_trajectories.items() if traj.size > 0}\n",
        "        if not self.client_envs:\n",
        "            raise ValueError(\"DataManager æœªèƒ½ç‚ºä»»ä½•å®¢æˆ¶ç«¯å‰µå»ºæœ‰æ•ˆçš„ç’°å¢ƒã€‚\")\n",
        "\n",
        "        self.config.num_clients = len(self.client_envs)\n",
        "\n",
        "        if self.config.mode == \"Centralized\":\n",
        "            central_config = copy.deepcopy(config)\n",
        "            central_config.enable_dp = False\n",
        "            pooled_trajectory = np.vstack([traj for traj in all_trajectories.values() if traj.size > 0])\n",
        "            self.central_env = PairedEnv(pooled_trajectory, central_config)\n",
        "            self.central_agent = RLAgent(self.central_env.state_size, self.central_env.action_size, central_config, 0, len(pooled_trajectory), False)\n",
        "            self.client_agents = {}\n",
        "        else:\n",
        "            self.client_agents = {}\n",
        "            for cid, env in self.client_envs.items():\n",
        "                dataset_size = len(env.trajectory) if env.trajectory.size > 0 else 1\n",
        "                self.client_agents[cid] = RLAgent(env.state_size, env.action_size, config, cid, dataset_size, False)\n",
        "\n",
        "        if self.client_agents:\n",
        "            self.global_model_state = self.client_agents[next(iter(self.client_agents))].get_clean_state_dict()\n",
        "        else:\n",
        "            self.global_model_state = self.central_agent.get_clean_state_dict()\n",
        "\n",
        "        self.config.save()\n",
        "        print(\"[ExperimentRunner] åˆå§‹åŒ–å®Œæˆã€‚\")\n",
        "\n",
        "    def _set_seeds(self):\n",
        "        seed = self.config.random_seed\n",
        "        torch.manual_seed(seed); torch.cuda.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "        np.random.seed(seed); random.seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    def _check_privacy_budget_and_reset(self, round_privacy_costs, current_round):\n",
        "        \"\"\"ã€ä¿®æ­£æ­»å¾ªç’°ã€‘éš±ç§é ç®—æª¢æŸ¥èˆ‡æ™ºèƒ½é‡è¨­æ©Ÿåˆ¶\"\"\"\n",
        "        if not self.config.enable_dp or self.config.mode == 'Centralized':\n",
        "            return\n",
        "\n",
        "        # ã€æ–°å¢ã€‘é‡è¨­è¼ªæ¬¡é–‹å§‹æ™‚é‡ç½®è¨ˆæ•¸å™¨\n",
        "        if current_round != getattr(self, '_last_checked_round', -1):\n",
        "            self.current_round_resets = 0\n",
        "            self._last_checked_round = current_round\n",
        "\n",
        "        # æ”¶é›†è©³ç´°éš±ç§è³‡è¨Š\n",
        "        round_detailed_info = []\n",
        "        for cid, agent in self.client_agents.items():\n",
        "            detailed_info = agent.get_privacy_detailed_info()\n",
        "            detailed_info['round'] = current_round\n",
        "            round_detailed_info.append(detailed_info)\n",
        "        self.detailed_privacy_logs.extend(round_detailed_info)\n",
        "\n",
        "        if round_privacy_costs:\n",
        "            # ä½¿ç”¨å¹³å‡å€¼é¿å…é‡è¤‡è¨ˆå¸³\n",
        "            round_avg_epsilon = np.mean(round_privacy_costs)\n",
        "            round_max_epsilon = np.max(round_privacy_costs)\n",
        "            round_min_epsilon = np.min(round_privacy_costs)\n",
        "\n",
        "            self.consumed_privacy_budget += round_avg_epsilon\n",
        "\n",
        "            # è©³ç´°çš„éš±ç§æˆæœ¬è¨˜éŒ„\n",
        "            self.round_privacy_costs.append({\n",
        "                'round': current_round,\n",
        "                'avg_epsilon': round_avg_epsilon,\n",
        "                'max_epsilon': round_max_epsilon,\n",
        "                'min_epsilon': round_min_epsilon,\n",
        "                'std_epsilon': np.std(round_privacy_costs),\n",
        "                'participating_clients': len(round_privacy_costs),\n",
        "                'cumulative_epsilon': self.consumed_privacy_budget,\n",
        "                'round_resets': self.current_round_resets\n",
        "            })\n",
        "\n",
        "            budget_ratio = self.consumed_privacy_budget / self.total_privacy_budget\n",
        "            reset_threshold = self.total_privacy_budget * self.config.dp_reset_threshold_multiplier\n",
        "\n",
        "            # æª¢æŸ¥æ˜¯å¦é¦–æ¬¡è¶…æ”¯\n",
        "            if budget_ratio > 1.0 and not self.privacy_budget_exceeded:\n",
        "                print(f\"\\n{'='*20} âš ï¸ éš±ç§é ç®—é¦–æ¬¡è¶…æ”¯ï¼ {'='*20}\")\n",
        "                print(f\"   - ç•¶å‰æ¶ˆè€—: Îµ = {self.consumed_privacy_budget:.4f}\")\n",
        "                print(f\"   - é ç®—ä¸Šé™: Îµ = {self.total_privacy_budget}\")\n",
        "                print(f\"   - æœ¬è¼ªçµ±è¨ˆ: å¹³å‡={round_avg_epsilon:.4f}, æœ€å¤§={round_max_epsilon:.4f}\")\n",
        "                print(f\"{'='*58}\")\n",
        "                self.privacy_budget_exceeded = True\n",
        "\n",
        "            # ã€é—œéµä¿®æ­£ã€‘æ™ºèƒ½é‡è¨­æ©Ÿåˆ¶ï¼Œé˜²æ­¢æ­»å¾ªç’°\n",
        "            should_reset = (\n",
        "                self.consumed_privacy_budget > reset_threshold and\n",
        "                self.config.enable_dp_reset and\n",
        "                self.current_round_resets < self.max_resets_per_round and  # é™åˆ¶æ¯è¼ªé‡è¨­æ¬¡æ•¸\n",
        "                self.consecutive_reset_rounds < self.max_consecutive_resets  # é™åˆ¶é€£çºŒé‡è¨­è¼ªæ•¸\n",
        "            )\n",
        "\n",
        "            if should_reset:\n",
        "                print(f\"\\nğŸ”„ è§¸ç™¼éš±ç§é ç®—é‡è¨­æ©Ÿåˆ¶ï¼ˆRound {current_round}ï¼‰\")\n",
        "                print(f\"   - è§¸ç™¼æ¢ä»¶: {self.consumed_privacy_budget:.4f} > {reset_threshold:.4f}\")\n",
        "                print(f\"   - æœ¬è¼ªé‡è¨­æ¬¡æ•¸: {self.current_round_resets}/{self.max_resets_per_round}\")\n",
        "                print(f\"   - é€£çºŒé‡è¨­è¼ªæ•¸: {self.consecutive_reset_rounds}/{self.max_consecutive_resets}\")\n",
        "\n",
        "                reset_count = 0\n",
        "                successful_resets = []\n",
        "                reset_failures = []\n",
        "\n",
        "                for cid, agent in self.client_agents.items():\n",
        "                    if agent.reset_dp_engine(current_round):\n",
        "                        reset_count += 1\n",
        "                        successful_resets.append(cid)\n",
        "                    else:\n",
        "                        reset_failures.append(cid)\n",
        "\n",
        "                if reset_count > 0:\n",
        "                    self.current_round_resets += 1\n",
        "                    self.consecutive_reset_rounds += 1\n",
        "\n",
        "                    self.dp_reset_history.append({\n",
        "                        'round': current_round,\n",
        "                        'reset_count': reset_count,\n",
        "                        'total_clients': len(self.client_agents),\n",
        "                        'successful_resets': successful_resets,\n",
        "                        'failed_resets': reset_failures,\n",
        "                        'budget_before_reset': self.consumed_privacy_budget,\n",
        "                        'trigger_threshold': reset_threshold,\n",
        "                        'round_reset_number': self.current_round_resets\n",
        "                    })\n",
        "\n",
        "                    # é‡è¨­ç³»çµ±é ç®—è¨ˆæ•¸\n",
        "                    self.consumed_privacy_budget = 0.0\n",
        "                    self.privacy_budget_exceeded = False\n",
        "\n",
        "                    print(f\"   - âœ… é‡è¨­å®Œæˆï¼š{reset_count}/{len(self.client_agents)}å€‹å®¢æˆ¶ç«¯\")\n",
        "                    print(f\"   - æœ¬è¼ªé‡è¨­è¨ˆæ•¸ï¼š{self.current_round_resets}\")\n",
        "                    if reset_failures:\n",
        "                        print(f\"   - âš ï¸ é‡è¨­å¤±æ•—çš„å®¢æˆ¶ç«¯: {reset_failures}\")\n",
        "\n",
        "            elif self.consumed_privacy_budget > reset_threshold and self.config.enable_dp_reset:\n",
        "                # é”åˆ°é‡è¨­é™åˆ¶çš„æƒ…æ³\n",
        "                if self.current_round_resets >= self.max_resets_per_round:\n",
        "                    print(f\"\\nâš ï¸ æœ¬è¼ªé‡è¨­æ¬¡æ•¸å·²é”ä¸Šé™ ({self.max_resets_per_round})ï¼Œè·³éé‡è¨­\")\n",
        "                if self.consecutive_reset_rounds >= self.max_consecutive_resets:\n",
        "                    print(f\"\\nâš ï¸ é€£çºŒé‡è¨­è¼ªæ•¸å·²é”ä¸Šé™ ({self.max_consecutive_resets})ï¼Œåœç”¨é‡è¨­\")\n",
        "                    self.config.enable_dp_reset = False  # æš«æ™‚åœç”¨é‡è¨­\n",
        "            else:\n",
        "                # æ²’æœ‰è§¸ç™¼é‡è¨­ï¼Œé‡ç½®é€£çºŒé‡è¨­è¨ˆæ•¸\n",
        "                self.consecutive_reset_rounds = 0\n",
        "\n",
        "    def _train_agent_locally(self, agent: RLAgent, env: PairedEnv, episodes: int, is_finetune: bool = False):\n",
        "        \"\"\"ã€ä¿®æ­£ã€‘è¨“ç·´é‚è¼¯ï¼Œå¢åŠ æ­¥æ•¸ç›£æ§\"\"\"\n",
        "        agent.model.train()\n",
        "        total_loss, total_reward, training_steps, episode_count = 0.0, 0.0, 0, 0\n",
        "\n",
        "        if episodes == 0:\n",
        "            return 0.0, 0.0, 0.0\n",
        "\n",
        "        # ã€æ–°å¢ã€‘è¨“ç·´æ­¥æ•¸ç›£æ§\n",
        "        initial_dp_steps = getattr(agent, 'dp_steps', 0)\n",
        "        max_steps_per_round = self.config.local_episodes_per_round * self.config.steps_per_episode * 2  # è¨­å®šä¸Šé™\n",
        "\n",
        "        for episode in range(episodes):\n",
        "            state, episode_reward, done = env.reset(), 0.0, False\n",
        "            for step in range(1, self.config.steps_per_episode + 1):\n",
        "                action = agent.act(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                agent.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "\n",
        "                if len(agent.memory) > self.config.replay_start_size and step % self.config.replay_frequency == 0:\n",
        "                    loss = agent.replay(num_batches=self.config.replay_batches_per_call)\n",
        "                    total_loss += loss\n",
        "                    training_steps += 1\n",
        "\n",
        "                    # ã€æ–°å¢ã€‘æ­¥æ•¸ç›£æ§ï¼Œé˜²æ­¢ç„¡é™è¨“ç·´\n",
        "                    current_dp_steps = getattr(agent, 'dp_steps', 0)\n",
        "                    if current_dp_steps - initial_dp_steps > max_steps_per_round:\n",
        "                        print(f\"[C-{agent.client_id}] âš ï¸ è¨“ç·´æ­¥æ•¸è¶…é™ ({current_dp_steps - initial_dp_steps}>{max_steps_per_round})ï¼Œæå‰çµæŸ\")\n",
        "                        break\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            total_reward += episode_reward\n",
        "            episode_count += 1\n",
        "            if (episode + 1) % self.config.target_update_freq == 0:\n",
        "                agent.update_target_model()\n",
        "\n",
        "        if not agent.is_eval_agent and not is_finetune and agent.epsilon > self.config.epsilon_min:\n",
        "            agent.epsilon *= self.config.epsilon_decay\n",
        "\n",
        "        avg_loss = total_loss / training_steps if training_steps > 0 else 0.0\n",
        "        avg_reward = total_reward / episode_count if episode_count > 0 else 0.0\n",
        "        privacy_cost = agent.get_privacy_cost() if training_steps > 0 and self.config.enable_dp and agent.privacy_engine else 0.0\n",
        "\n",
        "        return avg_loss, avg_reward, privacy_cost\n",
        "\n",
        "    def _run_federated_training(self):\n",
        "        \"\"\"ã€ä¿®æ­£ã€‘è¯é‚¦è¨“ç·´ä¸»æµç¨‹ï¼Œå¢åŠ èª¿è©¦ä¿¡æ¯\"\"\"\n",
        "        print(f\"\\n[æ¨¡å¼] åŸ·è¡Œè¯é‚¦å¼è¨“ç·´ ({self.config.mode})\")\n",
        "        available_client_ids = list(self.client_agents.keys())\n",
        "        progress_bar = tqdm(range(self.config.comm_rounds), desc=f\"{self.config.mode} Training\")\n",
        "\n",
        "        for comm_round in progress_bar:\n",
        "            print(f\"\\n--- é–‹å§‹ Round {comm_round+1}/{self.config.comm_rounds} ---\")\n",
        "\n",
        "            # èšé¡æ›´æ–°\n",
        "            if (self.config.mode == 'ClusteredFL' and comm_round > 0 and\n",
        "                comm_round % self.config.cluster_update_freq == 0):\n",
        "                self.server.update_clusters(self.client_agents, comm_round)\n",
        "\n",
        "            # å®¢æˆ¶ç«¯é¸æ“‡\n",
        "            num_to_select = min(self.config.num_clients_to_select, len(available_client_ids))\n",
        "            selected_ids = np.random.choice(available_client_ids, num_to_select, replace=False)\n",
        "            participating_ids = list(selected_ids)\n",
        "            straggler_ids = set()\n",
        "\n",
        "            # ç•°è³ªæ€§æ¨¡æ“¬\n",
        "            if self.config.enable_heterogeneity and len(participating_ids) > 1:\n",
        "                num_dropouts = int(self.config.dropout_ratio * len(participating_ids))\n",
        "                if num_dropouts > 0 and len(participating_ids) > num_dropouts:\n",
        "                    dropout_ids = set(np.random.choice(participating_ids, num_dropouts, replace=False))\n",
        "                    participating_ids = [cid for cid in participating_ids if cid not in dropout_ids]\n",
        "\n",
        "                if participating_ids and len(participating_ids) > 1:\n",
        "                    num_stragglers = int(self.config.straggler_ratio * len(participating_ids))\n",
        "                    if num_stragglers > 0:\n",
        "                        straggler_ids = set(np.random.choice(participating_ids, num_stragglers, replace=False))\n",
        "\n",
        "            if not participating_ids:\n",
        "                continue\n",
        "\n",
        "            print(f\"åƒèˆ‡å®¢æˆ¶ç«¯: {participating_ids}\")\n",
        "\n",
        "            participating_agents = {cid: self.client_agents[cid] for cid in participating_ids}\n",
        "            self.server.distribute_model(participating_agents, self.global_model_state)\n",
        "\n",
        "            # æœ¬åœ°è¨“ç·´ä¸¦æ”¶é›†éš±ç§æˆæœ¬\n",
        "            client_updates, round_losses, round_rewards, round_privacy_costs = [], [], [], []\n",
        "            for cid in participating_ids:\n",
        "                agent, env = self.client_agents[cid], self.client_envs[cid]\n",
        "                episodes = (self.config.local_episodes_per_round // 2 if cid in straggler_ids\n",
        "                           else self.config.local_episodes_per_round)\n",
        "\n",
        "                print(f\"[C-{cid}] é–‹å§‹è¨“ç·´ ({episodes} episodes)...\")\n",
        "                loss, reward, privacy_cost = self._train_agent_locally(agent, env, episodes)\n",
        "                print(f\"[C-{cid}] å®Œæˆ - Loss: {loss:.4f}, Reward: {reward:.4f}, Îµ: {privacy_cost:.4f}\")\n",
        "\n",
        "                client_updates.append((agent.get_model_for_upload(), len(env.trajectory)))\n",
        "                round_losses.append(loss); round_rewards.append(reward)\n",
        "                if self.config.enable_dp and privacy_cost > 0:\n",
        "                    round_privacy_costs.append(privacy_cost)\n",
        "\n",
        "            # éš±ç§é ç®—æª¢æŸ¥èˆ‡é‡è¨­\n",
        "            print(f\"æª¢æŸ¥éš±ç§é ç®—...\")\n",
        "            self._check_privacy_budget_and_reset(round_privacy_costs, comm_round)\n",
        "\n",
        "            # æ¨¡å‹èšåˆ\n",
        "            if self.config.mode == 'ClusteredFL':\n",
        "                client_updates_by_cluster = {i: [] for i in range(self.config.num_clusters)}\n",
        "                for i, (model_update, num_points) in enumerate(client_updates):\n",
        "                    cid = participating_ids[i]\n",
        "                    cluster_id = self.server.client_to_cluster.get(cid, 0)\n",
        "                    client_updates_by_cluster[cluster_id].append((model_update, num_points))\n",
        "\n",
        "                new_cluster_models = []\n",
        "                for cluster_id, updates in client_updates_by_cluster.items():\n",
        "                    if updates:\n",
        "                        updated_cluster_model = self.server.aggregate_weighted(updates)\n",
        "                        self.server.cluster_models[cluster_id] = updated_cluster_model\n",
        "                        new_cluster_models.append((updated_cluster_model, sum(n for _, n in updates)))\n",
        "                if new_cluster_models:\n",
        "                    self.global_model_state = self.server.aggregate_weighted(new_cluster_models)\n",
        "            else:\n",
        "                if client_updates:\n",
        "                    self.global_model_state = self.server.aggregate_weighted(client_updates)\n",
        "\n",
        "            # è¨˜éŒ„è¨“ç·´æ­·å²\n",
        "            avg_reward = np.mean(round_rewards) if round_rewards else 0\n",
        "            avg_loss = np.mean(round_losses) if round_losses else 0\n",
        "            self.training_history.append({'round': comm_round, 'avg_reward': avg_reward, 'avg_loss': avg_loss})\n",
        "\n",
        "            # æ›´è©³ç´°çš„éš±ç§æˆæœ¬è¨˜éŒ„\n",
        "            if self.config.enable_dp:\n",
        "                if round_privacy_costs:\n",
        "                    epsilon_stats = {\n",
        "                        'round': comm_round,\n",
        "                        'epsilon': np.mean(round_privacy_costs),\n",
        "                        'epsilon_max': np.max(round_privacy_costs),\n",
        "                        'epsilon_min': np.min(round_privacy_costs),\n",
        "                        'epsilon_std': np.std(round_privacy_costs),\n",
        "                        'cumulative_epsilon': self.consumed_privacy_budget,\n",
        "                        'budget_ratio': self.consumed_privacy_budget / self.total_privacy_budget,\n",
        "                        'reset_count': len(self.dp_reset_history),\n",
        "                        'participating_clients': len(round_privacy_costs)\n",
        "                    }\n",
        "                else:\n",
        "                    epsilon_stats = {\n",
        "                        'round': comm_round,\n",
        "                        'epsilon': 0.0, 'epsilon_max': 0.0, 'epsilon_min': 0.0, 'epsilon_std': 0.0,\n",
        "                        'cumulative_epsilon': self.consumed_privacy_budget,\n",
        "                        'budget_ratio': self.consumed_privacy_budget / self.total_privacy_budget,\n",
        "                        'reset_count': len(self.dp_reset_history),\n",
        "                        'participating_clients': 0\n",
        "                    }\n",
        "                self.privacy_costs.append(epsilon_stats)\n",
        "\n",
        "            # é€²åº¦æ¢æ›´æ–°\n",
        "            postfix = {'reward': f\"{avg_reward:.2f}\", 'loss': f\"{avg_loss:.4f}\"}\n",
        "            if self.config.enable_dp and self.consumed_privacy_budget > 0:\n",
        "                postfix['Îµ_used'] = f\"{self.consumed_privacy_budget:.3f}\"\n",
        "                postfix['resets'] = str(len(self.dp_reset_history))\n",
        "            progress_bar.set_postfix(postfix)\n",
        "\n",
        "            print(f\"Round {comm_round+1} å®Œæˆ - {postfix}\")\n",
        "\n",
        "    # å…¶ä»–æ–¹æ³•ä¿æŒä¸è®Šï¼ˆå¤ªé•·äº†ï¼Œé€™è£¡çœç•¥ï¼‰...\n",
        "    def _evaluate_agent(self, env, model_state, num_episodes=15):\n",
        "        if env.trajectory.size == 0: return 0.0\n",
        "        eval_config = copy.deepcopy(self.config); eval_config.enable_dp = False\n",
        "        eval_agent = RLAgent(env.state_size, env.action_size, eval_config, -1, 1, True)\n",
        "        eval_agent.model.load_state_dict(model_state); eval_agent.model.eval(); eval_agent.epsilon = 0.0\n",
        "        total_reward = 0\n",
        "        for _ in range(num_episodes):\n",
        "            state, episode_reward, done = env.reset(), 0, False\n",
        "            for _ in range(self.config.steps_per_episode):\n",
        "                action = eval_agent.act(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                episode_reward += reward; state = next_state\n",
        "                if done: break\n",
        "            total_reward += episode_reward\n",
        "        return total_reward / num_episodes\n",
        "\n",
        "    def _run_centralized_training(self):\n",
        "        print(f\"\\n[æ¨¡å¼] åŸ·è¡Œé›†ä¸­å¼è¨“ç·´ (Centralized)\")\n",
        "        progress_bar = tqdm(range(self.config.comm_rounds), desc=\"Centralized Training\")\n",
        "        num_clients_per_round = min(self.config.num_clients_to_select, self.config.num_clients)\n",
        "        equivalent_episodes = self.config.local_episodes_per_round * num_clients_per_round\n",
        "        for r in progress_bar:\n",
        "            loss, reward, _ = self._train_agent_locally(self.central_agent, self.central_env, episodes=equivalent_episodes)\n",
        "            self.training_history.append({'round': r, 'avg_reward': reward, 'avg_loss': loss})\n",
        "            self.privacy_costs.append({'round': r, 'epsilon': 0.0, 'cumulative_epsilon': 0.0, 'budget_ratio': 0.0})\n",
        "            progress_bar.set_postfix(reward=f\"{reward:.2f}\", loss=f\"{loss:.4f}\")\n",
        "        self.global_model_state = self.central_agent.get_clean_state_dict()\n",
        "\n",
        "    def _run_isolated_training(self):\n",
        "        print(f\"\\n[æ¨¡å¼] åŸ·è¡Œå­¤ç«‹å¼è¨“ç·´ (Isolated)\")\n",
        "        progress_bar = tqdm(range(self.config.comm_rounds), desc=\"Isolated Training Rounds\")\n",
        "        num_clients_per_round = min(self.config.num_clients_to_select, self.config.num_clients)\n",
        "        equivalent_episodes_per_client = int(np.ceil((self.config.local_episodes_per_round * num_clients_per_round) / self.config.num_clients))\n",
        "\n",
        "        for r in progress_bar:\n",
        "            round_rewards, round_losses, round_epsilons = [], [], []\n",
        "            for cid, agent in self.client_agents.items():\n",
        "                env = self.client_envs[cid]\n",
        "                loss, reward, privacy_cost = self._train_agent_locally(agent, env, episodes=equivalent_episodes_per_client)\n",
        "                round_rewards.append(reward); round_losses.append(loss)\n",
        "                if self.config.enable_dp and privacy_cost > 0:\n",
        "                    round_epsilons.append(privacy_cost)\n",
        "\n",
        "            avg_reward = np.mean(round_rewards) if round_rewards else np.nan\n",
        "            avg_loss = np.mean(round_losses) if round_losses else np.nan\n",
        "            self.training_history.append({'round': r, 'avg_reward': avg_reward, 'avg_loss': avg_loss})\n",
        "\n",
        "            if self.config.enable_dp and round_epsilons:\n",
        "                avg_epsilon = np.mean(round_epsilons)\n",
        "                self.consumed_privacy_budget += avg_epsilon\n",
        "                self.privacy_costs.append({\n",
        "                    'round': r, 'epsilon': avg_epsilon,\n",
        "                    'cumulative_epsilon': self.consumed_privacy_budget,\n",
        "                    'budget_ratio': self.consumed_privacy_budget / self.total_privacy_budget\n",
        "                })\n",
        "            else:\n",
        "                self.privacy_costs.append({\n",
        "                    'round': r, 'epsilon': 0.0, 'cumulative_epsilon': self.consumed_privacy_budget,\n",
        "                    'budget_ratio': self.consumed_privacy_budget / self.total_privacy_budget\n",
        "                })\n",
        "\n",
        "            postfix = {'reward': f\"{avg_reward:.2f}\" if not np.isnan(avg_reward) else \"NaN\",\n",
        "                      'loss': f\"{avg_loss:.4f}\" if not np.isnan(avg_loss) else \"NaN\"}\n",
        "            if self.config.enable_dp and self.consumed_privacy_budget > 0:\n",
        "                postfix['Îµ_used'] = f\"{self.consumed_privacy_budget:.3f}\"\n",
        "            progress_bar.set_postfix(postfix)\n",
        "\n",
        "    def _run_final_evaluation_and_pfl(self):\n",
        "        print(\"\\n[è©•ä¼°] æ­£åœ¨åŸ·è¡Œæœ€çµ‚è©•ä¼°...\")\n",
        "        final_model_path = os.path.join(self.config.output_dir, f'{self.config.experiment_name}_global_model.pt')\n",
        "        if self.global_model_state:\n",
        "            torch.save(self.global_model_state, final_model_path)\n",
        "\n",
        "        for cid, env in tqdm(self.client_envs.items(), desc=\"æœ€çµ‚è©•ä¼°\"):\n",
        "            eval_row = {'client_id': cid}\n",
        "            seed = self.config.random_seed + cid\n",
        "\n",
        "            if self.config.mode == \"Isolated\":\n",
        "                base_model_state = self.client_agents[cid].get_clean_state_dict()\n",
        "                personalized_model_state = base_model_state\n",
        "            else:\n",
        "                base_model_state = self.global_model_state\n",
        "                personalized_model_state = base_model_state\n",
        "                if self.config.mode == 'ClusteredFL':\n",
        "                    cluster_id = self.server.client_to_cluster.get(cid)\n",
        "                    if cluster_id is not None and cluster_id in self.server.cluster_models:\n",
        "                        personalized_model_state = self.server.cluster_models[cluster_id]\n",
        "\n",
        "            torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n",
        "            eval_row['reward_global'] = self._evaluate_agent(env, base_model_state)\n",
        "\n",
        "            if personalized_model_state is base_model_state:\n",
        "                eval_row['reward_personalized'] = eval_row['reward_global']\n",
        "            else:\n",
        "                torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n",
        "                eval_row['reward_personalized'] = self._evaluate_agent(env, personalized_model_state)\n",
        "\n",
        "            if self.config.use_pfl_finetune:\n",
        "                finetune_config = copy.deepcopy(self.config); finetune_config.enable_dp = False\n",
        "                finetune_agent = RLAgent(env.state_size, env.action_size, finetune_config, cid, len(env.trajectory), False)\n",
        "                finetune_agent.epsilon = 0.01; finetune_agent.model.load_state_dict(personalized_model_state)\n",
        "                self._train_agent_locally(finetune_agent, env, self.config.local_finetune_episodes, True)\n",
        "                torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n",
        "                finetuned_model_state = finetune_agent.get_clean_state_dict()\n",
        "                eval_row['reward_pfl_finetuned'] = self._evaluate_agent(env, finetuned_model_state)\n",
        "            else:\n",
        "                eval_row['reward_pfl_finetuned'] = eval_row['reward_personalized']\n",
        "\n",
        "            self.evaluation_results.append(eval_row)\n",
        "\n",
        "    def run(self):\n",
        "        print(f\"\\n{'='*20} ğŸƒâ™‚ï¸ é–‹å§‹åŸ·è¡Œå¯¦é©—: {self.config.experiment_name} ({self.config.mode}) {'='*20}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        if self.config.mode == 'Centralized':\n",
        "            self._run_centralized_training()\n",
        "        elif self.config.mode == 'Isolated':\n",
        "            self._run_isolated_training()\n",
        "        elif self.config.mode in ['FedAvg', 'FedProx', 'ClusteredFL']:\n",
        "            self._run_federated_training()\n",
        "        else:\n",
        "            raise ValueError(f\"æœªçŸ¥çš„å¯¦é©—æ¨¡å¼: {self.config.mode}\")\n",
        "\n",
        "        self._run_final_evaluation_and_pfl()\n",
        "\n",
        "        total_time = (time.time() - start_time) / 60\n",
        "        print(f\"âœ… å¯¦é©— {self.config.experiment_name} å®Œæˆï¼ç¸½è€—æ™‚: {total_time:.2f} åˆ†é˜\")\n",
        "\n",
        "        if self.config.enable_dp and self.config.mode != 'Centralized':\n",
        "            print(f\"ğŸ›¡ï¸ æœ€çµ‚éš±ç§å ±å‘Šï¼š\")\n",
        "            print(f\"   - ç¸½æ¶ˆè€—éš±ç§é ç®—: Îµ = {self.consumed_privacy_budget:.4f}\")\n",
        "            print(f\"   - DPå¼•æ“é‡è¨­æ¬¡æ•¸: {len(self.dp_reset_history)}\")\n",
        "            if self.privacy_budget_exceeded:\n",
        "                print(f\"   - âš ï¸ éš±ç§é ç®—å·²è¶…æ”¯\")\n",
        "            else:\n",
        "                print(f\"   - âœ… éš±ç§é ç®—æ§åˆ¶è‰¯å¥½\")\n",
        "\n",
        "            if self.dp_reset_history:\n",
        "                print(f\"   - é‡è¨­æ­·å²: {[r['round'] for r in self.dp_reset_history]}\")\n",
        "\n",
        "        # ä¿å­˜çµæœ\n",
        "        if self.training_history:\n",
        "            pd.DataFrame(self.training_history).to_csv(\n",
        "                os.path.join(self.config.output_dir, f'{self.config.experiment_name}_training_history.csv'), index=False)\n",
        "        if self.evaluation_results:\n",
        "            pd.DataFrame(self.evaluation_results).to_csv(\n",
        "                os.path.join(self.config.output_dir, f'{self.config.experiment_name}_evaluation_results.csv'), index=False)\n",
        "        if self.config.enable_dp and self.privacy_costs:\n",
        "            pd.DataFrame(self.privacy_costs).to_csv(\n",
        "                os.path.join(self.config.output_dir, f'{self.config.experiment_name}_privacy_costs.csv'), index=False)\n",
        "\n",
        "        # ä¿å­˜è©³ç´°éš±ç§æ—¥èªŒ\n",
        "        if self.config.enable_dp and self.detailed_privacy_logs:\n",
        "            pd.DataFrame(self.detailed_privacy_logs).to_csv(\n",
        "                os.path.join(self.config.output_dir, f'{self.config.experiment_name}_detailed_privacy_logs.csv'), index=False)\n",
        "\n",
        "        # ä¿å­˜é‡è¨­æ­·å²\n",
        "        if self.config.enable_dp and self.dp_reset_history:\n",
        "            pd.DataFrame(self.dp_reset_history).to_csv(\n",
        "                os.path.join(self.config.output_dir, f'{self.config.experiment_name}_dp_resets.csv'), index=False)\n",
        "\n",
        "        return pd.DataFrame(self.evaluation_results), pd.DataFrame(self.training_history)\n",
        "\n",
        "print(\"âœ… Cell 7: ExperimentRunnerï¼ˆæ­»å¾ªç’°ä¿®æ­£ç‰ˆï¼‰å®šç¾©å®Œæˆã€‚\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 8.1: ğŸ¬ å¯¦é©—åŸ·è¡Œå‡½æ•¸ï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
        "import time\n",
        "import gc\n",
        "from datetime import datetime\n",
        "\n",
        "def run_virtual_client_experiment(config_dict: dict, data_path: str):\n",
        "    \"\"\"\n",
        "    é‹è¡ŒåŸºæ–¼è™›æ“¬å®¢æˆ¶ç«¯æ“´å¢çš„å¯¦é©—ï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    runner = None  # ã€ä¿®æ­£1ã€‘åˆå§‹åŒ–è®Šæ•¸\n",
        "\n",
        "    try:\n",
        "        config = TrainingConfig(**config_dict)\n",
        "        print(f\"\\n{'='*15} ğŸ”„ è™›æ“¬å®¢æˆ¶ç«¯DPå¯¦é©—: {config.experiment_name} {'='*15}\")\n",
        "\n",
        "        # 1. æº–å‚™åŸå§‹æ•¸æ“š\n",
        "        print(f\"ğŸ“Š æ­£åœ¨æº–å‚™åŸå§‹æ•¸æ“š...\")\n",
        "        data_manager = DataManager(data_path, config.base_client_pairs)\n",
        "        original_trajectories = data_manager.get_client_trajectories()\n",
        "\n",
        "        print(f\"   - åŸå§‹æ•¸æ“šæº: {len(original_trajectories)} å€‹çœŸå¯¦å®¢æˆ¶ç«¯\")\n",
        "        for cid, traj in original_trajectories.items():\n",
        "            print(f\"     å®¢æˆ¶ç«¯ {cid}: {len(traj)} æ•¸æ“šé»\")\n",
        "\n",
        "        # 2. ç”Ÿæˆè™›æ“¬å®¢æˆ¶ç«¯\n",
        "        print(f\"\\nğŸ”„ é–‹å§‹è™›æ“¬å®¢æˆ¶ç«¯ç”Ÿæˆ...\")\n",
        "        virtual_generator = VirtualClientGenerator(config)\n",
        "        all_trajectories = virtual_generator.generate_virtual_clients(original_trajectories)\n",
        "\n",
        "        # 3. ç²å–ç”Ÿæˆçµ±è¨ˆ\n",
        "        virtual_info = virtual_generator.get_virtual_client_info()\n",
        "        print(f\"\\nğŸ“ˆ è™›æ“¬å®¢æˆ¶ç«¯ç”Ÿæˆçµ±è¨ˆ:\")\n",
        "        print(f\"   - çœŸå¯¦å®¢æˆ¶ç«¯: {virtual_info['base_clients']}\")\n",
        "        print(f\"   - è™›æ“¬å®¢æˆ¶ç«¯: {virtual_info['virtual_clients_generated']}\")\n",
        "        print(f\"   - ç”Ÿæˆå¤±æ•—: {virtual_info['failed_generations']}\")\n",
        "        print(f\"   - ç¸½è¨ˆå®¢æˆ¶ç«¯: {virtual_info['total_clients']}\")\n",
        "        print(f\"   - æˆåŠŸç‡: {virtual_info['generation_success_rate']:.2%}\")\n",
        "        print(f\"   - åˆ†å‰²æ–¹æ³•: {virtual_info['split_method']}\")\n",
        "        print(f\"   - ç¸½æ•¸æ“šé»: {virtual_info['total_data_points']:,}\")\n",
        "\n",
        "        # 4. ã€ä¿®æ­£2ã€‘æ•¸æ“šè³ªé‡é©—è­‰ - æ”¹å–„numpyé™£åˆ—è™•ç†\n",
        "        print(f\"\\nğŸ” æ•¸æ“šè³ªé‡é©—è­‰:\")\n",
        "        for cid, traj in list(all_trajectories.items())[:5]:\n",
        "            if len(traj) > 0:\n",
        "                try:\n",
        "                    # å®‰å…¨çš„çµ±è¨ˆè¨ˆç®—\n",
        "                    if traj.ndim == 1:\n",
        "                        mean_val = float(np.mean(traj))\n",
        "                        std_val = float(np.std(traj))\n",
        "                        mean_str = f\"{mean_val:.4f}\"\n",
        "                        std_str = f\"{std_val:.4f}\"\n",
        "                    else:\n",
        "                        mean_val = np.mean(traj, axis=0)\n",
        "                        std_val = np.std(traj, axis=0)\n",
        "                        if mean_val.size <= 3:\n",
        "                            mean_str = \"[\" + \",\".join([f\"{float(x):.4f}\" for x in mean_val.flatten()]) + \"]\"\n",
        "                            std_str = \"[\" + \",\".join([f\"{float(x):.4f}\" for x in std_val.flatten()]) + \"]\"\n",
        "                        else:\n",
        "                            mean_str = f\"[{float(mean_val.flatten()[0]):.4f},...,{float(mean_val.flatten()[-1]):.4f}]\"\n",
        "                            std_str = f\"[{float(std_val.flatten()[0]):.4f},...,{float(std_val.flatten()[-1]):.4f}]\"\n",
        "\n",
        "                    client_type = \"çœŸå¯¦\" if cid < config.num_real_clients else \"è™›æ“¬\"\n",
        "                    print(f\"   - å®¢æˆ¶ç«¯ {cid} ({client_type}): é•·åº¦={len(traj)}, å‡å€¼={mean_str}, æ¨™å·®={std_str}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"   - å®¢æˆ¶ç«¯ {cid}: çµ±è¨ˆè¨ˆç®—å¤±æ•— - {e}\")\n",
        "\n",
        "        # 5. æ›´æ–°é…ç½®\n",
        "        config.num_clients = len(all_trajectories)\n",
        "        config.num_clients_to_select = min(config.num_clients_to_select, config.num_clients)\n",
        "\n",
        "        # 6. å‰µå»ºå…¼å®¹çš„å®¢æˆ¶ç«¯é…å°\n",
        "        extended_client_pairs = []\n",
        "        for i in range(config.num_clients):\n",
        "            embb_id = i + 1\n",
        "            urllc_id = i + 2\n",
        "            extended_client_pairs.append((embb_id, urllc_id))\n",
        "        config.client_pairs = tuple(extended_client_pairs)\n",
        "\n",
        "        # 7. å·®åˆ†éš±ç§é…ç½®æª¢æŸ¥\n",
        "        if config.enable_dp and config.mode != 'Centralized':\n",
        "            print(f\"\\nğŸ›¡ï¸ å·®åˆ†éš±ç§é…ç½®:\")\n",
        "            print(f\"   - ç›®æ¨™Îµ: {config.dp_target_epsilon}\")\n",
        "            print(f\"   - æ‰¹æ¬¡å¤§å°: {config.batch_size}\")\n",
        "            print(f\"   - å™ªè²ä¹˜æ•¸: {config.dp_noise_multiplier}\")\n",
        "            print(f\"   - å®¢æˆ¶ç«¯æ•¸é‡: {config.num_clients} (å¤§å¹…å¢åŠ )\")\n",
        "            print(f\"   - æ¯è¼ªåƒèˆ‡: {config.num_clients_to_select}\")\n",
        "            print(f\"   - é‡è¨­æ©Ÿåˆ¶: {'å•Ÿç”¨' if config.enable_dp_reset else 'ç¦ç”¨'}\")\n",
        "\n",
        "        # 8. é‹è¡Œå¯¦é©—\n",
        "        print(f\"\\nğŸš€ é–‹å§‹è¯é‚¦å­¸ç¿’å¯¦é©—...\")\n",
        "        runner = ExperimentRunner(config, data_manager, all_trajectories, config.client_pairs)\n",
        "        eval_res, history_res = runner.run()\n",
        "\n",
        "        execution_time = (time.time() - start_time) / 60\n",
        "        print(f\"\\nâ±ï¸ å¯¦é©—å®Œæˆæ™‚é–“: {execution_time:.2f} åˆ†é˜\")\n",
        "\n",
        "        # 9. ã€ä¿®æ­£3ã€‘çµæœåˆ†æ - å¢å¼·éŒ¯èª¤è™•ç†\n",
        "        print(f\"\\nâœ… å¯¦é©—çµæœæ‘˜è¦:\")\n",
        "        if eval_res is not None and not eval_res.empty and len(eval_res) > 0:\n",
        "            try:\n",
        "                avg_global_reward = float(eval_res['reward_global'].mean())\n",
        "                avg_personalized_reward = float(eval_res['reward_personalized'].mean())\n",
        "                avg_pfl_reward = float(eval_res['reward_pfl_finetuned'].mean())\n",
        "\n",
        "                print(f\"   - å¹³å‡å…¨åŸŸçå‹µ: {avg_global_reward:.4f}\")\n",
        "                print(f\"   - å¹³å‡å€‹äººåŒ–çå‹µ: {avg_personalized_reward:.4f}\")\n",
        "                print(f\"   - å¹³å‡PFLå¾®èª¿çå‹µ: {avg_pfl_reward:.4f}\")\n",
        "\n",
        "                if avg_global_reward > 0:\n",
        "                    improvement = ((avg_personalized_reward/avg_global_reward-1)*100)\n",
        "                    print(f\"   - å€‹äººåŒ–æå‡: {improvement:.2f}%\")\n",
        "            except Exception as e:\n",
        "                print(f\"   - âš ï¸ çµæœçµ±è¨ˆè¨ˆç®—å¤±æ•—: {e}\")\n",
        "                avg_global_reward = avg_personalized_reward = avg_pfl_reward = 0.0\n",
        "        else:\n",
        "            print(f\"   - âš ï¸ è©•ä¼°çµæœç‚ºç©ºï¼Œå¯èƒ½å¯¦é©—æœªå®Œæˆ\")\n",
        "            avg_global_reward = avg_personalized_reward = avg_pfl_reward = 0.0\n",
        "\n",
        "        # 10. éš±ç§åˆ†æ\n",
        "        privacy_stats = None\n",
        "        if config.enable_dp and config.mode != 'Centralized' and runner is not None:\n",
        "            consumed_budget = getattr(runner, 'consumed_privacy_budget', 0.0)\n",
        "            reset_count = len(getattr(runner, 'dp_reset_history', []))\n",
        "            budget_exceeded = getattr(runner, 'privacy_budget_exceeded', False)\n",
        "\n",
        "            print(f\"\\nğŸ›¡ï¸ è™›æ“¬å®¢æˆ¶ç«¯éš±ç§åˆ†æ:\")\n",
        "            print(f\"   - æ•¸æ“šç¨ç«‹æ€§: âœ… æ™‚é–“åˆ†å‰²ç¢ºä¿éé‡ç–Š\")\n",
        "            print(f\"   - éš±ç§ä¿è­·: âœ… DPå™ªè² (Ïƒ={config.noise_injection_std})\")\n",
        "            print(f\"   - çµ±è¨ˆçœŸå¯¦æ€§: âœ… ä¿æŒåŸå§‹åˆ†ä½ˆç‰¹æ€§\")\n",
        "            print(f\"   - æœ€çµ‚Îµæ¶ˆè€—: {consumed_budget:.4f}\")\n",
        "            print(f\"   - é ç®—ä½¿ç”¨ç‡: {(consumed_budget/config.dp_target_epsilon*100):.1f}%\")\n",
        "            print(f\"   - é‡è¨­æ¬¡æ•¸: {reset_count}\")\n",
        "\n",
        "            if budget_exceeded:\n",
        "                print(f\"   - âš ï¸ éš±ç§é ç®—å·²è¶…æ”¯ - å•Ÿç”¨é‡è¨­æ©Ÿåˆ¶ç®¡æ§\")\n",
        "            else:\n",
        "                print(f\"   - âœ… éš±ç§é ç®—æ§åˆ¶è‰¯å¥½\")\n",
        "\n",
        "            privacy_stats = {\n",
        "                'consumed_epsilon': consumed_budget,\n",
        "                'reset_count': reset_count,\n",
        "                'budget_exceeded': budget_exceeded\n",
        "            }\n",
        "\n",
        "        # 11. ä¿å­˜è©³ç´°çµæœ\n",
        "        if eval_res is not None and not eval_res.empty and len(eval_res) > 0:\n",
        "            try:\n",
        "                eval_summary = eval_res[['client_id', 'reward_global', 'reward_personalized', 'reward_pfl_finetuned']].round(4)\n",
        "                print(f\"\\nğŸ“Š è©³ç´°çµæœ (å‰10å€‹å®¢æˆ¶ç«¯):\")\n",
        "                print(eval_summary.head(10).to_string(index=False))\n",
        "            except Exception as e:\n",
        "                print(f\"   - âš ï¸ è©³ç´°çµæœé¡¯ç¤ºå¤±æ•—: {e}\")\n",
        "\n",
        "        # 12. ã€ä¿®æ­£4ã€‘å®‰å…¨çš„è³‡æºæ¸…ç†\n",
        "        try:\n",
        "            if 'data_manager' in locals():\n",
        "                del data_manager\n",
        "            if 'virtual_generator' in locals():\n",
        "                del virtual_generator\n",
        "            if 'all_trajectories' in locals():\n",
        "                del all_trajectories\n",
        "            if runner is not None:\n",
        "                del runner\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ è³‡æºæ¸…ç†è­¦å‘Š: {e}\")\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return True, {\n",
        "            'virtual_info': virtual_info,\n",
        "            'execution_time': execution_time,\n",
        "            'avg_rewards': {\n",
        "                'global': avg_global_reward,\n",
        "                'personalized': avg_personalized_reward,\n",
        "                'pfl': avg_pfl_reward\n",
        "            },\n",
        "            'privacy_stats': privacy_stats\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        execution_time = (time.time() - start_time) / 60\n",
        "        experiment_name = config_dict.get('experiment_name', 'Unknown')\n",
        "\n",
        "        print(f\"\\nâŒ è™›æ“¬å®¢æˆ¶ç«¯å¯¦é©—å¤±æ•—: {experiment_name}\")\n",
        "        print(f\"â±ï¸ å¤±æ•—æ™‚é–“: {execution_time:.2f} åˆ†é˜\")\n",
        "        print(f\"ğŸ” éŒ¯èª¤è©³æƒ…: {str(e)}\")\n",
        "\n",
        "        import traceback\n",
        "        print(f\"ğŸ“‹ éŒ¯èª¤å †ç–Š:\")\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "        # ã€ä¿®æ­£5ã€‘å®‰å…¨çš„éŒ¯èª¤æ¸…ç†\n",
        "        try:\n",
        "            if runner is not None:\n",
        "                del runner\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "        except Exception as cleanup_error:\n",
        "            print(f\"âš ï¸ æ¸…ç†éç¨‹å‡ºéŒ¯: {cleanup_error}\")\n",
        "\n",
        "        return False, {\n",
        "            'error': str(e),\n",
        "            'execution_time': execution_time,\n",
        "            'traceback': traceback.format_exc()\n",
        "        }\n",
        "\n",
        "print(\"âœ… Cell 8.1: å¯¦é©—åŸ·è¡Œå‡½æ•¸ï¼ˆä¿®æ­£ç‰ˆï¼‰å®šç¾©å®Œæˆ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "050-srilLiIa",
        "outputId": "cc90a52e-d263-48a5-88f3-5cd0a2bd36fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 8.1: å¯¦é©—åŸ·è¡Œå‡½æ•¸ï¼ˆä¿®æ­£ç‰ˆï¼‰å®šç¾©å®Œæˆ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 8.2: ğŸŒ ç’°å¢ƒåˆå§‹åŒ–èˆ‡è·¯å¾‘è¨­å®š\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# GPUç’°å¢ƒè¨­å®š\n",
        "setup_gpu_environment()\n",
        "\n",
        "# ç’°å¢ƒè·¯å¾‘è¨­å®š\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_WORK_DIR = \"/content/drive/MyDrive/FRL_Slicing_Sim\"\n",
        "    print(\"ğŸ”— Google Drive æ›è¼‰æˆåŠŸ\")\n",
        "except ImportError:\n",
        "    BASE_WORK_DIR = \"./FRL_Slicing_Sim\"\n",
        "    print(\"ğŸ’» æœ¬åœ°ç’°å¢ƒæ¨¡å¼\")\n",
        "\n",
        "os.makedirs(BASE_WORK_DIR, exist_ok=True)\n",
        "DATA_PATH = os.path.join(BASE_WORK_DIR, \"kpi_traces_final_robust0.parquet\")\n",
        "BASE_OUTPUT_DIR = os.path.join(BASE_WORK_DIR, \"outputs_virtual_clients\")\n",
        "\n",
        "print(f\"ğŸ“ æ•¸æ“šè·¯å¾‘: {DATA_PATH}\")\n",
        "print(f\"ğŸ“ è¼¸å‡ºç›®éŒ„: {BASE_OUTPUT_DIR}\")\n",
        "\n",
        "# æª¢æŸ¥æ•¸æ“šæ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
        "if os.path.exists(DATA_PATH):\n",
        "    print(f\"âœ… æ•¸æ“šæ–‡ä»¶å·²æ‰¾åˆ°\")\n",
        "else:\n",
        "    print(f\"âŒ æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {DATA_PATH}\")\n",
        "\n",
        "# å…¨åŸŸé…ç½®å‡½æ•¸\n",
        "def get_base_config(mode: str, seed: int):\n",
        "    \"\"\"ç²å–åŸºç¤å¯¦é©—é…ç½®\"\"\"\n",
        "    exp_name = f\"{mode}_virtual_clients_s{seed}\"\n",
        "    output_dir = os.path.join(BASE_OUTPUT_DIR, f\"seed_{seed}\", mode)\n",
        "\n",
        "    return {\n",
        "        \"experiment_name\": exp_name,\n",
        "        \"output_dir\": output_dir,\n",
        "        \"mode\": mode,\n",
        "        \"random_seed\": seed,\n",
        "\n",
        "        # è™›æ“¬å®¢æˆ¶ç«¯æ“´å¢è¨­å®š\n",
        "        \"base_client_pairs\": ((1, 2), (3, 7), (5, 6)),\n",
        "        \"virtual_expansion_factor\": 4,\n",
        "        \"total_clients\": 12,\n",
        "        \"num_clients_to_select\": 8,\n",
        "\n",
        "        # è¨“ç·´åƒæ•¸\n",
        "        \"comm_rounds\": 15,\n",
        "        \"local_episodes_per_round\": 2,\n",
        "        \"steps_per_episode\": 500,\n",
        "        \"batch_size\": 128,\n",
        "        \"memory_capacity\": 50000,\n",
        "        \"replay_batches_per_call\": 3,\n",
        "\n",
        "        # è¯é‚¦å­¸ç¿’åƒæ•¸\n",
        "        \"fedprox_mu\": 0.15,\n",
        "        \"num_clusters\": 3,\n",
        "        \"cluster_update_freq\": 8,\n",
        "\n",
        "        # è™›æ“¬å®¢æˆ¶ç«¯ç”Ÿæˆåƒæ•¸\n",
        "        \"temporal_split_method\": \"sliding_window\",\n",
        "        \"noise_injection_std\": 0.03,\n",
        "        \"feature_augmentation\": True,\n",
        "        \"cross_validation_split\": True,\n",
        "\n",
        "        # å·®åˆ†éš±ç§åƒæ•¸\n",
        "        \"enable_dp\": True,\n",
        "        \"dp_target_epsilon\": 8.0,\n",
        "        \"dp_target_delta\": 1e-5,\n",
        "        \"dp_noise_multiplier\": 0.3,\n",
        "        \"dp_max_grad_norm\": 1.0,\n",
        "        \"enable_dp_reset\": True,\n",
        "        \"dp_reset_threshold_multiplier\": 1.5,\n",
        "\n",
        "        # å…¶ä»–åƒæ•¸\n",
        "        \"lr\": 1e-4,\n",
        "        \"gamma\": 0.99,\n",
        "        \"enable_heterogeneity\": True,\n",
        "        \"enable_compression\": True,\n",
        "        \"use_pfl_finetune\": True,\n",
        "    }\n",
        "\n",
        "print(\"âœ… Cell 8.2: ç’°å¢ƒåˆå§‹åŒ–å®Œæˆ\")"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK_MeII2LoaD",
        "outputId": "6c621173-4c3d-463e-c6fd-8f0cab5686b4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ® GPU æª¢æ¸¬: Tesla T4\n",
            "ğŸ“Š ç¸½è¨˜æ†¶é«”: 15.8 GB\n",
            "ğŸ”§ è¨ˆç®—èƒ½åŠ›: 7.x\n",
            "ğŸ’» å¯ç”¨CPUæ•¸: 2\n",
            "ğŸ¯ é«˜ç«¯GPUè¨­å®šï¼šä½¿ç”¨90%è¨˜æ†¶é«” (15.8 GB)\n",
            "ğŸ”¥ é–‹å§‹GPUé ç†±...\n",
            "   âœ… GPUé ç†±å®Œæˆ (378.7ms)\n",
            "ğŸ“Š è¨˜æ†¶é«”ç‹€æ…‹: å·²åˆ†é… 0.03GB, å·²ä¿ç•™ 0.04GB\n",
            "ğŸ§¹ GPUç’°å¢ƒè¨­å®šå®Œæˆï¼ˆç¸½è€—æ™‚: 639.2msï¼‰\n",
            "Mounted at /content/drive\n",
            "ğŸ”— Google Drive æ›è¼‰æˆåŠŸ\n",
            "ğŸ“ æ•¸æ“šè·¯å¾‘: /content/drive/MyDrive/FRL_Slicing_Sim/kpi_traces_final_robust0.parquet\n",
            "ğŸ“ è¼¸å‡ºç›®éŒ„: /content/drive/MyDrive/FRL_Slicing_Sim/outputs_virtual_clients\n",
            "âœ… æ•¸æ“šæ–‡ä»¶å·²æ‰¾åˆ°\n",
            "âœ… Cell 8.2: ç’°å¢ƒåˆå§‹åŒ–å®Œæˆ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 8.3: ğŸ”¬ å¯¦é©—1 - ClusteredFL\n",
        "print(\"ğŸš€ é–‹å§‹å¯¦é©—1 - ClusteredFL\")\n",
        "\n",
        "# é…ç½®åƒæ•¸\n",
        "SEED = 42\n",
        "MODE = \"ClusteredFL\"\n",
        "config_params = get_base_config(MODE, SEED)\n",
        "\n",
        "print(f\"ğŸ”§ å¯¦é©—é…ç½®:\")\n",
        "print(f\"   - æ¨¡å¼: {MODE}\")\n",
        "print(f\"   - ç¨®å­: {SEED}\")\n",
        "print(f\"   - å®¢æˆ¶ç«¯: {config_params['total_clients']} (çœŸå¯¦:3, è™›æ“¬:9)\")\n",
        "print(f\"   - å›åˆæ•¸: {config_params['comm_rounds']}\")\n",
        "print(f\"   - æ‰¹æ¬¡å¤§å°: {config_params['batch_size']}\")\n",
        "print(f\"   - FedProx Î¼: {config_params['fedprox_mu']}\")\n",
        "print(f\"   - DPå™ªè²: {config_params['dp_noise_multiplier']}\")\n",
        "\n",
        "# åŸ·è¡Œå¯¦é©—\n",
        "start_time = time.time()\n",
        "success, result_info = run_virtual_client_experiment(config_params, DATA_PATH)\n",
        "execution_time = (time.time() - start_time) / 60\n",
        "\n",
        "# çµæœå ±å‘Š\n",
        "if success:\n",
        "    print(f\"\\nâœ… ClusteredFL å¯¦é©—æˆåŠŸå®Œæˆ\")\n",
        "    print(f\"â±ï¸ åŸ·è¡Œæ™‚é–“: {execution_time:.2f} åˆ†é˜\")\n",
        "\n",
        "    if result_info and 'avg_rewards' in result_info:\n",
        "        rewards = result_info['avg_rewards']\n",
        "        print(f\"ğŸ“Š é—œéµæŒ‡æ¨™:\")\n",
        "        print(f\"   - å…¨åŸŸçå‹µ: {rewards['global']:.4f}\")\n",
        "        print(f\"   - å€‹äººåŒ–çå‹µ: {rewards['personalized']:.4f}\")\n",
        "        print(f\"   - PFLçå‹µ: {rewards['pfl']:.4f}\")\n",
        "\n",
        "        if result_info.get('privacy_stats'):\n",
        "            privacy = result_info['privacy_stats']\n",
        "            print(f\"ğŸ›¡ï¸ éš±ç§çµ±è¨ˆ:\")\n",
        "            print(f\"   - Îµæ¶ˆè€—: {privacy.get('consumed_epsilon', 0):.4f}\")\n",
        "            print(f\"   - é‡è¨­æ¬¡æ•¸: {privacy.get('reset_count', 0)}\")\n",
        "            print(f\"   - é ç®—è¶…æ”¯: {'æ˜¯' if privacy.get('budget_exceeded', False) else 'å¦'}\")\n",
        "else:\n",
        "    print(f\"\\nâŒ ClusteredFL å¯¦é©—å¤±æ•—\")\n",
        "    print(f\"â±ï¸ å¤±æ•—æ™‚é–“: {execution_time:.2f} åˆ†é˜\")\n",
        "    if result_info and 'error' in result_info:\n",
        "        print(f\"ğŸ” éŒ¯èª¤: {result_info['error']}\")\n",
        "\n",
        "# æ¸…ç†è³‡æº\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(f\"ğŸ§¹ è³‡æºæ¸…ç†å®Œæˆ\")\n",
        "\n",
        "print(\"âœ… å¯¦é©—1 - ClusteredFL å®Œæˆ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "32e9f8543316465083995fb0f2454c99",
            "aebb265f05664258a35cd2d2f9157af0",
            "413b9881ba634052a7e738fb874d4035",
            "1bd96cdf692f403d995ddc88a1624819",
            "e0983c9ef9694b828ffa371f14db6212",
            "1a1e0d29fe4c43398fb6183278f0f2cd",
            "58d3070e5a9e475d8c74c8b3c3ac4579",
            "d60681a7d6f24be3997acd88e3c20269",
            "a6618534e9404e8db25f828d99b25e3a",
            "d17ea8e71e834fbf9ed9b4c3c496680d",
            "16c93c42e90d485088bc29505aa8ca43",
            "093d03e4fe3c476192a6369a4aac6aec",
            "a8ee9bc7d6794d33b40dce6c1ef119d5",
            "9ab6c7fa418d40c3a3497f011abb0c56",
            "77022653894943798c4da9f665de7592",
            "b550fd41ccb54fca84b46fff002f6cf0",
            "311401d48b1c447e815e2a377f741e83",
            "545daa784ddb41c1b67656782da72aa4",
            "025b671d53d54e3f877cc068e31e4493",
            "a4700a7e76ea44049ac178f1f02bd569",
            "b1734435a75541dfbc72d0d276e60c3f",
            "b394dac30c674dd1bd3530b49b428387"
          ]
        },
        "cellView": "form",
        "id": "HpvP_4sxLrW-",
        "outputId": "504e1b5c-b393-4f05-cb1c-f9c43ef10b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ é–‹å§‹å¯¦é©—1 - ClusteredFL\n",
            "ğŸ”§ å¯¦é©—é…ç½®:\n",
            "   - æ¨¡å¼: ClusteredFL\n",
            "   - ç¨®å­: 42\n",
            "   - å®¢æˆ¶ç«¯: 12 (çœŸå¯¦:3, è™›æ“¬:9)\n",
            "   - å›åˆæ•¸: 15\n",
            "   - æ‰¹æ¬¡å¤§å°: 128\n",
            "   - FedProx Î¼: 0.15\n",
            "   - DPå™ªè²: 0.3\n",
            "ğŸ”„ è™›æ“¬å®¢æˆ¶ç«¯é…ç½®:\n",
            "   - çœŸå¯¦åŸºç«™å°: 3 (((1, 2), (3, 7), (5, 6)))\n",
            "   - æ“´å¢å› å­: 4\n",
            "   - ç¸½å®¢æˆ¶ç«¯æ•¸: 12 (çœŸå¯¦: 3, è™›æ“¬: 9)\n",
            "   - æ¯è¼ªåƒèˆ‡: 8\n",
            "ğŸ›¡ï¸ å·®åˆ†éš±ç§å·²å•Ÿç”¨ï¼ˆGDP/PRV Accountant + Poisson Samplingï¼‰\n",
            "   - ç›®æ¨™éš±ç§é ç®—: Îµ=8.0\n",
            "   - æ‰¹æ¬¡å¤§å°: 128\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - é‡è¨­æ©Ÿåˆ¶: å•Ÿç”¨\n",
            "\n",
            "=============== ğŸ”„ è™›æ“¬å®¢æˆ¶ç«¯DPå¯¦é©—: ClusteredFL_virtual_clients_s42 ===============\n",
            "ğŸ“Š æ­£åœ¨æº–å‚™åŸå§‹æ•¸æ“š...\n",
            "\n",
            "[DataManager] æ­£åœ¨å¾ /content/drive/MyDrive/FRL_Slicing_Sim/kpi_traces_final_robust0.parquet è®€å–æ•¸æ“š...\n",
            "\n",
            "==================== DataManager å•Ÿå‹•å‰é æª¢æŸ¥ ====================\n",
            "âœ… æ¸…ç†å¾Œçš„æ¬„ä½åˆ—è¡¨ (å…± 38 å€‹):\n",
            "   - ååé‡æ¬„ä½æˆåŠŸåŒ¹é…: 'throughput_dl_mbps'\n",
            "   - å»¶é²/ç·©è¡å€æ¬„ä½æˆåŠŸåŒ¹é…: 'buffer_occupancy_dl_bytes'\n",
            "   - å¯ç”¨BSç¯€é»: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7)]\n",
            "   - å®¢æˆ¶ç«¯BSé…å°é©—è­‰é€šé\n",
            "=================================================================\n",
            "\n",
            "[DataManager] æ­£åœ¨ç‚ºæ¯å€‹å®¢æˆ¶ç«¯ç”Ÿæˆæ•¸æ“šè»Œè·¡...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "è™•ç†å®¢æˆ¶ç«¯æ•¸æ“š:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32e9f8543316465083995fb0f2454c99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   - å®¢æˆ¶ç«¯ 0: 10568 å€‹æ™‚é–“æ­¥\n",
            "   - å®¢æˆ¶ç«¯ 1: 6918 å€‹æ™‚é–“æ­¥\n",
            "   - å®¢æˆ¶ç«¯ 2: 8756 å€‹æ™‚é–“æ­¥\n",
            "\n",
            "[DataManager] æ•¸æ“šè™•ç†å®Œæˆï¼æˆåŠŸç‚º 3 / 3 å€‹å®¢æˆ¶ç«¯å‰µå»ºäº†ç’°å¢ƒã€‚\n",
            "   - åŸå§‹æ•¸æ“šæº: 3 å€‹çœŸå¯¦å®¢æˆ¶ç«¯\n",
            "     å®¢æˆ¶ç«¯ 0: 10568 æ•¸æ“šé»\n",
            "     å®¢æˆ¶ç«¯ 1: 6918 æ•¸æ“šé»\n",
            "     å®¢æˆ¶ç«¯ 2: 8756 æ•¸æ“šé»\n",
            "\n",
            "ğŸ”„ é–‹å§‹è™›æ“¬å®¢æˆ¶ç«¯ç”Ÿæˆ...\n",
            "\n",
            "ğŸ”„ é–‹å§‹ç”Ÿæˆè™›æ“¬å®¢æˆ¶ç«¯...\n",
            "   - åŸå§‹å®¢æˆ¶ç«¯æ•¸: 3\n",
            "   - ç›®æ¨™ç¸½å®¢æˆ¶ç«¯æ•¸: 12\n",
            "[çœŸå¯¦å®¢æˆ¶ç«¯ 0] åŸºç«™å°: (1, 2) - æ•¸æ“šé»: 10568\n",
            "[çœŸå¯¦å®¢æˆ¶ç«¯ 1] åŸºç«™å°: (3, 7) - æ•¸æ“šé»: 6918\n",
            "[çœŸå¯¦å®¢æˆ¶ç«¯ 2] åŸºç«™å°: (5, 6) - æ•¸æ“šé»: 8756\n",
            "[è™›æ“¬å®¢æˆ¶ç«¯ 3] åŸºæ–¼åŸºç«™å° (1, 2) - è®Šé«” 1 - æ•¸æ“šé»: 2113\n",
            "[è™›æ“¬å®¢æˆ¶ç«¯ 4] åŸºæ–¼åŸºç«™å° (1, 2) - è®Šé«” 2 - æ•¸æ“šé»: 2113\n",
            "[è™›æ“¬å®¢æˆ¶ç«¯ 5] åŸºæ–¼åŸºç«™å° (1, 2) - è®Šé«” 3 - æ•¸æ“šé»: 2113\n",
            "[è™›æ“¬å®¢æˆ¶ç«¯ 6] åŸºæ–¼åŸºç«™å° (1, 2) - è®Šé«” 4 - æ•¸æ“šé»: 2113\n",
            "[è™›æ“¬å®¢æˆ¶ç«¯ 7] åŸºæ–¼åŸºç«™å° (3, 7) - è®Šé«” 1 - æ•¸æ“šé»: 1383\n",
            "[è™›æ“¬å®¢æˆ¶ç«¯ 8] åŸºæ–¼åŸºç«™å° (3, 7) - è®Šé«” 2 - æ•¸æ“šé»: 1383\n",
            "[è™›æ“¬å®¢æˆ¶ç«¯ 9] åŸºæ–¼åŸºç«™å° (3, 7) - è®Šé«” 3 - æ•¸æ“šé»: 1383\n",
            "[è™›æ“¬å®¢æˆ¶ç«¯ 10] åŸºæ–¼åŸºç«™å° (3, 7) - è®Šé«” 4 - æ•¸æ“šé»: 1383\n",
            "[è™›æ“¬å®¢æˆ¶ç«¯ 11] åŸºæ–¼åŸºç«™å° (5, 6) - è®Šé«” 1 - æ•¸æ“šé»: 1751\n",
            "[è™›æ“¬å®¢æˆ¶ç«¯ 12] åŸºæ–¼åŸºç«™å° (5, 6) - è®Šé«” 2 - æ•¸æ“šé»: 1751\n",
            "[è™›æ“¬å®¢æˆ¶ç«¯ 13] åŸºæ–¼åŸºç«™å° (5, 6) - è®Šé«” 3 - æ•¸æ“šé»: 1751\n",
            "[è™›æ“¬å®¢æˆ¶ç«¯ 14] åŸºæ–¼åŸºç«™å° (5, 6) - è®Šé«” 4 - æ•¸æ“šé»: 1751\n",
            "\n",
            "âœ… è™›æ“¬å®¢æˆ¶ç«¯ç”Ÿæˆå®Œæˆ:\n",
            "   - çœŸå¯¦å®¢æˆ¶ç«¯: 3\n",
            "   - è™›æ“¬å®¢æˆ¶ç«¯: 12\n",
            "   - å¤±æ•—ç”Ÿæˆ: 0\n",
            "   - ç¸½å®¢æˆ¶ç«¯: 15\n",
            "   - ç¸½æ•¸æ“šé»: 47230\n",
            "\n",
            "ğŸ“ˆ è™›æ“¬å®¢æˆ¶ç«¯ç”Ÿæˆçµ±è¨ˆ:\n",
            "   - çœŸå¯¦å®¢æˆ¶ç«¯: 3\n",
            "   - è™›æ“¬å®¢æˆ¶ç«¯: 12\n",
            "   - ç”Ÿæˆå¤±æ•—: 0\n",
            "   - ç¸½è¨ˆå®¢æˆ¶ç«¯: 15\n",
            "   - æˆåŠŸç‡: 100.00%\n",
            "   - åˆ†å‰²æ–¹æ³•: sliding_window\n",
            "   - ç¸½æ•¸æ“šé»: 47,230\n",
            "\n",
            "ğŸ” æ•¸æ“šè³ªé‡é©—è­‰:\n",
            "   - å®¢æˆ¶ç«¯ 0 (çœŸå¯¦): é•·åº¦=10568, å‡å€¼=[2.4685,...,25.7212], æ¨™å·®=[1.3085,...,82.7368]\n",
            "   - å®¢æˆ¶ç«¯ 1 (çœŸå¯¦): é•·åº¦=6918, å‡å€¼=[2.6377,...,695.1311], æ¨™å·®=[1.7496,...,3634.1282]\n",
            "   - å®¢æˆ¶ç«¯ 2 (çœŸå¯¦): é•·åº¦=8756, å‡å€¼=[2.0969,...,15.2451], æ¨™å·®=[1.6933,...,54.7714]\n",
            "   - å®¢æˆ¶ç«¯ 3 (è™›æ“¬): é•·åº¦=2113, å‡å€¼=[1.3988,...,27.2162], æ¨™å·®=[0.5423,...,63.4062]\n",
            "   - å®¢æˆ¶ç«¯ 4 (è™›æ“¬): é•·åº¦=2113, å‡å€¼=[2.4551,...,34.6273], æ¨™å·®=[1.2283,...,99.4359]\n",
            "\n",
            "ğŸ›¡ï¸ å·®åˆ†éš±ç§é…ç½®:\n",
            "   - ç›®æ¨™Îµ: 8.0\n",
            "   - æ‰¹æ¬¡å¤§å°: 128\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - å®¢æˆ¶ç«¯æ•¸é‡: 15 (å¤§å¹…å¢åŠ )\n",
            "   - æ¯è¼ªåƒèˆ‡: 8\n",
            "   - é‡è¨­æ©Ÿåˆ¶: å•Ÿç”¨\n",
            "\n",
            "ğŸš€ é–‹å§‹è¯é‚¦å­¸ç¿’å¯¦é©—...\n",
            "[FLServer] åˆå§‹åŒ–å®Œæˆ - èšé¡æ•¸: 3\n",
            "\n",
            "[ExperimentRunner] æ­£åœ¨åˆå§‹åŒ–å®¢æˆ¶ç«¯ç’°å¢ƒèˆ‡ä»£ç†...\n",
            "ğŸ›¡ï¸ å·®åˆ†éš±ç§æ¨¡å¼å•Ÿç”¨ï¼ˆæ­»å¾ªç’°é˜²è­·ç‰ˆï¼‰ï¼š\n",
            "   - ç¸½éš±ç§é ç®—ä¸Šé™: Îµ=8.0\n",
            "   - é‡è¨­é–¾å€¼: Îµ=12.0\n",
            "   - æ­»å¾ªç’°é˜²è­·: æ¯è¼ªæœ€å¤š1æ¬¡é‡è¨­\n",
            "   - å®¢æˆ¶ç«¯æ•¸é‡: 15\n",
            "[C-0] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.012112\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "[C-1] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.018502\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "[C-2] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.014619\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "[C-3] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.060577\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "[C-4] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.060577\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "[C-5] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.060577\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "[C-6] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.060577\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "[C-7] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.092552\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "[C-8] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.092552\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "[C-9] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.092552\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "[C-10] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.092552\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "[C-11] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.073101\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "[C-12] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.073101\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "[C-13] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.073101\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "[C-14] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.073101\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "âœ… é…ç½®å·²ä¿å­˜è‡³: /content/drive/MyDrive/FRL_Slicing_Sim/outputs_virtual_clients/seed_42/ClusteredFL/ClusteredFL_virtual_clients_s42_config.json\n",
            "[ExperimentRunner] åˆå§‹åŒ–å®Œæˆã€‚\n",
            "\n",
            "==================== ğŸƒâ™‚ï¸ é–‹å§‹åŸ·è¡Œå¯¦é©—: ClusteredFL_virtual_clients_s42 (ClusteredFL) ====================\n",
            "\n",
            "[æ¨¡å¼] åŸ·è¡Œè¯é‚¦å¼è¨“ç·´ (ClusteredFL)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ClusteredFL Training:   0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "093d03e4fe3c476192a6369a4aac6aec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- é–‹å§‹ Round 1/15 ---\n",
            "åƒèˆ‡å®¢æˆ¶ç«¯: [np.int64(10), np.int64(13), np.int64(12), np.int64(5), np.int64(11), np.int64(6), np.int64(0), np.int64(9)]\n",
            "[FLServer] å®¢æˆ¶ç«¯ 10: æ¨¡å‹åˆ†ç™¼æˆåŠŸ\n",
            "  - DPæ¨¡å‹: True\n",
            "  - åŸå§‹éµæ•¸: 6\n",
            "  - è½‰æ›å¾Œéµæ•¸: 6\n",
            "  - éµåç¯„ä¾‹: 0.weight â†’ _module.0.weight\n",
            "[FLServer] æ¨¡å‹åˆ†ç™¼å®Œæˆ: æˆåŠŸ 8/8\n",
            "[C-10] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-10] å®Œæˆ - Loss: 0.0000, Reward: 194.9083, Îµ: 0.0000\n",
            "[C-13] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-13] å®Œæˆ - Loss: 0.0000, Reward: 341.7491, Îµ: 0.0000\n",
            "[C-12] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-12] å®Œæˆ - Loss: 0.0000, Reward: 415.1353, Îµ: 0.0000\n",
            "[C-5] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-5] å®Œæˆ - Loss: 0.0000, Reward: 421.3020, Îµ: 0.0000\n",
            "[C-11] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-11] å®Œæˆ - Loss: 0.0000, Reward: 169.6227, Îµ: 0.0000\n",
            "[C-6] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-6] å®Œæˆ - Loss: 0.0000, Reward: 212.7530, Îµ: 0.0000\n",
            "[C-0] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-0] å®Œæˆ - Loss: 0.0000, Reward: 326.9034, Îµ: 0.0000\n",
            "[C-9] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-9] å®Œæˆ - Loss: 0.0000, Reward: 214.4674, Îµ: 0.0000\n",
            "æª¢æŸ¥éš±ç§é ç®—...\n",
            "Round 1 å®Œæˆ - {'reward': '287.11', 'loss': '0.0000'}\n",
            "\n",
            "--- é–‹å§‹ Round 2/15 ---\n",
            "åƒèˆ‡å®¢æˆ¶ç«¯: [np.int64(13), np.int64(9), np.int64(0), np.int64(3), np.int64(12), np.int64(4), np.int64(2), np.int64(6)]\n",
            "[FLServer] å®¢æˆ¶ç«¯ 13: æ¨¡å‹åˆ†ç™¼æˆåŠŸ\n",
            "  - DPæ¨¡å‹: True\n",
            "  - åŸå§‹éµæ•¸: 6\n",
            "  - è½‰æ›å¾Œéµæ•¸: 6\n",
            "  - éµåç¯„ä¾‹: 0.weight â†’ _module.0.weight\n",
            "[FLServer] æ¨¡å‹åˆ†ç™¼å®Œæˆ: æˆåŠŸ 8/8\n",
            "[C-13] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-13] âš ï¸ éš±ç§æˆæœ¬è¨ˆç®—å¤±æ•—: f(a) and f(b) must have different signs\n",
            "   ğŸ”„ ä½¿ç”¨ä¼°ç®—å€¼: Îµ â‰ˆ 8.0000 (åŸºæ–¼ 1500 æ­¥)\n",
            "[C-13] å®Œæˆ - Loss: 7214984.7836, Reward: 307.1206, Îµ: 8.0000\n",
            "[C-9] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-9] âš ï¸ éš±ç§æˆæœ¬è¨ˆç®—å¤±æ•—: f(a) and f(b) must have different signs\n",
            "   ğŸ”„ ä½¿ç”¨ä¼°ç®—å€¼: Îµ â‰ˆ 8.0000 (åŸºæ–¼ 1500 æ­¥)\n",
            "[C-9] å®Œæˆ - Loss: 3926183.8552, Reward: 211.8680, Îµ: 8.0000\n",
            "[C-0] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-0] âš ï¸ éš±ç§æˆæœ¬è¨ˆç®—å¤±æ•—: f(a) and f(b) must have different signs\n",
            "   ğŸ”„ ä½¿ç”¨ä¼°ç®—å€¼: Îµ â‰ˆ 8.0000 (åŸºæ–¼ 1500 æ­¥)\n",
            "[C-0] å®Œæˆ - Loss: 2653052.9912, Reward: 317.1639, Îµ: 8.0000\n",
            "[C-3] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-3] å®Œæˆ - Loss: 0.0000, Reward: 207.5715, Îµ: 0.0000\n",
            "[C-12] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-12] âš ï¸ éš±ç§æˆæœ¬è¨ˆç®—å¤±æ•—: f(a) and f(b) must have different signs\n",
            "   ğŸ”„ ä½¿ç”¨ä¼°ç®—å€¼: Îµ â‰ˆ 8.0000 (åŸºæ–¼ 1500 æ­¥)\n",
            "[C-12] å®Œæˆ - Loss: 6081421.7655, Reward: 410.1770, Îµ: 8.0000\n",
            "[C-4] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-4] å®Œæˆ - Loss: 0.0000, Reward: 255.0589, Îµ: 0.0000\n",
            "[C-2] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-2] å®Œæˆ - Loss: 0.0000, Reward: 425.8754, Îµ: 0.0000\n",
            "[C-6] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-6] âš ï¸ éš±ç§æˆæœ¬è¨ˆç®—å¤±æ•—: f(a) and f(b) must have different signs\n",
            "   ğŸ”„ ä½¿ç”¨ä¼°ç®—å€¼: Îµ â‰ˆ 8.0000 (åŸºæ–¼ 1500 æ­¥)\n",
            "[C-6] å®Œæˆ - Loss: 17352818.9539, Reward: 317.7803, Îµ: 8.0000\n",
            "æª¢æŸ¥éš±ç§é ç®—...\n",
            "Round 2 å®Œæˆ - {'reward': '306.58', 'loss': '4653557.7937', 'Îµ_used': '8.000', 'resets': '0'}\n",
            "\n",
            "--- é–‹å§‹ Round 3/15 ---\n",
            "åƒèˆ‡å®¢æˆ¶ç«¯: [np.int64(7), np.int64(1), np.int64(5), np.int64(11), np.int64(8), np.int64(0), np.int64(13), np.int64(6)]\n",
            "[FLServer] å®¢æˆ¶ç«¯ 7: æ¨¡å‹åˆ†ç™¼æˆåŠŸ\n",
            "  - DPæ¨¡å‹: True\n",
            "  - åŸå§‹éµæ•¸: 6\n",
            "  - è½‰æ›å¾Œéµæ•¸: 6\n",
            "  - éµåç¯„ä¾‹: 0.weight â†’ _module.0.weight\n",
            "[FLServer] æ¨¡å‹åˆ†ç™¼å®Œæˆ: æˆåŠŸ 8/8\n",
            "[C-7] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-7] å®Œæˆ - Loss: 0.0000, Reward: 419.5440, Îµ: 0.0000\n",
            "[C-1] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-1] å®Œæˆ - Loss: 0.0000, Reward: 316.1287, Îµ: 0.0000\n",
            "[C-5] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-5] âš ï¸ éš±ç§æˆæœ¬è¨ˆç®—å¤±æ•—: f(a) and f(b) must have different signs\n",
            "   ğŸ”„ ä½¿ç”¨ä¼°ç®—å€¼: Îµ â‰ˆ 8.0000 (åŸºæ–¼ 1500 æ­¥)\n",
            "[C-5] å®Œæˆ - Loss: 27893.6165, Reward: 407.3351, Îµ: 8.0000\n",
            "[C-11] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-11] âš ï¸ éš±ç§æˆæœ¬è¨ˆç®—å¤±æ•—: f(a) and f(b) must have different signs\n",
            "   ğŸ”„ ä½¿ç”¨ä¼°ç®—å€¼: Îµ â‰ˆ 8.0000 (åŸºæ–¼ 1500 æ­¥)\n",
            "[C-11] å®Œæˆ - Loss: 1522230.3881, Reward: 169.3800, Îµ: 8.0000\n",
            "[C-8] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-8] å®Œæˆ - Loss: 0.0000, Reward: 334.5645, Îµ: 0.0000\n",
            "[C-0] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-0] âš ï¸ éš±ç§æˆæœ¬è¨ˆç®—å¤±æ•—: f(a) and f(b) must have different signs\n",
            "   ğŸ”„ ä½¿ç”¨ä¼°ç®—å€¼: Îµ â‰ˆ 8.0000 (åŸºæ–¼ 3000 æ­¥)\n",
            "[C-0] å®Œæˆ - Loss: 213490.1024, Reward: 313.1339, Îµ: 8.0000\n",
            "[C-13] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-13] âš ï¸ éš±ç§æˆæœ¬è¨ˆç®—å¤±æ•—: f(a) and f(b) must have different signs\n",
            "   ğŸ”„ ä½¿ç”¨ä¼°ç®—å€¼: Îµ â‰ˆ 8.0000 (åŸºæ–¼ 3000 æ­¥)\n",
            "[C-13] å®Œæˆ - Loss: 497490.9306, Reward: 201.7589, Îµ: 8.0000\n",
            "[C-6] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-6] âš ï¸ éš±ç§æˆæœ¬è¨ˆç®—å¤±æ•—: f(a) and f(b) must have different signs\n",
            "   ğŸ”„ ä½¿ç”¨ä¼°ç®—å€¼: Îµ â‰ˆ 8.0000 (åŸºæ–¼ 3000 æ­¥)\n",
            "[C-6] å®Œæˆ - Loss: 2134090.0942, Reward: 256.9749, Îµ: 8.0000\n",
            "æª¢æŸ¥éš±ç§é ç®—...\n",
            "\n",
            "==================== âš ï¸ éš±ç§é ç®—é¦–æ¬¡è¶…æ”¯ï¼ ====================\n",
            "   - ç•¶å‰æ¶ˆè€—: Îµ = 16.0000\n",
            "   - é ç®—ä¸Šé™: Îµ = 8.0\n",
            "   - æœ¬è¼ªçµ±è¨ˆ: å¹³å‡=8.0000, æœ€å¤§=8.0000\n",
            "==========================================================\n",
            "\n",
            "ğŸ”„ è§¸ç™¼éš±ç§é ç®—é‡è¨­æ©Ÿåˆ¶ï¼ˆRound 2ï¼‰\n",
            "   - è§¸ç™¼æ¢ä»¶: 16.0000 > 12.0000\n",
            "   - æœ¬è¼ªé‡è¨­æ¬¡æ•¸: 0/1\n",
            "   - é€£çºŒé‡è¨­è¼ªæ•¸: 0/2\n",
            "[C-0] ğŸ”„ é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound 2ï¼‰...\n",
            "[C-0] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.012112\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬1æ¬¡ï¼‰\n",
            "   - èˆŠÎµ: 8.0000 â†’ æ–°Îµ: 0.0000\n",
            "[C-1] ğŸ”„ é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound 2ï¼‰...\n",
            "[C-1] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.018502\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬1æ¬¡ï¼‰\n",
            "   - èˆŠÎµ: 0.0000 â†’ æ–°Îµ: 0.0000\n",
            "[C-2] ğŸ”„ é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound 2ï¼‰...\n",
            "[C-2] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.014619\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬1æ¬¡ï¼‰\n",
            "   - èˆŠÎµ: 0.0000 â†’ æ–°Îµ: 0.0000\n",
            "[C-3] ğŸ”„ é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound 2ï¼‰...\n",
            "[C-3] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.060577\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬1æ¬¡ï¼‰\n",
            "   - èˆŠÎµ: 0.0000 â†’ æ–°Îµ: 0.0000\n",
            "[C-4] ğŸ”„ é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound 2ï¼‰...\n",
            "[C-4] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.060577\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬1æ¬¡ï¼‰\n",
            "   - èˆŠÎµ: 0.0000 â†’ æ–°Îµ: 0.0000\n",
            "[C-5] ğŸ”„ é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound 2ï¼‰...\n",
            "[C-5] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.060577\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬1æ¬¡ï¼‰\n",
            "   - èˆŠÎµ: 8.0000 â†’ æ–°Îµ: 0.0000\n",
            "[C-6] ğŸ”„ é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound 2ï¼‰...\n",
            "[C-6] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.060577\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬1æ¬¡ï¼‰\n",
            "   - èˆŠÎµ: 8.0000 â†’ æ–°Îµ: 0.0000\n",
            "[C-7] ğŸ”„ é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound 2ï¼‰...\n",
            "[C-7] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.092552\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬1æ¬¡ï¼‰\n",
            "   - èˆŠÎµ: 0.0000 â†’ æ–°Îµ: 0.0000\n",
            "[C-8] ğŸ”„ é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound 2ï¼‰...\n",
            "[C-8] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.092552\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬1æ¬¡ï¼‰\n",
            "   - èˆŠÎµ: 0.0000 â†’ æ–°Îµ: 0.0000\n",
            "[C-9] ğŸ”„ é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound 2ï¼‰...\n",
            "[C-9] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.092552\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬1æ¬¡ï¼‰\n",
            "   - èˆŠÎµ: 8.0000 â†’ æ–°Îµ: 0.0000\n",
            "[C-10] ğŸ”„ é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound 2ï¼‰...\n",
            "[C-10] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.092552\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬1æ¬¡ï¼‰\n",
            "   - èˆŠÎµ: 0.0000 â†’ æ–°Îµ: 0.0000\n",
            "[C-11] ğŸ”„ é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound 2ï¼‰...\n",
            "[C-11] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.073101\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬1æ¬¡ï¼‰\n",
            "   - èˆŠÎµ: 8.0000 â†’ æ–°Îµ: 0.0000\n",
            "[C-12] ğŸ”„ é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound 2ï¼‰...\n",
            "[C-12] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.073101\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬1æ¬¡ï¼‰\n",
            "   - èˆŠÎµ: 8.0000 â†’ æ–°Îµ: 0.0000\n",
            "[C-13] ğŸ”„ é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound 2ï¼‰...\n",
            "[C-13] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.073101\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬1æ¬¡ï¼‰\n",
            "   - èˆŠÎµ: 8.0000 â†’ æ–°Îµ: 0.0000\n",
            "[C-14] ğŸ”„ é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound 2ï¼‰...\n",
            "[C-14] ğŸ›¡ï¸ åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\n",
            "   - å˜—è©¦ Accountant: GDP\n",
            "   - Sample Rate: 0.073101\n",
            "   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\n",
            "   - å™ªè²ä¹˜æ•¸: 0.3\n",
            "   - æ¢¯åº¦è£å‰ª: 1.0\n",
            "   - Poissonæ¡æ¨£: True\n",
            "   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬1æ¬¡ï¼‰\n",
            "   - èˆŠÎµ: 0.0000 â†’ æ–°Îµ: 0.0000\n",
            "   - âœ… é‡è¨­å®Œæˆï¼š15/15å€‹å®¢æˆ¶ç«¯\n",
            "   - æœ¬è¼ªé‡è¨­è¨ˆæ•¸ï¼š1\n",
            "Round 3 å®Œæˆ - {'reward': '302.35', 'loss': '549399.3915'}\n",
            "\n",
            "--- é–‹å§‹ Round 4/15 ---\n",
            "åƒèˆ‡å®¢æˆ¶ç«¯: [np.int64(14), np.int64(5), np.int64(7), np.int64(1), np.int64(4), np.int64(11), np.int64(6), np.int64(3)]\n",
            "[FLServer] å®¢æˆ¶ç«¯ 14: æ¨¡å‹åˆ†ç™¼æˆåŠŸ\n",
            "  - DPæ¨¡å‹: True\n",
            "  - åŸå§‹éµæ•¸: 6\n",
            "  - è½‰æ›å¾Œéµæ•¸: 6\n",
            "  - éµåç¯„ä¾‹: 0.weight â†’ _module.0.weight\n",
            "[FLServer] æ¨¡å‹åˆ†ç™¼å®Œæˆ: æˆåŠŸ 8/8\n",
            "[C-14] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-14] å®Œæˆ - Loss: 0.0000, Reward: 328.7891, Îµ: 0.0000\n",
            "[C-5] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-5] âš ï¸ éš±ç§æˆæœ¬è¨ˆç®—å¤±æ•—: f(a) and f(b) must have different signs\n",
            "   ğŸ”„ ä½¿ç”¨ä¼°ç®—å€¼: Îµ â‰ˆ 8.0000 (åŸºæ–¼ 1500 æ­¥)\n",
            "[C-5] å®Œæˆ - Loss: 22121.6917, Reward: 431.1817, Îµ: 8.0000\n",
            "[C-7] é–‹å§‹è¨“ç·´ (2 episodes)...\n",
            "[C-7] âš ï¸ éš±ç§æˆæœ¬è¨ˆç®—å¤±æ•—: f(a) and f(b) must have different signs\n",
            "   ğŸ”„ ä½¿ç”¨ä¼°ç®—å€¼: Îµ â‰ˆ 8.0000 (åŸºæ–¼ 1500 æ­¥)\n",
            "[C-7] å®Œæˆ - Loss: 6970.9349, Reward: 274.8082, Îµ: 8.0000\n",
            "[C-1] é–‹å§‹è¨“ç·´ (2 episodes)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 8.4: ğŸ”¬ å¯¦é©—2 - FedProx\n",
        "print(\"ğŸš€ é–‹å§‹å¯¦é©—2 - FedProx\")\n",
        "\n",
        "# é…ç½®åƒæ•¸\n",
        "SEED = 42\n",
        "MODE = \"FedProx\"\n",
        "config_params = get_base_config(MODE, SEED)\n",
        "\n",
        "print(f\"ğŸ”§ å¯¦é©—é…ç½®:\")\n",
        "print(f\"   - æ¨¡å¼: {MODE}\")\n",
        "print(f\"   - ç¨®å­: {SEED}\")\n",
        "print(f\"   - å®¢æˆ¶ç«¯: {config_params['total_clients']} (çœŸå¯¦:3, è™›æ“¬:9)\")\n",
        "print(f\"   - å›åˆæ•¸: {config_params['comm_rounds']}\")\n",
        "print(f\"   - æ‰¹æ¬¡å¤§å°: {config_params['batch_size']}\")\n",
        "print(f\"   - FedProx Î¼: {config_params['fedprox_mu']}\")\n",
        "print(f\"   - DPå™ªè²: {config_params['dp_noise_multiplier']}\")\n",
        "\n",
        "# åŸ·è¡Œå¯¦é©—\n",
        "start_time = time.time()\n",
        "success, result_info = run_virtual_client_experiment(config_params, DATA_PATH)\n",
        "execution_time = (time.time() - start_time) / 60\n",
        "\n",
        "# çµæœå ±å‘Š\n",
        "if success:\n",
        "    print(f\"\\nâœ… FedProx å¯¦é©—æˆåŠŸå®Œæˆ\")\n",
        "    print(f\"â±ï¸ åŸ·è¡Œæ™‚é–“: {execution_time:.2f} åˆ†é˜\")\n",
        "\n",
        "    if result_info and 'avg_rewards' in result_info:\n",
        "        rewards = result_info['avg_rewards']\n",
        "        print(f\"ğŸ“Š é—œéµæŒ‡æ¨™:\")\n",
        "        print(f\"   - å…¨åŸŸçå‹µ: {rewards['global']:.4f}\")\n",
        "        print(f\"   - å€‹äººåŒ–çå‹µ: {rewards['personalized']:.4f}\")\n",
        "        print(f\"   - PFLçå‹µ: {rewards['pfl']:.4f}\")\n",
        "\n",
        "        if result_info.get('privacy_stats'):\n",
        "            privacy = result_info['privacy_stats']\n",
        "            print(f\"ğŸ›¡ï¸ éš±ç§çµ±è¨ˆ:\")\n",
        "            print(f\"   - Îµæ¶ˆè€—: {privacy.get('consumed_epsilon', 0):.4f}\")\n",
        "            print(f\"   - é‡è¨­æ¬¡æ•¸: {privacy.get('reset_count', 0)}\")\n",
        "            print(f\"   - é ç®—è¶…æ”¯: {'æ˜¯' if privacy.get('budget_exceeded', False) else 'å¦'}\")\n",
        "else:\n",
        "    print(f\"\\nâŒ FedProx å¯¦é©—å¤±æ•—\")\n",
        "    print(f\"â±ï¸ å¤±æ•—æ™‚é–“: {execution_time:.2f} åˆ†é˜\")\n",
        "    if result_info and 'error' in result_info:\n",
        "        print(f\"ğŸ” éŒ¯èª¤: {result_info['error']}\")\n",
        "\n",
        "# æ¸…ç†è³‡æº\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(f\"ğŸ§¹ è³‡æºæ¸…ç†å®Œæˆ\")\n",
        "\n",
        "print(\"âœ… å¯¦é©—2 - FedProx å®Œæˆ\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7yaU3w_eMEif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 8.5: ğŸ”¬ å¯¦é©—3 - FedAvg\n",
        "print(\"ğŸš€ é–‹å§‹å¯¦é©—3 - FedAvg\")\n",
        "\n",
        "# é…ç½®åƒæ•¸\n",
        "SEED = 42\n",
        "MODE = \"FedAvg\"\n",
        "config_params = get_base_config(MODE, SEED)\n",
        "\n",
        "print(f\"ğŸ”§ å¯¦é©—é…ç½®:\")\n",
        "print(f\"   - æ¨¡å¼: {MODE}\")\n",
        "print(f\"   - ç¨®å­: {SEED}\")\n",
        "print(f\"   - å®¢æˆ¶ç«¯: {config_params['total_clients']} (çœŸå¯¦:3, è™›æ“¬:9)\")\n",
        "print(f\"   - å›åˆæ•¸: {config_params['comm_rounds']}\")\n",
        "print(f\"   - æ‰¹æ¬¡å¤§å°: {config_params['batch_size']}\")\n",
        "print(f\"   - FedProx Î¼: {config_params['fedprox_mu']}\")\n",
        "print(f\"   - DPå™ªè²: {config_params['dp_noise_multiplier']}\")\n",
        "\n",
        "# åŸ·è¡Œå¯¦é©—\n",
        "start_time = time.time()\n",
        "success, result_info = run_virtual_client_experiment(config_params, DATA_PATH)\n",
        "execution_time = (time.time() - start_time) / 60\n",
        "\n",
        "# çµæœå ±å‘Š\n",
        "if success:\n",
        "    print(f\"\\nâœ… FedAvg å¯¦é©—æˆåŠŸå®Œæˆ\")\n",
        "    print(f\"â±ï¸ åŸ·è¡Œæ™‚é–“: {execution_time:.2f} åˆ†é˜\")\n",
        "\n",
        "    if result_info and 'avg_rewards' in result_info:\n",
        "        rewards = result_info['avg_rewards']\n",
        "        print(f\"ğŸ“Š é—œéµæŒ‡æ¨™:\")\n",
        "        print(f\"   - å…¨åŸŸçå‹µ: {rewards['global']:.4f}\")\n",
        "        print(f\"   - å€‹äººåŒ–çå‹µ: {rewards['personalized']:.4f}\")\n",
        "        print(f\"   - PFLçå‹µ: {rewards['pfl']:.4f}\")\n",
        "\n",
        "        if result_info.get('privacy_stats'):\n",
        "            privacy = result_info['privacy_stats']\n",
        "            print(f\"ğŸ›¡ï¸ éš±ç§çµ±è¨ˆ:\")\n",
        "            print(f\"   - Îµæ¶ˆè€—: {privacy.get('consumed_epsilon', 0):.4f}\")\n",
        "            print(f\"   - é‡è¨­æ¬¡æ•¸: {privacy.get('reset_count', 0)}\")\n",
        "            print(f\"   - é ç®—è¶…æ”¯: {'æ˜¯' if privacy.get('budget_exceeded', False) else 'å¦'}\")\n",
        "else:\n",
        "    print(f\"\\nâŒ FedAvg å¯¦é©—å¤±æ•—\")\n",
        "    print(f\"â±ï¸ å¤±æ•—æ™‚é–“: {execution_time:.2f} åˆ†é˜\")\n",
        "    if result_info and 'error' in result_info:\n",
        "        print(f\"ğŸ” éŒ¯èª¤: {result_info['error']}\")\n",
        "\n",
        "# æ¸…ç†è³‡æº\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(f\"ğŸ§¹ è³‡æºæ¸…ç†å®Œæˆ\")\n",
        "\n",
        "print(\"âœ… å¯¦é©—3 - FedAvg å®Œæˆ\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pnfWQP8fMHCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 8.6: ğŸ“Š å¯¦é©—çµæœå½™ç¸½èˆ‡åˆ†æ\n",
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "print(\"ğŸ“Š é–‹å§‹å¯¦é©—çµæœå½™ç¸½åˆ†æ...\")\n",
        "\n",
        "def summarize_experiment_results(base_output_dir, seed=42):\n",
        "    \"\"\"å½™ç¸½å¯¦é©—çµæœ\"\"\"\n",
        "    modes = [\"ClusteredFL\", \"FedProx\", \"FedAvg\"]\n",
        "    summary_data = []\n",
        "\n",
        "    for mode in modes:\n",
        "        exp_path = os.path.join(base_output_dir, f\"seed_{seed}\", mode)\n",
        "\n",
        "        if not os.path.exists(exp_path):\n",
        "            print(f\"âš ï¸ æ‰¾ä¸åˆ° {mode} çš„çµæœç›®éŒ„\")\n",
        "            continue\n",
        "\n",
        "        # å°‹æ‰¾è©•ä¼°çµæœæ–‡ä»¶\n",
        "        eval_files = glob.glob(os.path.join(exp_path, '*_evaluation_results.csv'))\n",
        "        history_files = glob.glob(os.path.join(exp_path, '*_training_history.csv'))\n",
        "        privacy_files = glob.glob(os.path.join(exp_path, '*_privacy_costs.csv'))\n",
        "\n",
        "        result_summary = {\n",
        "            'mode': mode,\n",
        "            'seed': seed,\n",
        "            'has_evaluation': len(eval_files) > 0,\n",
        "            'has_history': len(history_files) > 0,\n",
        "            'has_privacy': len(privacy_files) > 0,\n",
        "            'status': 'unknown'\n",
        "        }\n",
        "\n",
        "        # åˆ†æè©•ä¼°çµæœ\n",
        "        if eval_files:\n",
        "            try:\n",
        "                eval_df = pd.read_csv(eval_files[0])\n",
        "                if not eval_df.empty:\n",
        "                    result_summary.update({\n",
        "                        'avg_global_reward': float(eval_df['reward_global'].mean()),\n",
        "                        'avg_personalized_reward': float(eval_df['reward_personalized'].mean()),\n",
        "                        'avg_pfl_reward': float(eval_df['reward_pfl_finetuned'].mean()),\n",
        "                        'num_clients': len(eval_df),\n",
        "                        'status': 'completed'\n",
        "                    })\n",
        "\n",
        "                    # è¨ˆç®—æ”¹å–„å¹…åº¦\n",
        "                    if result_summary['avg_global_reward'] > 0:\n",
        "                        improvement = ((result_summary['avg_personalized_reward'] /\n",
        "                                       result_summary['avg_global_reward'] - 1) * 100)\n",
        "                        result_summary['personalization_improvement'] = improvement\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ {mode} è©•ä¼°çµæœè®€å–å¤±æ•—: {e}\")\n",
        "                result_summary['status'] = 'failed'\n",
        "\n",
        "        # åˆ†æéš±ç§æˆæœ¬\n",
        "        if privacy_files:\n",
        "            try:\n",
        "                privacy_df = pd.read_csv(privacy_files[0])\n",
        "                if not privacy_df.empty:\n",
        "                    final_epsilon = privacy_df['cumulative_epsilon'].iloc[-1]\n",
        "                    result_summary.update({\n",
        "                        'final_epsilon': float(final_epsilon),\n",
        "                        'privacy_rounds': len(privacy_df)\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ {mode} éš±ç§æˆæœ¬è®€å–å¤±æ•—: {e}\")\n",
        "\n",
        "        # åˆ†æè¨“ç·´æ­·å²\n",
        "        if history_files:\n",
        "            try:\n",
        "                history_df = pd.read_csv(history_files[0])\n",
        "                if not history_df.empty:\n",
        "                    final_reward = history_df['avg_reward'].iloc[-1]\n",
        "                    result_summary.update({\n",
        "                        'final_training_reward': float(final_reward),\n",
        "                        'training_rounds': len(history_df)\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ {mode} è¨“ç·´æ­·å²è®€å–å¤±æ•—: {e}\")\n",
        "\n",
        "        summary_data.append(result_summary)\n",
        "\n",
        "        # é¡¯ç¤ºå€‹åˆ¥çµæœ\n",
        "        print(f\"\\nğŸ“‹ {mode} å¯¦é©—çµæœ:\")\n",
        "        print(f\"   - ç‹€æ…‹: {result_summary['status']}\")\n",
        "        if result_summary['status'] == 'completed':\n",
        "            print(f\"   - å…¨åŸŸçå‹µ: {result_summary.get('avg_global_reward', 0):.4f}\")\n",
        "            print(f\"   - å€‹äººåŒ–çå‹µ: {result_summary.get('avg_personalized_reward', 0):.4f}\")\n",
        "            print(f\"   - PFLçå‹µ: {result_summary.get('avg_pfl_reward', 0):.4f}\")\n",
        "            if 'personalization_improvement' in result_summary:\n",
        "                print(f\"   - å€‹äººåŒ–æå‡: {result_summary['personalization_improvement']:.2f}%\")\n",
        "            if 'final_epsilon' in result_summary:\n",
        "                print(f\"   - æœ€çµ‚Îµæ¶ˆè€—: {result_summary['final_epsilon']:.4f}\")\n",
        "        else:\n",
        "            print(f\"   - âš ï¸ å¯¦é©—æœªå®Œæˆæˆ–å¤±æ•—\")\n",
        "\n",
        "    return summary_data\n",
        "\n",
        "# åŸ·è¡Œçµæœå½™ç¸½\n",
        "print(f\"ğŸ” æ­£åœ¨æƒæçµæœç›®éŒ„: {BASE_OUTPUT_DIR}\")\n",
        "summary_results = summarize_experiment_results(BASE_OUTPUT_DIR, seed=42)\n",
        "\n",
        "# å‰µå»ºå½™ç¸½DataFrame\n",
        "if summary_results:\n",
        "    summary_df = pd.DataFrame(summary_results)\n",
        "\n",
        "    print(f\"\\nğŸ“Š å¯¦é©—å½™ç¸½è¡¨:\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # é¡¯ç¤ºå®Œæˆç‹€æ…‹\n",
        "    completed_experiments = summary_df[summary_df['status'] == 'completed']\n",
        "    failed_experiments = summary_df[summary_df['status'] == 'failed']\n",
        "\n",
        "    print(f\"âœ… å®Œæˆå¯¦é©—æ•¸: {len(completed_experiments)}/3\")\n",
        "    print(f\"âŒ å¤±æ•—å¯¦é©—æ•¸: {len(failed_experiments)}/3\")\n",
        "\n",
        "    if not completed_experiments.empty:\n",
        "        print(f\"\\nğŸ† å®Œæˆå¯¦é©—æ€§èƒ½æ¯”è¼ƒ:\")\n",
        "        for _, row in completed_experiments.iterrows():\n",
        "            mode = row['mode']\n",
        "            global_reward = row.get('avg_global_reward', 0)\n",
        "            pfl_reward = row.get('avg_pfl_reward', 0)\n",
        "            improvement = row.get('personalization_improvement', 0)\n",
        "            epsilon = row.get('final_epsilon', 0)\n",
        "\n",
        "            print(f\"   {mode:12}: å…¨åŸŸ={global_reward:7.3f}, PFL={pfl_reward:7.3f}, \"\n",
        "                  f\"æå‡={improvement:6.2f}%, Îµ={epsilon:6.3f}\")\n",
        "\n",
        "    if not failed_experiments.empty:\n",
        "        print(f\"\\nâŒ å¤±æ•—å¯¦é©—:\")\n",
        "        for _, row in failed_experiments.iterrows():\n",
        "            print(f\"   - {row['mode']}\")\n",
        "\n",
        "    # ä¿å­˜å½™ç¸½çµæœ\n",
        "    summary_output_path = os.path.join(BASE_OUTPUT_DIR, \"experiment_summary.csv\")\n",
        "    try:\n",
        "        summary_df.to_csv(summary_output_path, index=False)\n",
        "        print(f\"\\nğŸ’¾ å½™ç¸½çµæœå·²ä¿å­˜è‡³: {summary_output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ å½™ç¸½çµæœä¿å­˜å¤±æ•—: {e}\")\n",
        "\n",
        "    # è™›æ“¬å®¢æˆ¶ç«¯æ–¹æ³•ç¸½çµ\n",
        "    print(f\"\\nğŸ”„ è™›æ“¬å®¢æˆ¶ç«¯æ–¹æ³•æˆæœ:\")\n",
        "    print(f\"   - æ•¸æ“šæ“´å¢: 3 â†’ 12 å®¢æˆ¶ç«¯ (4å€å¢é•·)\")\n",
        "    print(f\"   - å­¸è¡“åˆç†æ€§: âœ… æ™‚é–“åˆ†å‰² + ç‰¹å¾µå¢å¼·\")\n",
        "    print(f\"   - éš±ç§ä¿è­·: âœ… éé‡ç–Šæ•¸æ“š + DPå™ªè²\")\n",
        "    print(f\"   - å®Œæˆç‡: {len(completed_experiments)}/3 = {len(completed_experiments)/3*100:.1f}%\")\n",
        "\n",
        "    if len(completed_experiments) > 0:\n",
        "        avg_global = completed_experiments['avg_global_reward'].mean()\n",
        "        avg_pfl = completed_experiments['avg_pfl_reward'].mean()\n",
        "        avg_improvement = completed_experiments.get('personalization_improvement', pd.Series([0])).mean()\n",
        "\n",
        "        print(f\"   - å¹³å‡å…¨åŸŸçå‹µ: {avg_global:.3f}\")\n",
        "        print(f\"   - å¹³å‡PFLçå‹µ: {avg_pfl:.3f}\")\n",
        "        print(f\"   - å¹³å‡å€‹äººåŒ–æå‡: {avg_improvement:.2f}%\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ æœªæ‰¾åˆ°ä»»ä½•å¯¦é©—çµæœ\")\n",
        "\n",
        "print(\"\\nâœ… Cell 8.6: å¯¦é©—çµæœå½™ç¸½å®Œæˆ\")\n",
        ""
      ],
      "metadata": {
        "cellView": "form",
        "id": "cy7n1km4MJj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 8.7: ğŸ”„ å¿«é€Ÿé‡æ–°åŸ·è¡Œå¤±æ•—å¯¦é©—ï¼ˆå¯é¸ï¼‰\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def check_failed_experiments(base_output_dir, seed=42):\n",
        "    \"\"\"æª¢æŸ¥å¤±æ•—çš„å¯¦é©—\"\"\"\n",
        "    modes = [\"ClusteredFL\", \"FedProx\", \"FedAvg\"]\n",
        "    failed_modes = []\n",
        "\n",
        "    for mode in modes:\n",
        "        exp_path = os.path.join(base_output_dir, f\"seed_{seed}\", mode)\n",
        "        eval_files = glob.glob(os.path.join(exp_path, '*_evaluation_results.csv'))\n",
        "\n",
        "        # æª¢æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„è©•ä¼°çµæœ\n",
        "        has_complete_results = False\n",
        "        if eval_files:\n",
        "            try:\n",
        "                eval_df = pd.read_csv(eval_files[0])\n",
        "                if not eval_df.empty and len(eval_df) > 0:\n",
        "                    has_complete_results = True\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if not has_complete_results:\n",
        "            failed_modes.append(mode)\n",
        "\n",
        "    return failed_modes\n",
        "\n",
        "# æª¢æŸ¥å¤±æ•—çš„å¯¦é©—\n",
        "failed_modes = check_failed_experiments(BASE_OUTPUT_DIR, seed=42)\n",
        "\n",
        "if failed_modes:\n",
        "    print(f\"ğŸ” ç™¼ç¾å¤±æ•—å¯¦é©—: {failed_modes}\")\n",
        "    print(f\"ğŸ’¡ æ‚¨å¯ä»¥å€‹åˆ¥é‡æ–°åŸ·è¡Œå°æ‡‰çš„Cell:\")\n",
        "    for mode in failed_modes:\n",
        "        if mode == \"ClusteredFL\":\n",
        "            print(f\"   - é‡æ–°åŸ·è¡Œ Cell 8.3 (ClusteredFL)\")\n",
        "        elif mode == \"FedProx\":\n",
        "            print(f\"   - é‡æ–°åŸ·è¡Œ Cell 8.4 (FedProx)\")\n",
        "        elif mode == \"FedAvg\":\n",
        "            print(f\"   - é‡æ–°åŸ·è¡Œ Cell 8.5 (FedAvg)\")\n",
        "\n",
        "    print(f\"\\nâš ï¸ å»ºè­°:\")\n",
        "    print(f\"   1. æª¢æŸ¥GPUè¨˜æ†¶é«”æ˜¯å¦å……è¶³\")\n",
        "    print(f\"   2. é‡æ–°å•Ÿå‹•Runtimeå¦‚æœè¨˜æ†¶é«”ä¸è¶³\")\n",
        "    print(f\"   3. é€ä¸€åŸ·è¡Œå¤±æ•—çš„å¯¦é©—Cell\")\n",
        "\n",
        "    # æä¾›å¿«é€Ÿé‡åŸ·è¡Œé¸é …\n",
        "    print(f\"\\nğŸš€ å¿«é€Ÿé‡åŸ·è¡Œç¬¬ä¸€å€‹å¤±æ•—å¯¦é©—:\")\n",
        "    if failed_modes:\n",
        "        first_failed = failed_modes[0]\n",
        "        print(f\"å³å°‡é‡æ–°åŸ·è¡Œ: {first_failed}\")\n",
        "\n",
        "        # é€™è£¡å¯ä»¥å–æ¶ˆè¨»è§£ä¾†è‡ªå‹•é‡åŸ·è¡Œ\n",
        "        # config_params = get_base_config(first_failed, 42)\n",
        "        # success, result_info = run_virtual_client_experiment(config_params, DATA_PATH)\n",
        "        # if success:\n",
        "        #     print(f\"âœ… {first_failed} é‡åŸ·è¡ŒæˆåŠŸ\")\n",
        "        # else:\n",
        "        #     print(f\"âŒ {first_failed} é‡åŸ·è¡Œä»ç„¶å¤±æ•—\")\n",
        "else:\n",
        "    print(\"âœ… æ‰€æœ‰å¯¦é©—éƒ½å·²æˆåŠŸå®Œæˆ\")\n",
        "\n",
        "print(\"âœ… Cell 8.7: å¤±æ•—å¯¦é©—æª¢æŸ¥å®Œæˆ\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3LRr-ckWMaLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTjF-hQJwzzf"
      },
      "source": [
        "### æ¥ä¸‹ä¾†\n",
        "* æ“´å±•å®¢æˆ¶ç«¯æ•¸é‡ç ”ç©¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vPm31hDNISu5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Cell 8: ğŸ¬ ä¸»è¦åŸ·è¡Œå€å¡Šï¼ˆéŒ¯èª¤ä¿®æ­£ç‰ˆï¼‰\n",
        "import time\n",
        "import gc\n",
        "from datetime import datetime\n",
        "\n",
        "def run_virtual_client_experiment(config_dict: dict, data_path: str):\n",
        "    \"\"\"\n",
        "    é‹è¡ŒåŸºæ–¼è™›æ“¬å®¢æˆ¶ç«¯æ“´å¢çš„å¯¦é©—\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        config = TrainingConfig(**config_dict)\n",
        "        print(f\"\\n{'='*15} ğŸ”„ è™›æ“¬å®¢æˆ¶ç«¯DPå¯¦é©—: {config.experiment_name} {'='*15}\")\n",
        "\n",
        "        # 1. æº–å‚™åŸå§‹æ•¸æ“š\n",
        "        print(f\"ğŸ“Š æ­£åœ¨æº–å‚™åŸå§‹æ•¸æ“š...\")\n",
        "        data_manager = DataManager(data_path, config.base_client_pairs)\n",
        "        original_trajectories = data_manager.get_client_trajectories()\n",
        "\n",
        "        print(f\"   - åŸå§‹æ•¸æ“šæº: {len(original_trajectories)} å€‹çœŸå¯¦å®¢æˆ¶ç«¯\")\n",
        "        for cid, traj in original_trajectories.items():\n",
        "            print(f\"     å®¢æˆ¶ç«¯ {cid}: {len(traj)} æ•¸æ“šé»\")\n",
        "\n",
        "        # 2. ç”Ÿæˆè™›æ“¬å®¢æˆ¶ç«¯\n",
        "        print(f\"\\nğŸ”„ é–‹å§‹è™›æ“¬å®¢æˆ¶ç«¯ç”Ÿæˆ...\")\n",
        "        virtual_generator = VirtualClientGenerator(config)\n",
        "        all_trajectories = virtual_generator.generate_virtual_clients(original_trajectories)\n",
        "\n",
        "        # 3. ç²å–ç”Ÿæˆçµ±è¨ˆ\n",
        "        virtual_info = virtual_generator.get_virtual_client_info()\n",
        "        print(f\"\\nğŸ“ˆ è™›æ“¬å®¢æˆ¶ç«¯ç”Ÿæˆçµ±è¨ˆ:\")\n",
        "        print(f\"   - çœŸå¯¦å®¢æˆ¶ç«¯: {virtual_info['base_clients']}\")\n",
        "        print(f\"   - è™›æ“¬å®¢æˆ¶ç«¯: {virtual_info['virtual_clients_generated']}\")\n",
        "        print(f\"   - ç”Ÿæˆå¤±æ•—: {virtual_info['failed_generations']}\")\n",
        "        print(f\"   - ç¸½è¨ˆå®¢æˆ¶ç«¯: {virtual_info['total_clients']}\")\n",
        "        print(f\"   - æˆåŠŸç‡: {virtual_info['generation_success_rate']:.2%}\")\n",
        "        print(f\"   - åˆ†å‰²æ–¹æ³•: {virtual_info['split_method']}\")\n",
        "        print(f\"   - ç¸½æ•¸æ“šé»: {virtual_info['total_data_points']:,}\")\n",
        "\n",
        "        # 4. ã€ä¿®æ­£ã€‘æ•¸æ“šè³ªé‡é©—è­‰ - è™•ç†numpyé™£åˆ—æ ¼å¼åŒ–å•é¡Œ\n",
        "        print(f\"\\nğŸ” æ•¸æ“šè³ªé‡é©—è­‰:\")\n",
        "        for cid, traj in list(all_trajectories.items())[:5]:  # é¡¯ç¤ºå‰5å€‹çš„çµ±è¨ˆ\n",
        "            if len(traj) > 0:\n",
        "                # è¨ˆç®—å‡å€¼å’Œæ¨™æº–å·®\n",
        "                if traj.ndim == 1:\n",
        "                    mean_val = np.mean(traj)\n",
        "                    std_val = np.std(traj)\n",
        "                else:\n",
        "                    mean_val = np.mean(traj, axis=0)\n",
        "                    std_val = np.std(traj, axis=0)\n",
        "\n",
        "                client_type = \"çœŸå¯¦\" if cid < config.num_real_clients else \"è™›æ“¬\"\n",
        "\n",
        "                # ã€ä¿®æ­£ã€‘è™•ç†numpyé™£åˆ—æ ¼å¼åŒ–å•é¡Œ\n",
        "                if isinstance(mean_val, np.ndarray):\n",
        "                    if mean_val.size == 1:\n",
        "                        mean_str = f\"{mean_val.item():.4f}\"\n",
        "                    else:\n",
        "                        mean_str = \"[\" + \",\".join([f\"{x:.4f}\" for x in mean_val.flatten()[:3]]) + \"]\"\n",
        "                else:\n",
        "                    mean_str = f\"{float(mean_val):.4f}\"\n",
        "\n",
        "                if isinstance(std_val, np.ndarray):\n",
        "                    if std_val.size == 1:\n",
        "                        std_str = f\"{std_val.item():.4f}\"\n",
        "                    else:\n",
        "                        std_str = \"[\" + \",\".join([f\"{x:.4f}\" for x in std_val.flatten()[:3]]) + \"]\"\n",
        "                else:\n",
        "                    std_str = f\"{float(std_val):.4f}\"\n",
        "\n",
        "                print(f\"   - å®¢æˆ¶ç«¯ {cid} ({client_type}): é•·åº¦={len(traj)}, å‡å€¼={mean_str}, æ¨™å·®={std_str}\")\n",
        "\n",
        "        # 5. æ›´æ–°é…ç½®ä»¥åŒ¹é…ç”Ÿæˆçš„å®¢æˆ¶ç«¯\n",
        "        config.num_clients = len(all_trajectories)\n",
        "        config.num_clients_to_select = min(config.num_clients_to_select, config.num_clients)\n",
        "\n",
        "        # 6. å‰µå»ºå…¼å®¹çš„å®¢æˆ¶ç«¯é…å°\n",
        "        extended_client_pairs = []\n",
        "        for i in range(config.num_clients):\n",
        "            embb_id = i + 1\n",
        "            urllc_id = i + 2\n",
        "            extended_client_pairs.append((embb_id, urllc_id))\n",
        "        config.client_pairs = tuple(extended_client_pairs)\n",
        "\n",
        "        # 7. å·®åˆ†éš±ç§é…ç½®æª¢æŸ¥\n",
        "        if config.enable_dp and config.mode != 'Centralized':\n",
        "            print(f\"\\nğŸ›¡ï¸ å·®åˆ†éš±ç§é…ç½®:\")\n",
        "            print(f\"   - ç›®æ¨™Îµ: {config.dp_target_epsilon}\")\n",
        "            print(f\"   - æ‰¹æ¬¡å¤§å°: {config.batch_size}\")\n",
        "            print(f\"   - å™ªè²ä¹˜æ•¸: {config.dp_noise_multiplier}\")\n",
        "            print(f\"   - å®¢æˆ¶ç«¯æ•¸é‡: {config.num_clients} (å¤§å¹…å¢åŠ )\")\n",
        "            print(f\"   - æ¯è¼ªåƒèˆ‡: {config.num_clients_to_select}\")\n",
        "            print(f\"   - é‡è¨­æ©Ÿåˆ¶: {'å•Ÿç”¨' if config.enable_dp_reset else 'ç¦ç”¨'}\")\n",
        "\n",
        "        # 8. é‹è¡Œå¯¦é©—\n",
        "        print(f\"\\nğŸš€ é–‹å§‹è¯é‚¦å­¸ç¿’å¯¦é©—...\")\n",
        "        runner = ExperimentRunner(config, data_manager, all_trajectories, config.client_pairs)\n",
        "        eval_res, history_res = runner.run()\n",
        "\n",
        "        execution_time = (time.time() - start_time) / 60\n",
        "        print(f\"\\nâ±ï¸ å¯¦é©—å®Œæˆæ™‚é–“: {execution_time:.2f} åˆ†é˜\")\n",
        "\n",
        "        # 9. ã€ä¿®æ­£ã€‘çµæœåˆ†æ - è™•ç†ç©ºçµæœæƒ…æ³\n",
        "        print(f\"\\nâœ… å¯¦é©—çµæœæ‘˜è¦:\")\n",
        "        if not eval_res.empty and len(eval_res) > 0:\n",
        "            avg_global_reward = eval_res['reward_global'].mean()\n",
        "            avg_personalized_reward = eval_res['reward_personalized'].mean()\n",
        "            avg_pfl_reward = eval_res['reward_pfl_finetuned'].mean()\n",
        "\n",
        "            print(f\"   - å¹³å‡å…¨åŸŸçå‹µ: {avg_global_reward:.4f}\")\n",
        "            print(f\"   - å¹³å‡å€‹äººåŒ–çå‹µ: {avg_personalized_reward:.4f}\")\n",
        "            print(f\"   - å¹³å‡PFLå¾®èª¿çå‹µ: {avg_pfl_reward:.4f}\")\n",
        "            print(f\"   - å€‹äººåŒ–æå‡: {((avg_personalized_reward/avg_global_reward-1)*100):.2f}%\")\n",
        "        else:\n",
        "            print(f\"   - âš ï¸ è©•ä¼°çµæœç‚ºç©ºï¼Œå¯èƒ½å¯¦é©—æœªå®Œæˆ\")\n",
        "            avg_global_reward = avg_personalized_reward = avg_pfl_reward = 0.0\n",
        "\n",
        "        # 10. éš±ç§åˆ†æ\n",
        "        if config.enable_dp and config.mode != 'Centralized':\n",
        "            print(f\"\\nğŸ›¡ï¸ è™›æ“¬å®¢æˆ¶ç«¯éš±ç§åˆ†æ:\")\n",
        "            print(f\"   - æ•¸æ“šç¨ç«‹æ€§: âœ… æ™‚é–“åˆ†å‰²ç¢ºä¿éé‡ç–Š\")\n",
        "            print(f\"   - éš±ç§ä¿è­·: âœ… DPå™ªè² (Ïƒ={config.noise_injection_std})\")\n",
        "            print(f\"   - çµ±è¨ˆçœŸå¯¦æ€§: âœ… ä¿æŒåŸå§‹åˆ†ä½ˆç‰¹æ€§\")\n",
        "            print(f\"   - æœ€çµ‚Îµæ¶ˆè€—: {runner.consumed_privacy_budget:.4f}\")\n",
        "            print(f\"   - é ç®—ä½¿ç”¨ç‡: {(runner.consumed_privacy_budget/config.dp_target_epsilon*100):.1f}%\")\n",
        "            print(f\"   - é‡è¨­æ¬¡æ•¸: {len(runner.dp_reset_history)}\")\n",
        "\n",
        "            # éš±ç§æ•ˆç‡åˆ†æ\n",
        "            if runner.consumed_privacy_budget > 0 and avg_global_reward > 0:\n",
        "                privacy_efficiency = avg_global_reward / runner.consumed_privacy_budget\n",
        "                print(f\"   - éš±ç§æ•ˆç‡: {privacy_efficiency:.4f} (çå‹µ/Îµ)\")\n",
        "\n",
        "            if runner.privacy_budget_exceeded:\n",
        "                print(f\"   - âš ï¸ éš±ç§é ç®—å·²è¶…æ”¯ - å•Ÿç”¨é‡è¨­æ©Ÿåˆ¶ç®¡æ§\")\n",
        "            else:\n",
        "                print(f\"   - âœ… éš±ç§é ç®—æ§åˆ¶è‰¯å¥½\")\n",
        "\n",
        "        # 11. ä¿å­˜è©³ç´°çµæœ\n",
        "        if not eval_res.empty and len(eval_res) > 0:\n",
        "            eval_summary = eval_res[['client_id', 'reward_global', 'reward_personalized', 'reward_pfl_finetuned']].round(4)\n",
        "            print(f\"\\nğŸ“Š è©³ç´°çµæœ (å‰10å€‹å®¢æˆ¶ç«¯):\")\n",
        "            print(eval_summary.head(10).to_string(index=False))\n",
        "\n",
        "        # 12. æ¸…ç†è³‡æº\n",
        "        del runner, data_manager, virtual_generator, all_trajectories\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return True, {\n",
        "            'virtual_info': virtual_info,\n",
        "            'execution_time': execution_time,\n",
        "            'avg_rewards': {\n",
        "                'global': avg_global_reward,\n",
        "                'personalized': avg_personalized_reward,\n",
        "                'pfl': avg_pfl_reward\n",
        "            },\n",
        "            'privacy_stats': {\n",
        "                'consumed_epsilon': runner.consumed_privacy_budget if config.enable_dp else 0,\n",
        "                'reset_count': len(runner.dp_reset_history) if config.enable_dp else 0\n",
        "            } if config.enable_dp else None\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        execution_time = (time.time() - start_time) / 60\n",
        "        print(f\"\\nâŒ è™›æ“¬å®¢æˆ¶ç«¯å¯¦é©—å¤±æ•—: {config_dict.get('experiment_name')}\")\n",
        "        print(f\"â±ï¸ å¤±æ•—æ™‚é–“: {execution_time:.2f} åˆ†é˜\")\n",
        "        print(f\"ğŸ” éŒ¯èª¤è©³æƒ…: {str(e)}\")\n",
        "        import traceback\n",
        "        print(f\"ğŸ“‹ éŒ¯èª¤å †ç–Š: {traceback.format_exc()}\")\n",
        "\n",
        "        # æ¸…ç†è³‡æº\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        return False, None\n",
        "\n",
        "# ==================== ä¸»è¦åŸ·è¡Œå€å¡Š ====================\n",
        "\n",
        "# GPUç’°å¢ƒè¨­å®š\n",
        "setup_gpu_environment()\n",
        "\n",
        "# ç’°å¢ƒè·¯å¾‘è¨­å®š\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_WORK_DIR = \"/content/drive/MyDrive/FRL_Slicing_Sim\"\n",
        "    print(\"ğŸ”— Google Drive æ›è¼‰æˆåŠŸ\")\n",
        "except ImportError:\n",
        "    BASE_WORK_DIR = \"./FRL_Slicing_Sim\"\n",
        "    print(\"ğŸ’» æœ¬åœ°ç’°å¢ƒæ¨¡å¼\")\n",
        "\n",
        "os.makedirs(BASE_WORK_DIR, exist_ok=True)\n",
        "DATA_PATH = os.path.join(BASE_WORK_DIR, \"kpi_traces_final_robust0.parquet\")\n",
        "print(f\"ğŸ“ æ•¸æ“šè·¯å¾‘: {DATA_PATH}\")\n",
        "\n",
        "# å¯¦é©—é…ç½®\n",
        "MODES_TO_RUN = [\"ClusteredFL\", \"FedProx\", \"FedAvg\"]  # é‡é»æ¸¬è©¦è¯é‚¦æ–¹æ³•\n",
        "SEEDS = [42]  # ä½¿ç”¨å…©å€‹ç¨®å­é©—è­‰ç©©å®šæ€§\n",
        "# SEEDS = [42, ]  # ä½¿ç”¨å…©å€‹ç¨®å­é©—è­‰ç©©å®šæ€§\n",
        "BASE_OUTPUT_DIR = os.path.join(BASE_WORK_DIR, \"outputs_virtual_clients\")\n",
        "\n",
        "print(f\"\\nğŸ¯ å¯¦é©—è¨ˆåŠƒ:\")\n",
        "print(f\"   - æ¸¬è©¦æ¨¡å¼: {MODES_TO_RUN}\")\n",
        "print(f\"   - éš¨æ©Ÿç¨®å­: {SEEDS}\")\n",
        "print(f\"   - ç¸½å¯¦é©—æ•¸: {len(MODES_TO_RUN) * len(SEEDS)}\")\n",
        "print(f\"   - è¼¸å‡ºç›®éŒ„: {BASE_OUTPUT_DIR}\")\n",
        "\n",
        "# åŸ·è¡Œè™›æ“¬å®¢æˆ¶ç«¯å¯¦é©—\n",
        "total_start_time = time.time()\n",
        "successful_experiments = 0\n",
        "total_experiments = len(SEEDS) * len(MODES_TO_RUN)\n",
        "experiment_results = []\n",
        "\n",
        "print(f\"\\nğŸš€ é–‹å§‹è™›æ“¬å®¢æˆ¶ç«¯è¯é‚¦å­¸ç¿’å¯¦é©—...\")\n",
        "\n",
        "for seed_idx, seed in enumerate(SEEDS):\n",
        "    print(f\"\\n{'='*20} ç¨®å­ {seed} ({seed_idx+1}/{len(SEEDS)}) {'='*20}\")\n",
        "\n",
        "    for mode_idx, mode in enumerate(MODES_TO_RUN):\n",
        "        exp_num = seed_idx * len(MODES_TO_RUN) + mode_idx + 1\n",
        "        exp_name = f\"{mode}_virtual_clients_s{seed}\"\n",
        "        output_dir = os.path.join(BASE_OUTPUT_DIR, f\"seed_{seed}\", mode)\n",
        "\n",
        "        print(f\"\\n--- å¯¦é©— {exp_num}/{total_experiments}: {mode} (ç¨®å­={seed}) ---\")\n",
        "\n",
        "        # è™›æ“¬å®¢æˆ¶ç«¯å¯¦é©—é…ç½®\n",
        "        config_params = {\n",
        "            \"experiment_name\": exp_name,\n",
        "            \"output_dir\": output_dir,\n",
        "            \"mode\": mode,\n",
        "            \"random_seed\": seed,\n",
        "\n",
        "            # ã€ä¿®æ”¹5ã€‘è™›æ“¬å®¢æˆ¶ç«¯æ“´å¢è¨­å®š\n",
        "            \"base_client_pairs\": ((1, 2), (3, 7), (5, 6)),  # çœŸå¯¦åŸºç«™å°\n",
        "            \"virtual_expansion_factor\": 4,  # æ¯å€‹åŸºç«™ç”Ÿæˆ3å€‹è™›æ“¬å®¢æˆ¶ç«¯\n",
        "            \"total_clients\": 12,           # 3çœŸå¯¦ + 9è™›æ“¬ = 12\n",
        "            \"num_clients_to_select\": 8,    # å¾12å€‹ä¸­é¸8å€‹åƒèˆ‡\n",
        "\n",
        "            # è¨“ç·´åƒæ•¸å„ªåŒ–\n",
        "            \"comm_rounds\": 15,             # é©ä¸­çš„å›åˆæ•¸\n",
        "            \"local_episodes_per_round\": 2,  # ã€ä¿®æ”¹4ã€‘å¢åŠ æœ¬åœ°è¨“ç·´\n",
        "            \"steps_per_episode\": 500,      # ã€ä¿®æ”¹4ã€‘å¢åŠ æ­¥æ•¸\n",
        "            \"batch_size\": 128,             # ã€ä¿®æ”¹4ã€‘å¤§æ‰¹æ¬¡\n",
        "            \"memory_capacity\": 50000,     # å¢å¤§è¨˜æ†¶å®¹é‡\n",
        "            \"replay_batches_per_call\": 3,  # æ›´å¤šå›æ”¾æ‰¹æ¬¡\n",
        "\n",
        "            # è¯é‚¦å­¸ç¿’åƒæ•¸\n",
        "            \"fedprox_mu\": 0.15,           # ã€ä¿®æ”¹6ã€‘å¼·æ­£å‰‡åŒ–\n",
        "            \"num_clusters\": 3,            # é›†ç¾¤æ•¸é‡\n",
        "            \"cluster_update_freq\": 8,     # é›†ç¾¤æ›´æ–°é »ç‡\n",
        "\n",
        "            # è™›æ“¬å®¢æˆ¶ç«¯ç”Ÿæˆåƒæ•¸\n",
        "            \"temporal_split_method\": \"sliding_window\",\n",
        "            \"noise_injection_std\": 0.03,    # é©åº¦å™ªè²\n",
        "            \"feature_augmentation\": True,\n",
        "            \"cross_validation_split\": True,\n",
        "\n",
        "            # å·®åˆ†éš±ç§åƒæ•¸\n",
        "            \"enable_dp\": True,\n",
        "            \"dp_target_epsilon\": 8.0,\n",
        "            \"dp_target_delta\": 1e-5,\n",
        "            \"dp_noise_multiplier\": 0.3,   # ã€ä¿®æ”¹4ã€‘é…åˆå¤§æ‰¹æ¬¡\n",
        "            \"dp_max_grad_norm\": 1.0,\n",
        "            \"enable_dp_reset\": True,\n",
        "            \"dp_reset_threshold_multiplier\": 1.5,\n",
        "\n",
        "            # å…¶ä»–åƒæ•¸\n",
        "            \"lr\": 1e-4,\n",
        "            \"gamma\": 0.99,\n",
        "            \"enable_heterogeneity\": True,\n",
        "            \"enable_compression\": True,\n",
        "            \"use_pfl_finetune\": True,\n",
        "        }\n",
        "\n",
        "        print(f\"ğŸ”§ é…ç½®æ‘˜è¦:\")\n",
        "        print(f\"   - å®¢æˆ¶ç«¯: {config_params['total_clients']} (çœŸå¯¦:3, è™›æ“¬:9)\")\n",
        "        print(f\"   - æ‰¹æ¬¡å¤§å°: {config_params['batch_size']}\")\n",
        "        print(f\"   - FedProx Î¼: {config_params['fedprox_mu']}\")\n",
        "        print(f\"   - DPå™ªè²: {config_params['dp_noise_multiplier']}\")\n",
        "\n",
        "        # åŸ·è¡Œå¯¦é©—\n",
        "        success, result_info = run_virtual_client_experiment(config_params, DATA_PATH)\n",
        "\n",
        "        if success:\n",
        "            successful_experiments += 1\n",
        "            experiment_results.append({\n",
        "                'seed': seed,\n",
        "                'mode': mode,\n",
        "                'success': True,\n",
        "                'result_info': result_info\n",
        "            })\n",
        "            print(f\"âœ… {mode} å¯¦é©—æˆåŠŸ\")\n",
        "\n",
        "            # é¡¯ç¤ºé—œéµæŒ‡æ¨™\n",
        "            if result_info and 'avg_rewards' in result_info:\n",
        "                rewards = result_info['avg_rewards']\n",
        "                print(f\"   - å…¨åŸŸçå‹µ: {rewards['global']:.4f}\")\n",
        "                print(f\"   - å€‹äººåŒ–çå‹µ: {rewards['personalized']:.4f}\")\n",
        "                if result_info.get('privacy_stats'):\n",
        "                    privacy = result_info['privacy_stats']\n",
        "                    print(f\"   - Îµæ¶ˆè€—: {privacy['consumed_epsilon']:.4f}\")\n",
        "                    print(f\"   - é‡è¨­æ¬¡æ•¸: {privacy['reset_count']}\")\n",
        "        else:\n",
        "            experiment_results.append({\n",
        "                'seed': seed,\n",
        "                'mode': mode,\n",
        "                'success': False,\n",
        "                'result_info': None\n",
        "            })\n",
        "            print(f\"âŒ {mode} å¯¦é©—å¤±æ•—\")\n",
        "\n",
        "        # æ¸…ç†å’Œä¼‘æ¯\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        time.sleep(2)\n",
        "\n",
        "# å¯¦é©—ç¸½çµ\n",
        "total_time = (time.time() - total_start_time) / 60\n",
        "success_rate = (successful_experiments / total_experiments) * 100\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"ğŸ‰ è™›æ“¬å®¢æˆ¶ç«¯è¯é‚¦å­¸ç¿’å¯¦é©—å®Œæˆï¼\")\n",
        "print(f\"â±ï¸ ç¸½åŸ·è¡Œæ™‚é–“: {total_time:.2f} åˆ†é˜\")\n",
        "print(f\"âœ… æˆåŠŸç‡: {success_rate:.1f}% ({successful_experiments}/{total_experiments})\")\n",
        "\n",
        "# æˆåŠŸå¯¦é©—çµ±è¨ˆ\n",
        "successful_results = [r for r in experiment_results if r['success']]\n",
        "if successful_results:\n",
        "    print(f\"\\nğŸ“Š æˆåŠŸå¯¦é©—çµ±è¨ˆ:\")\n",
        "    for mode in MODES_TO_RUN:\n",
        "        mode_results = [r for r in successful_results if r['mode'] == mode]\n",
        "        if mode_results:\n",
        "            avg_global_reward = np.mean([r['result_info']['avg_rewards']['global'] for r in mode_results])\n",
        "            avg_personalized_reward = np.mean([r['result_info']['avg_rewards']['personalized'] for r in mode_results])\n",
        "            avg_epsilon = np.mean([r['result_info']['privacy_stats']['consumed_epsilon'] for r in mode_results if r['result_info']['privacy_stats']])\n",
        "\n",
        "            print(f\"   - {mode}:\")\n",
        "            print(f\"     å…¨åŸŸçå‹µ: {avg_global_reward:.4f}\")\n",
        "            print(f\"     å€‹äººåŒ–çå‹µ: {avg_personalized_reward:.4f}\")\n",
        "            print(f\"     å¹³å‡Îµæ¶ˆè€—: {avg_epsilon:.4f}\")\n",
        "\n",
        "print(f\"\\nğŸ”„ è™›æ“¬å®¢æˆ¶ç«¯æ–¹æ³•å„ªå‹¢:\")\n",
        "print(f\"   - å®¢æˆ¶ç«¯æ“´å¢: 3 â†’ 12 (4å€å¢é•·)\")\n",
        "print(f\"   - å­¸è¡“åˆç†æ€§: âœ… åŸºæ–¼æ™‚é–“åˆ†å‰²çš„æ¨™æº–æ–¹æ³•\")\n",
        "print(f\"   - éš±ç§ä¿è­·: âœ… éé‡ç–Šæ•¸æ“š + DPå™ªè²\")\n",
        "print(f\"   - çµ±è¨ˆå¤šæ¨£æ€§: âœ… ä¿æŒåŸå§‹æ•¸æ“šç‰¹æ€§\")\n",
        "print(f\"   - è¨ˆç®—æ•ˆç‡: âœ… GPUå„ªåŒ–å¤§æ‰¹æ¬¡è¨“ç·´\")\n",
        "print(f\"   - é æœŸæ”¹å–„: æ¨¡å‹æ³›åŒ–æ€§â†‘, Îµæ§åˆ¶â†‘, æ”¶æ–‚ç©©å®šæ€§â†‘\")\n",
        "\n",
        "print(f\"\\nğŸ“ çµæœå·²ä¿å­˜è‡³: {BASE_OUTPUT_DIR}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "izZVG_0CIV2e"
      },
      "outputs": [],
      "source": [
        "# @title Cell 9: ğŸ“Š çµæœè¦–è¦ºåŒ–ï¼ˆè‹±æ–‡ç‰ˆï¼‰\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "def load_all_results(base_output_dir):\n",
        "    all_evals, all_histories, all_privacies = [], [], []\n",
        "    config_data = None\n",
        "\n",
        "    if not os.path.exists(base_output_dir):\n",
        "        print(f\"âŒ Results directory not found: {base_output_dir}\")\n",
        "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), None\n",
        "\n",
        "    # Try to load configuration files first\n",
        "    config_files = glob.glob(os.path.join(base_output_dir, '**', '*_config.json'), recursive=True)\n",
        "    if config_files:\n",
        "        try:\n",
        "            with open(config_files[0], 'r') as f:\n",
        "                config_data = json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"ğŸŸ¡ Warning: Failed to read config file: {e}\")\n",
        "\n",
        "    # List found files for debugging\n",
        "    print(f\"ğŸ” Searching directory: {base_output_dir}\")\n",
        "    if os.path.exists(base_output_dir):\n",
        "        print(f\"ğŸ“ Found subdirectories:\")\n",
        "        for item in sorted(os.listdir(base_output_dir)):\n",
        "            item_path = os.path.join(base_output_dir, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                print(f\"   ğŸ“‚ {item}\")\n",
        "                for subitem in sorted(os.listdir(item_path)):\n",
        "                    subitem_path = os.path.join(item_path, subitem)\n",
        "                    if os.path.isdir(subitem_path):\n",
        "                        print(f\"      ğŸ“‚ {subitem}\")\n",
        "                        csv_files = glob.glob(os.path.join(subitem_path, \"*.csv\"))\n",
        "                        for csv_file in csv_files:\n",
        "                            print(f\"         ğŸ“„ {os.path.basename(csv_file)}\")\n",
        "\n",
        "    for seed_folder in sorted(os.listdir(base_output_dir)):\n",
        "        if not seed_folder.startswith('seed_'): continue\n",
        "        try:\n",
        "            seed = int(seed_folder.split('_')[1])\n",
        "        except (ValueError, IndexError):\n",
        "            continue\n",
        "\n",
        "        for mode_folder in sorted(os.listdir(os.path.join(base_output_dir, seed_folder))):\n",
        "            exp_path = os.path.join(base_output_dir, seed_folder, mode_folder)\n",
        "            if not os.path.isdir(exp_path): continue\n",
        "\n",
        "            eval_files = glob.glob(os.path.join(exp_path, '*_evaluation_results.csv'))\n",
        "            history_files = glob.glob(os.path.join(exp_path, '*_training_history.csv'))\n",
        "            privacy_files = glob.glob(os.path.join(exp_path, '*_privacy_costs.csv'))\n",
        "\n",
        "            def read_and_append(file_list, data_list, mode_name, seed_val):\n",
        "                if not file_list:\n",
        "                    print(f\"ğŸŸ¡ Warning: No files found for {mode_name} (seed {seed_val})\")\n",
        "                    return\n",
        "                file_path = file_list[0]\n",
        "                try:\n",
        "                    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "                        df = pd.read_csv(file_path)\n",
        "                        df['mode'] = mode_name\n",
        "                        df['seed'] = seed_val\n",
        "                        data_list.append(df)\n",
        "                        print(f\"âœ… Successfully loaded: {mode_name} (seed {seed_val}) - {len(df)} rows\")\n",
        "                    else:\n",
        "                        print(f\"ğŸŸ¡ Warning: File is empty or doesn't exist: {file_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"ğŸŸ¡ Warning: Failed to read file: {file_path}, {e}\")\n",
        "\n",
        "            read_and_append(eval_files, all_evals, mode_folder, seed)\n",
        "            read_and_append(history_files, all_histories, mode_folder, seed)\n",
        "            read_and_append(privacy_files, all_privacies, mode_folder, seed)\n",
        "\n",
        "    return (pd.concat(all_evals, ignore_index=True) if all_evals else pd.DataFrame()), \\\n",
        "           (pd.concat(all_histories, ignore_index=True) if all_histories else pd.DataFrame()), \\\n",
        "           (pd.concat(all_privacies, ignore_index=True) if all_privacies else pd.DataFrame()), \\\n",
        "           config_data\n",
        "\n",
        "# --- Visualization Settings ---\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n",
        "BASE_WORK_DIR = \"/content/drive/MyDrive/FRL_Slicing_Sim\"\n",
        "BASE_OUTPUT_DIR = os.path.join(BASE_WORK_DIR, \"outputs_virtual_clients\")  # Match Cell 8 path\n",
        "FIGURES_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, \"figures\")\n",
        "os.makedirs(FIGURES_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"ğŸ” Loading results from: {BASE_OUTPUT_DIR}\")\n",
        "eval_df, history_df, privacy_df, loaded_config = load_all_results(BASE_OUTPUT_DIR)\n",
        "\n",
        "if eval_df.empty and history_df.empty:\n",
        "    print(\"âŒ No result files found, cannot generate plots. Please ensure Cell 8 has completed successfully.\")\n",
        "    print(f\"ğŸ“ Expected path: {BASE_OUTPUT_DIR}\")\n",
        "    print(f\"ğŸ“ Path exists: {os.path.exists(BASE_OUTPUT_DIR)}\")\n",
        "else:\n",
        "    # Corrected settings to match Cell 8\n",
        "    SEEDS = [42]  # Only one seed\n",
        "    mode_order = [\"Centralized\", \"Isolated\", \"FedAvg\", \"FedProx\", \"ClusteredFL\"]  # Removed Centralized\n",
        "\n",
        "    print(f\"âœ… Successfully loaded results from {len(eval_df['seed'].unique()) if not eval_df.empty else 0} runs.\")\n",
        "    if not eval_df.empty:\n",
        "        print(f\"ğŸ“Š Found modes: {sorted(eval_df['mode'].unique())}\")\n",
        "        print(f\"ğŸ“Š Found seeds: {sorted(eval_df['seed'].unique())}\")\n",
        "\n",
        "    # --- Figure 1: Training History Comparison ---\n",
        "    if not history_df.empty:\n",
        "        plt.figure(figsize=(15, 8))\n",
        "\n",
        "        # Only plot existing modes\n",
        "        available_modes = [mode for mode in mode_order if mode in history_df['mode'].unique()]\n",
        "\n",
        "        sns.lineplot(data=history_df, x='round', y='avg_reward', hue='mode',\n",
        "                     hue_order=available_modes, errorbar=('sd', 1), linewidth=2.5,\n",
        "                     err_style=\"band\", alpha=0.8)\n",
        "\n",
        "        plt.title('Differentially Private Federated Reinforcement Learning Training Performance', fontsize=18, weight='bold')\n",
        "        plt.xlabel('Communication Round / Equivalent Round', fontsize=14)\n",
        "        plt.ylabel('Average Episodic Reward', fontsize=14)\n",
        "        plt.legend(title='Training Mode', fontsize=12)\n",
        "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "        plt.xlim(0, 25)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(FIGURES_OUTPUT_DIR, 'training_history_dp_eng.png'), dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"ğŸŸ¡ Warning: No training history data found\")\n",
        "\n",
        "    # --- Figure 2: Final Performance Comparison ---\n",
        "    if not eval_df.empty:\n",
        "        eval_to_plot = eval_df.rename(columns={'reward_pfl_finetuned': 'Final Reward Score'})\n",
        "        available_modes = [mode for mode in mode_order if mode in eval_to_plot['mode'].unique()]\n",
        "\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        ax = sns.boxplot(data=eval_to_plot, x='mode', y='Final Reward Score',\n",
        "                         order=available_modes, palette=\"viridis\")\n",
        "\n",
        "        # Add median annotations\n",
        "        medians = eval_to_plot.groupby(['mode'])['Final Reward Score'].median().reindex(available_modes)\n",
        "        for xtick in ax.get_xticks():\n",
        "            if xtick < len(available_modes):\n",
        "                mode_name = available_modes[xtick]\n",
        "                median_val = medians.get(mode_name)\n",
        "                if pd.notna(median_val):\n",
        "                    ax.text(xtick, median_val + 5, f'Median: {median_val:.1f}',\n",
        "                            horizontalalignment='center', size='medium',\n",
        "                            color='black', weight='semibold',\n",
        "                            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "\n",
        "        plt.title('Differentially Private Federated Learning Final Performance Comparison', fontsize=18)\n",
        "        plt.xlabel('Experiment Mode', fontsize=14)\n",
        "        plt.ylabel('Final Reward Score', fontsize=14)\n",
        "        plt.xticks(rotation=15)\n",
        "\n",
        "        # Statistical testing\n",
        "        groups = [eval_to_plot['Final Reward Score'][eval_to_plot['mode'] == m].dropna()\n",
        "                 for m in available_modes if m in eval_to_plot['mode'].unique()]\n",
        "        if len(groups) > 1:\n",
        "            h_stat, p_value = stats.kruskal(*groups)\n",
        "            plt.figtext(0.5, 0.01, f'Kruskal-Wallis Test: H = {h_stat:.2f}, p = {p_value:.4f}',\n",
        "                        ha='center', fontsize=12,\n",
        "                        bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 1])\n",
        "        plt.savefig(os.path.join(FIGURES_OUTPUT_DIR, 'final_performance_dp_eng.png'), dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"ğŸŸ¡ Warning: No evaluation results data found\")\n",
        "\n",
        "    # --- Figure 3: Personalization Benefit Analysis ---\n",
        "    if not eval_df.empty:\n",
        "        first_seed = SEEDS[0]\n",
        "        # Prefer ClusteredFL, otherwise use the first available mode\n",
        "        if 'ClusteredFL' in eval_df['mode'].unique():\n",
        "            target_mode = 'ClusteredFL'\n",
        "        else:\n",
        "            target_mode = eval_df['mode'].unique()[0]\n",
        "\n",
        "        target_eval = eval_df[(eval_df['mode'] == target_mode) & (eval_df['seed'] == first_seed)]\n",
        "\n",
        "        if not target_eval.empty:\n",
        "            target_melted = target_eval.melt(\n",
        "                id_vars=['client_id'],\n",
        "                value_vars=['reward_global', 'reward_personalized', 'reward_pfl_finetuned'],\n",
        "                var_name='Model Type', value_name='Average Reward'\n",
        "            )\n",
        "\n",
        "            plt.figure(figsize=(14, 7))\n",
        "            sns.barplot(data=target_melted, x='client_id', y='Average Reward',\n",
        "                       hue='Model Type', palette='viridis')\n",
        "\n",
        "            plt.title(f'{target_mode} Mode Personalization Benefits Analysis (Seed={first_seed})', fontsize=16)\n",
        "            plt.xlabel('Client ID', fontsize=14)\n",
        "            plt.ylabel('Average Reward', fontsize=14)\n",
        "            plt.legend(title='Model Type', fontsize=12)\n",
        "            plt.grid(axis='y', linestyle='--')\n",
        "            plt.tight_layout()\n",
        "\n",
        "            plt.savefig(os.path.join(FIGURES_OUTPUT_DIR, 'personalization_benefit_dp_eng.png'), dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"ğŸŸ¡ Warning: No data found for {target_mode} mode\")\n",
        "\n",
        "    # --- Figure 4: Privacy Cost Tracking ---\n",
        "    if not privacy_df.empty:\n",
        "        privacy_to_plot = privacy_df[privacy_df['mode'].isin(['FedAvg', 'FedProx', 'ClusteredFL'])]\n",
        "\n",
        "        if not privacy_to_plot.empty:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            sns.lineplot(data=privacy_to_plot, x='round', y='cumulative_epsilon', hue='mode',\n",
        "                         errorbar=('sd', 1), linewidth=2.5)\n",
        "\n",
        "            # Add target epsilon line\n",
        "            if loaded_config and 'dp_target_epsilon' in loaded_config:\n",
        "                target_eps = loaded_config['dp_target_epsilon']\n",
        "                plt.axhline(y=target_eps, color='r', linestyle='--',\n",
        "                           label=f'Target Îµ = {target_eps}')\n",
        "            else:\n",
        "                # If no config file, use Cell 8 default\n",
        "                target_eps = 15.0\n",
        "                plt.axhline(y=target_eps, color='r', linestyle='--',\n",
        "                           label=f'Target Îµ = {target_eps}')\n",
        "\n",
        "            plt.title('Differential Privacy Budget Consumption Tracking', fontsize=16)\n",
        "            plt.xlabel('Communication Round', fontsize=14)\n",
        "            plt.ylabel('Cumulative Privacy Loss Îµ (Epsilon)', fontsize=14)\n",
        "            plt.legend(fontsize=12)\n",
        "            plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(FIGURES_OUTPUT_DIR, 'privacy_cost_dp_eng.png'), dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"ğŸŸ¡ Warning: No privacy cost data found\")\n",
        "    else:\n",
        "        print(\"ğŸŸ¡ Warning: No privacy cost files found\")\n",
        "\n",
        "print(\"âœ… Cell 9: Results Visualization (English Version) completed.\")\n",
        "print(f\"ğŸ“ Figures saved to: {FIGURES_OUTPUT_DIR}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "32e9f8543316465083995fb0f2454c99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aebb265f05664258a35cd2d2f9157af0",
              "IPY_MODEL_413b9881ba634052a7e738fb874d4035",
              "IPY_MODEL_1bd96cdf692f403d995ddc88a1624819"
            ],
            "layout": "IPY_MODEL_e0983c9ef9694b828ffa371f14db6212"
          }
        },
        "aebb265f05664258a35cd2d2f9157af0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a1e0d29fe4c43398fb6183278f0f2cd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_58d3070e5a9e475d8c74c8b3c3ac4579",
            "value": "è™•ç†å®¢æˆ¶ç«¯æ•¸æ“š:â€‡100%"
          }
        },
        "413b9881ba634052a7e738fb874d4035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d60681a7d6f24be3997acd88e3c20269",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6618534e9404e8db25f828d99b25e3a",
            "value": 3
          }
        },
        "1bd96cdf692f403d995ddc88a1624819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d17ea8e71e834fbf9ed9b4c3c496680d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_16c93c42e90d485088bc29505aa8ca43",
            "value": "â€‡3/3â€‡[00:01&lt;00:00,â€‡â€‡1.81it/s]"
          }
        },
        "e0983c9ef9694b828ffa371f14db6212": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a1e0d29fe4c43398fb6183278f0f2cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58d3070e5a9e475d8c74c8b3c3ac4579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d60681a7d6f24be3997acd88e3c20269": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6618534e9404e8db25f828d99b25e3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d17ea8e71e834fbf9ed9b4c3c496680d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16c93c42e90d485088bc29505aa8ca43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "093d03e4fe3c476192a6369a4aac6aec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8ee9bc7d6794d33b40dce6c1ef119d5",
              "IPY_MODEL_9ab6c7fa418d40c3a3497f011abb0c56",
              "IPY_MODEL_77022653894943798c4da9f665de7592"
            ],
            "layout": "IPY_MODEL_b550fd41ccb54fca84b46fff002f6cf0"
          }
        },
        "a8ee9bc7d6794d33b40dce6c1ef119d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_311401d48b1c447e815e2a377f741e83",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_545daa784ddb41c1b67656782da72aa4",
            "value": "ClusteredFLâ€‡Training:â€‡â€‡20%"
          }
        },
        "9ab6c7fa418d40c3a3497f011abb0c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_025b671d53d54e3f877cc068e31e4493",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4700a7e76ea44049ac178f1f02bd569",
            "value": 3
          }
        },
        "77022653894943798c4da9f665de7592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1734435a75541dfbc72d0d276e60c3f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b394dac30c674dd1bd3530b49b428387",
            "value": "â€‡3/15â€‡[22:35&lt;1:46:00,â€‡530.01s/it,â€‡reward=302.35,â€‡loss=549399.3915]"
          }
        },
        "b550fd41ccb54fca84b46fff002f6cf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "311401d48b1c447e815e2a377f741e83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "545daa784ddb41c1b67656782da72aa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "025b671d53d54e3f877cc068e31e4493": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4700a7e76ea44049ac178f1f02bd569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1734435a75541dfbc72d0d276e60c3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b394dac30c674dd1bd3530b49b428387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}