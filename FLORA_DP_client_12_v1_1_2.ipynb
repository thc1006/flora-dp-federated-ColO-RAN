{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thc1006/flora-dp-federated-ColO-RAN/blob/main/FLORA_DP_client_12_v1_1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjO4xoSZJ2Ag"
      },
      "source": [
        "# è¯é‚¦å¼·åŒ–å­¸ç¿’æ–¼ç„¡ç·šç¶²è·¯åˆ‡ç‰‡è³‡æºåˆ†é…ä¹‹é€²éšå¯¦ä½œ\n",
        "\n",
        "### å¯¦é©—ç›®æ¨™ (Pareto Optimization)\n",
        "æœ¬å¯¦é©—æ—¨åœ¨å„ªåŒ–ä¸€å€‹è¯é‚¦å¼·åŒ–å­¸ç¿’ç³»çµ±ï¼Œä½¿å…¶åœ¨æ¨¡æ“¬çš„ 5G ç¶²è·¯åˆ‡ç‰‡è³‡æºåˆ†é…ä»»å‹™ä¸­ï¼Œé”æˆä»¥ä¸‹ä¸‰å€‹æ ¸å¿ƒç›®æ¨™çš„æ¬Šè¡¡ï¼š\n",
        "1.  **ä½å»¶é² (Low Latency)**: ç«¯åˆ°ç«¯è¨“ç·´å»¶é² (P95) ` < 40s`ã€‚\n",
        "2.  **å¼·éš±ç§ (Strong Privacy)**: ç¸½éš±ç§é ç®—æ¶ˆè€— `Îµ < 4.0`ã€‚\n",
        "3.  **é«˜æ•ˆèƒ½ (High Reward)**: æœ€çµ‚è©•ä¼°çš„å¹³å‡çå‹µ `Avg Reward > 165`ã€‚\n",
        "\n",
        "### ç’°å¢ƒéœ€æ±‚\n",
        "- **ç¡¬é«”**: Google Colab with T4 GPU\n",
        "- **Python**: 3.10+\n",
        "- **æ ¸å¿ƒå¥—ä»¶**:\n",
        "  - `torch`: 2.1.0+\n",
        "  - `opacus`: 1.4.0+\n",
        "  - `numpy`: 1.23.5+\n",
        "  - `pandas`: 1.5.3+\n",
        "  - `scikit-learn`: 1.2.2+\n",
        "  - `matplotlib`: 3.7.1+\n",
        "  - `seaborn`: 0.12.2+\n",
        "  - `pyarrow`: 10.0.1+\n",
        "  - `nest_asyncio`: 1.6.0+\n",
        "\n",
        "  ---\n",
        "\n",
        "  æª”æ¡ˆä¸­çš„ã€å¯¦éš›æ¬„ä½åˆ—è¡¨ã€‘å¦‚ä¸‹ï¼š\n",
        "['timestamp', 'num_ues', 'IMSI', 'RNTI', 'slicing_enabled']\n",
        "['Slice_ID', 'slice_prb', 'power_multiplier', 'scheduling_policy', 'dl_mcs']\n",
        "['dl_n_samples', 'Buffer_Occupancy_DL_bytes', 'Throughput_DL_Mbps', 'tx_pkts downlink', 'tx_errors downlink (%)']\n",
        "['dl_cqi', 'ul_mcs', 'ul_n_samples', 'ul_buffer [bytes]', 'Throughput_UL_Mbps']\n",
        "['rx_pkts uplink', 'rx_errors uplink (%)', 'ul_rssi', 'ul_sinr', 'phr']\n",
        "['sum_requested_prbs', 'sum_granted_prbs', 'dl_pmi', 'dl_ri', 'ul_n']\n",
        "['ul_turbo_iters', 'Scheduling_Policy_Active', 'Training_Config_ID', 'exp_id', 'BS_ID']\n",
        "['nof_ue', 'dl_brate', 'ul_brate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOClVs50DQe5",
        "outputId": "48ec337a-b3b0-4a03-bd99-c9c71f6bc003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 1: ç’°å¢ƒèˆ‡å‡½å¼åº«æº–å‚™å°±ç·’ï¼ˆå« Deterministic èˆ‡ Asyncio ä¿®æ­£ï¼‰ã€‚\n",
            "PyTorch/Opacus ç‰ˆæœ¬: 2.6.0+cu124 / 1.5.4\n",
            "CUDA æ˜¯å¦å¯ç”¨: True\n",
            "Deterministic æ¨¡å¼: å·²å•Ÿç”¨\n",
            "Nest Asyncio: å·²æ‡‰ç”¨\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 1: ç’°å¢ƒè¨­å®šèˆ‡å‡½å¼åº«åŒ¯å…¥ï¼ˆå¼·åŒ–ç‰ˆï¼‰\n",
        "# å®‰è£å¿…è¦å¥—ä»¶\n",
        "!pip install --upgrade opacus -q\n",
        "!pip install nest_asyncio -q\n",
        "!pip install dask -q\n",
        "!pip install pyarrow -q\n",
        "\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np, pandas as pd, random, copy, json, os, time, warnings, math, re, contextlib\n",
        "from collections import deque\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from dataclasses import dataclass, asdict, field\n",
        "from sklearn.cluster import KMeans\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.validators import ModuleValidator\n",
        "from opacus.data_loader import DPDataLoader\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "import scipy.stats as stats\n",
        "from datetime import datetime\n",
        "import nest_asyncio\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import asyncio\n",
        "import gc\n",
        "\n",
        "# --- ç’°å¢ƒè¨­å®š ---\n",
        "try: torch._dynamo.disable()\n",
        "except Exception: pass\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\".*overflow encountered.*\", category=RuntimeWarning)\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "# --- æ–°å¢ï¼šç’°å¢ƒè®Šæ•¸è¨­å®š (é€šç”¨ç´„æŸ4) ---\n",
        "BASE_OUTPUT_DIR = Path(os.getenv(\"OUTPUT_DIR\", \"./\")) / \"outputs\"\n",
        "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- æ–°å¢ï¼šå…¨å±€é…ç½®å­—å…¸ (é€šç”¨ç´„æŸ4) ---\n",
        "CONFIG = {\n",
        "    \"BASE_OUTPUT_DIR\": str(BASE_OUTPUT_DIR),\n",
        "    \"GPU_MEMORY_LIMIT\": 13 * 1024 * 1024 * 1024,  # 13GB in bytes\n",
        "    \"LATENCY_SLA\": 0.05,  # 50ms SLA for latency\n",
        "    \"SEED\": 42\n",
        "}\n",
        "\n",
        "# --- æ–°å¢ï¼šDeterministic è¨­å®š (é€šç”¨ç´„æŸ6) ---\n",
        "def set_global_seed(seed: int = 42):\n",
        "    \"\"\"è¨­å®šå…¨å±€éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯é‡ç¾æ€§\"\"\"\n",
        "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "    # ç¢ºä¿ deterministic è¡Œç‚º\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    if hasattr(torch, 'use_deterministic_algorithms'):\n",
        "        try:\n",
        "            torch.use_deterministic_algorithms(True)\n",
        "        except RuntimeError:\n",
        "            print(\"âš ï¸ éƒ¨åˆ†æ“ä½œä¸æ”¯æ´å®Œå…¨ deterministic æ¨¡å¼\")\n",
        "\n",
        "# --- æ–°å¢ï¼šè¨­ç½®å…¨å±€æ—¥èªŒç³»çµ± (æ—¥èªŒèˆ‡éŒ¯èª¤éŸŒæ€§) ---\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler(BASE_OUTPUT_DIR / 'experiment.log')\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# è¨­å®šéš¨æ©Ÿç¨®å­\n",
        "set_global_seed(CONFIG[\"SEED\"])\n",
        "\n",
        "# æ‡‰ç”¨ nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print(\"âœ… Cell 1: ç’°å¢ƒèˆ‡å‡½å¼åº«æº–å‚™å°±ç·’ï¼ˆå« Deterministic èˆ‡ Asyncio ä¿®æ­£ï¼‰ã€‚\")\n",
        "import opacus\n",
        "print(f\"PyTorch/Opacus ç‰ˆæœ¬: {torch.__version__} / {opacus.__version__}\")\n",
        "print(f\"CUDA æ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}\")\n",
        "print(f\"Deterministic æ¨¡å¼: å·²å•Ÿç”¨\")\n",
        "print(f\"Nest Asyncio: å·²æ‡‰ç”¨\")\n",
        "logger.info(\"Environment setup completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pay4V1DhDTWE",
        "outputId": "e0a59768-5854-469a-c2ea-a9b0840833c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 2: TrainingConfigï¼ˆå¼·åŒ–ç‰ˆï¼‰å®šç¾©å®Œæˆã€‚\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 2: ğŸ“ å¯¦é©—åƒæ•¸è¨­å®šï¼ˆå¼·åŒ–ç‰ˆ - æ”¯æ´ Non-IID & CQLï¼‰\n",
        "from dataclasses import dataclass, field\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    experiment_name: str\n",
        "    output_dir: str\n",
        "    mode: str = \"ClusteredFL\"  # æ–°å¢æ”¯æ´: \"CQL\"\n",
        "    random_seed: int = 42\n",
        "    comm_rounds: int = 20  # å»ºè­°å€¼ > 20 for ClusteredFL\n",
        "\n",
        "    # ã€æ–°å¢ã€‘ClusteredFL å‹•æ…‹èšé¡åƒæ•¸ (ä»»å‹™1)\n",
        "    base_cluster_freq: int = 2  # åŸºç¤èšé¡æ›´æ–°é »ç‡\n",
        "    cluster_decay_rate: float = 0.1  # èšé¡é »ç‡è¡°æ¸›ç‡\n",
        "    early_stopping_rounds: int = 2  # æ—©æœŸåœæ­¢è¼ªæ•¸\n",
        "    use_cosine_similarity: bool = True  # ä½¿ç”¨é¤˜å¼¦ç›¸ä¼¼åº¦åˆ¤æ–·\n",
        "    knowledge_transfer: bool = True  # å•Ÿç”¨çŸ¥è­˜é·ç§»\n",
        "\n",
        "    # ã€æ–°å¢ã€‘CQL é…ç½® (ä»»å‹™2)\n",
        "    enable_cql: bool = False\n",
        "    cql_alpha: float = 1.0  # å°‡æ ¹æ“šÎµå‹•æ…‹èª¿æ•´\n",
        "    cql_min_q_weight: float = 5.0  # å°‡æ ¹æ“šÎµå‹•æ…‹èª¿æ•´\n",
        "    cql_loss_alpha_scaler: float = 0.1  # æå¤±ç¸®æ”¾å› å­\n",
        "    use_mixed_precision: bool = True  # æ··åˆç²¾åº¦è¨“ç·´\n",
        "\n",
        "    # ã€æ–°å¢ã€‘FedAvg å‹•æ…‹åŠ æ¬Š (ä»»å‹™3)\n",
        "    use_weighted_fedavg: bool = True\n",
        "\n",
        "    # ã€æ–°å¢ã€‘FedProx å‹•æ…‹mu (ä»»å‹™4)\n",
        "    fedprox_mu_base: float = 0.15\n",
        "    fedprox_mu_adaptive: bool = True\n",
        "\n",
        "    # ã€æ–°å¢ã€‘å»¶é²ç›£æ§é…ç½® (ä»»å‹™5)\n",
        "    enable_latency_monitor: bool = True\n",
        "    latency_warning_threshold: float = 1.0  # 1ç§’è­¦å‘Šé–¾å€¼\n",
        "\n",
        "    # ã€æ–°å¢ã€‘DP å„ªåŒ–é…ç½® (ä»»å‹™6)\n",
        "    enable_l2_regularization: bool = False\n",
        "    weight_decay: float = 1e-4\n",
        "    enable_federated_dropout: bool = False\n",
        "    federated_dropout_rate: float = 0.2\n",
        "    use_rdp_accountant: bool = True  # ä½¿ç”¨ RDP accountant\n",
        "\n",
        "    # ã€æ–°å¢ã€‘Non-IID é…ç½®\n",
        "    enable_non_iid: bool = True\n",
        "    dirichlet_alpha: float = 0.5  # æ§åˆ¶ Non-IID ç¨‹åº¦ï¼Œè¶Šå°è¶Šç•°è³ª\n",
        "    min_clients_non_iid: int = 10  # æœ€å°‘å®¢æˆ¶ç«¯æ•¸é‡\n",
        "\n",
        "    # ã€æ–°å¢ã€‘Adaptive Clipping é…ç½®\n",
        "    enable_adaptive_clip: bool = True\n",
        "    clip_window: int = 50  # çµ±è¨ˆçª—å£å¤§å°\n",
        "    clip_percentile: float = 0.9  # ä½¿ç”¨ 90 åˆ†ä½æ•¸\n",
        "\n",
        "    # ã€æ–°å¢ã€‘éåŒæ­¥èšåˆé…ç½®\n",
        "    enable_async_aggregation: bool = True\n",
        "    timeout_threshold: float = 0.3  # 30% å®¢æˆ¶ç«¯è¶…æ™‚è§¸ç™¼éåŒæ­¥\n",
        "    client_timeout_seconds: float = 30.0  # å®¢æˆ¶ç«¯è¶…æ™‚æ™‚é–“\n",
        "\n",
        "    # è™›æ“¬å®¢æˆ¶ç«¯ç›¸é—œï¼ˆä¿ç•™åŸæœ‰ï¼‰\n",
        "    base_client_pairs: tuple = ((1, 2), (3, 7), (5, 6))\n",
        "    virtual_expansion_factor: int = 3\n",
        "    num_virtual_clients: int = 9\n",
        "    num_real_clients: int = 3\n",
        "    total_clients: int = 12\n",
        "    num_clients: int = 12\n",
        "    num_clients_to_select: int = 8\n",
        "\n",
        "    # å…¶ä»–åŸæœ‰åƒæ•¸ä¿æŒä¸è®Š\n",
        "    temporal_split_method: str = \"sliding_window\"\n",
        "    noise_injection_std: float = 0.03\n",
        "    feature_augmentation: bool = True\n",
        "    cross_validation_split: bool = True\n",
        "\n",
        "    local_episodes_per_round: int = 4\n",
        "    steps_per_episode: int = 500\n",
        "    batch_size: int = 256\n",
        "    gamma: float = 0.99\n",
        "\n",
        "    lr: float = 1e-4\n",
        "    target_update_freq: int = 15\n",
        "\n",
        "    epsilon_start: float = 1.0\n",
        "    epsilon_decay: float = 0.9995\n",
        "    epsilon_min: float = 0.05\n",
        "\n",
        "    memory_capacity: int = 50000\n",
        "    replay_start_size: int = 1000\n",
        "    replay_frequency: int = 2\n",
        "    replay_batches_per_call: int = 3\n",
        "\n",
        "    fedprox_mu: float = 0.15  # å°‡è¢«å‹•æ…‹èª¿æ•´\n",
        "    num_clusters: int = 3\n",
        "    cluster_update_freq: int = 8  # å°‡è¢«å‹•æ…‹èª¿æ•´\n",
        "\n",
        "    enable_dp: bool = True\n",
        "    dp_target_epsilon: float = 8.0\n",
        "    dp_target_delta: float = 1e-5\n",
        "    dp_max_grad_norm: float = 1.0\n",
        "    dp_noise_multiplier: float = 0.5\n",
        "    dp_sampling_probability: float = 0.1\n",
        "    dp_virtual_batch_size: int = 256\n",
        "    dp_microbatch_size: int = 1\n",
        "\n",
        "    dp_reset_threshold_multiplier: float = 1.5\n",
        "    enable_dp_reset: bool = True\n",
        "\n",
        "    enable_heterogeneity: bool = True\n",
        "    enable_compression: bool = True\n",
        "\n",
        "    straggler_ratio: float = 0.1\n",
        "    dropout_ratio: float = 0.05\n",
        "    compression_type: str = \"quantize_fp16\"\n",
        "    use_pfl_finetune: bool = True\n",
        "    local_finetune_episodes: int = 15\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    reset_to_random_start: bool = True\n",
        "\n",
        "    client_pairs: tuple = field(init=False)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # å‹•æ…‹ç”Ÿæˆå®¢æˆ¶ç«¯é…å°\n",
        "        extended_client_pairs = []\n",
        "        for i in range(self.total_clients):\n",
        "            embb_id = i + 1\n",
        "            urllc_id = i + 2\n",
        "            extended_client_pairs.append((embb_id, urllc_id))\n",
        "        self.client_pairs = tuple(extended_client_pairs)\n",
        "\n",
        "        # GPU ç’°å¢ƒæª¢æ¸¬\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_name = torch.cuda.get_device_name(0)\n",
        "            if \"T4\" in gpu_name:\n",
        "                logger.info(f\"T4 GPUæª¢æ¸¬åˆ°ï¼Œå„ªåŒ–é…ç½®\")\n",
        "                self.batch_size = min(self.batch_size, 128)  # é™åˆ¶æ‰¹æ¬¡å¤§å°\n",
        "            elif \"L4\" in gpu_name:\n",
        "                logger.info(f\"L4 GPUæª¢æ¸¬åˆ°ï¼Œå•Ÿç”¨å¤§æ‰¹æ¬¡å„ªåŒ–é…ç½®\")\n",
        "\n",
        "        # å·®åˆ†éš±ç§æ¨¡å¼æª¢æ¸¬\n",
        "        if self.mode == 'Centralized':\n",
        "            self.enable_dp = False\n",
        "\n",
        "        # CQL æ¨¡å¼æª¢æ¸¬\n",
        "        if self.mode == 'CQL':\n",
        "            self.enable_cql = True\n",
        "            logger.info(f\"CQL æ¨¡å¼å•Ÿç”¨ - Î±={self.cql_alpha}, min_q_weight={self.cql_min_q_weight}\")\n",
        "\n",
        "        # å‹•æ…‹èª¿æ•´ CQL åƒæ•¸ (æ ¹æ“šéš±ç§é ç®—)\n",
        "        if self.enable_cql and self.enable_dp:\n",
        "            self.adjust_cql_params_by_epsilon()\n",
        "\n",
        "        # é…ç½®ä¿¡æ¯é¡¯ç¤º\n",
        "        logger.info(f\"å¯¦é©—é…ç½®:\")\n",
        "        logger.info(f\"   - Non-IID: {'å•Ÿç”¨' if self.enable_non_iid else 'ç¦ç”¨'} (Î±={self.dirichlet_alpha})\")\n",
        "        logger.info(f\"   - Adaptive Clipping: {'å•Ÿç”¨' if self.enable_adaptive_clip else 'ç¦ç”¨'}\")\n",
        "        logger.info(f\"   - éåŒæ­¥èšåˆ: {'å•Ÿç”¨' if self.enable_async_aggregation else 'ç¦ç”¨'}\")\n",
        "        logger.info(f\"   - ç¸½å®¢æˆ¶ç«¯æ•¸: {self.total_clients}\")\n",
        "\n",
        "        if self.enable_dp and self.mode != 'Centralized':\n",
        "            logger.info(f\"å·®åˆ†éš±ç§å·²å•Ÿç”¨\")\n",
        "            logger.info(f\"   - ç›®æ¨™éš±ç§é ç®—: Îµ={self.dp_target_epsilon}\")\n",
        "            logger.info(f\"   - æ‰¹æ¬¡å¤§å°: {self.batch_size}\")\n",
        "\n",
        "    def adjust_cql_params_by_epsilon(self):\n",
        "        \"\"\"æ ¹æ“šéš±ç§é ç®—å‹•æ…‹èª¿æ•´CQLåƒæ•¸\"\"\"\n",
        "        if self.dp_target_epsilon > 10:\n",
        "            self.cql_alpha = 0.5\n",
        "            self.cql_min_q_weight = 2.0\n",
        "        elif self.dp_target_epsilon < 1:\n",
        "            self.cql_alpha = 2.0\n",
        "            self.cql_min_q_weight = 10.0\n",
        "        logger.info(f\"CQLåƒæ•¸å·²æ ¹æ“šÎµ={self.dp_target_epsilon}èª¿æ•´: Î±={self.cql_alpha}, min_q_weight={self.cql_min_q_weight}\")\n",
        "\n",
        "    def get_dynamic_cluster_freq(self, round_idx: int) -> int:\n",
        "        \"\"\"ç²å–å‹•æ…‹èšé¡æ›´æ–°é »ç‡\"\"\"\n",
        "        if not hasattr(self, '_cluster_freq_cache'):\n",
        "            self._cluster_freq_cache = {}\n",
        "\n",
        "        if round_idx not in self._cluster_freq_cache:\n",
        "            freq = int(self.base_cluster_freq * (1 + self.cluster_decay_rate * round_idx))\n",
        "            self._cluster_freq_cache[round_idx] = max(freq, 10)  # æœ€å°‘10è¼ªæ›´æ–°ä¸€æ¬¡\n",
        "\n",
        "        return self._cluster_freq_cache[round_idx]\n",
        "\n",
        "    def adjust_fedprox_mu(self, client_heterogeneity: float) -> float:\n",
        "        \"\"\"æ ¹æ“šå®¢æˆ¶ç«¯ç•°è³ªæ€§èª¿æ•´mu\"\"\"\n",
        "        if not self.fedprox_mu_adaptive:\n",
        "            return self.fedprox_mu\n",
        "\n",
        "        # é«˜ç•°è³ªæ€§ä½¿ç”¨è¼ƒå°çš„muï¼Œä½ç•°è³ªæ€§ä½¿ç”¨è¼ƒå¤§çš„mu\n",
        "        if client_heterogeneity > 0.5:\n",
        "            return 0.001\n",
        "        elif client_heterogeneity < 0.1:\n",
        "            return 0.1\n",
        "        else:\n",
        "            return self.fedprox_mu_base\n",
        "\n",
        "    def save(self):\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        path = os.path.join(self.output_dir, f'{self.experiment_name}_config.json')\n",
        "        config_dict = {k: (list(v) if isinstance(v, tuple) else v) for k, v in self.__dict__.items()\n",
        "                      if not k.startswith('_')}\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(config_dict, f, indent=4)\n",
        "        logger.info(f\"é…ç½®å·²ä¿å­˜è‡³: {path}\")\n",
        "\n",
        "print(\"âœ… Cell 2: TrainingConfigï¼ˆå¼·åŒ–ç‰ˆï¼‰å®šç¾©å®Œæˆã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvXzlMywDW6t",
        "outputId": "35aff95c-5069-45a8-e74b-a76bc4d070a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 3: DataManagerï¼ˆæ”¯æ´ Non-IIDï¼‰å®šç¾©å®Œæˆã€‚\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 3: ğŸ§© æ•¸æ“šèˆ‡ç’°å¢ƒæº–å‚™ï¼ˆæ”¯æ´ Non-IID åˆ†é…ï¼‰\n",
        "class DataManager:\n",
        "    def __init__(self, data_path, client_pairs_config, enable_non_iid=False,\n",
        "                 dirichlet_alpha=0.5, min_clients=10):\n",
        "        logger.info(f\"[DataManager] æ­£åœ¨å¾ {data_path} è®€å–æ•¸æ“š...\")\n",
        "        self.df_kpi = pd.read_parquet(data_path)\n",
        "        self.client_pairs_config = client_pairs_config\n",
        "        self.enable_non_iid = enable_non_iid\n",
        "        self.dirichlet_alpha = dirichlet_alpha\n",
        "        self.min_clients = min_clients\n",
        "        self._sanitize_column_names()\n",
        "        self._preflight_check()\n",
        "\n",
        "        # è¨˜éŒ„èšé¡é‡æ–°åˆ†é…æ¬¡æ•¸\n",
        "        self.cluster_reassign_count = 0\n",
        "\n",
        "    def _sanitize_column_names(self):\n",
        "        sanitized_columns = [re.sub(r'[\\\\[\\\\]\\\\(\\\\)%\\\\s\\\\.-]+', '_', col.strip().lower()).strip('_')\n",
        "                           for col in self.df_kpi.columns]\n",
        "        self.df_kpi.columns = sanitized_columns\n",
        "\n",
        "    def _preflight_check(self):\n",
        "        logger.info(\"\\n\" + \"=\"*20 + \" DataManager å•Ÿå‹•å‰é æª¢æŸ¥ \" + \"=\"*20)\n",
        "        cols = self.df_kpi.columns.tolist()\n",
        "        tput_cand = ['throughput_dl_mbps', 'tx_brate_downlink_mbps']\n",
        "        lat_cand = ['buffer_occupancy_dl_bytes', 'dl_buffer_bytes']\n",
        "\n",
        "        self.tput_col = next((c for c in tput_cand if c in cols), None)\n",
        "        self.lat_col = next((c for c in lat_cand if c in cols), None)\n",
        "\n",
        "        logger.info(f\"æ¸…ç†å¾Œçš„æ¬„ä½åˆ—è¡¨ (å…± {len(cols)} å€‹):\")\n",
        "        logger.info(f\"   - ååé‡æ¬„ä½æˆåŠŸåŒ¹é…: '{self.tput_col}'\" if self.tput_col\n",
        "                   else \"   - ååé‡æ¬„ä½åŒ¹é…å¤±æ•—ï¼\")\n",
        "        logger.info(f\"   - å»¶é²/ç·©è¡å€æ¬„ä½æˆåŠŸåŒ¹é…: '{self.lat_col}'\" if self.lat_col\n",
        "                   else \"   - å»¶é²/ç·©è¡å€æ¬„ä½åŒ¹é…å¤±æ•—ï¼\")\n",
        "\n",
        "        available_bs = sorted(self.df_kpi['bs_id'].unique())\n",
        "        logger.info(f\"   - å¯ç”¨BSç¯€é»: {available_bs}\")\n",
        "        logger.info(\"=\"*65 + \"\\n\")\n",
        "\n",
        "        if not (self.tput_col and self.lat_col):\n",
        "            raise ValueError(\"é æª¢æŸ¥å¤±æ•—: æ‰¾ä¸åˆ°å¿…è¦çš„æ•¸æ“šæ¬„ä½ã€‚\")\n",
        "\n",
        "    def _get_clean_df(self, gnb_id, slice_id):\n",
        "        \"\"\"ç²å–æ¸…ç†å¾Œçš„æ•¸æ“š\"\"\"\n",
        "        df, bs_col, sl_col = self.df_kpi, 'bs_id', 'slice_id'\n",
        "\n",
        "        mask = (df[bs_col].astype(int) == int(gnb_id)) & (df[sl_col].astype(int) == int(slice_id))\n",
        "        subset = df.loc[mask, ['timestamp', self.tput_col, self.lat_col]].copy()\n",
        "\n",
        "        subset = subset.rename(columns={self.tput_col: 'throughput', self.lat_col: 'latency'})\n",
        "        subset = subset.dropna()\n",
        "\n",
        "        if not subset.empty:\n",
        "            subset = subset[\n",
        "                (subset['throughput'] >= 0) & (subset['throughput'] <= 1000) &\n",
        "                (subset['latency'] >= 0) & (subset['latency'] <= 1e9)\n",
        "            ]\n",
        "\n",
        "        return subset\n",
        "\n",
        "    def _create_non_iid_distribution(self, total_data_points):\n",
        "        \"\"\"ä½¿ç”¨ Dirichlet åˆ†é…å‰µå»º Non-IID æ•¸æ“šåˆ†å‰²\"\"\"\n",
        "        # ç¢ºä¿è‡³å°‘æœ‰ min_clients å€‹å®¢æˆ¶ç«¯\n",
        "        num_clients = max(self.min_clients, len(self.client_pairs_config) * 3)\n",
        "\n",
        "        # ä½¿ç”¨ Dirichlet åˆ†é…ç”Ÿæˆå®¢æˆ¶ç«¯æ•¸æ“šæ¯”ä¾‹\n",
        "        proportions = np.random.dirichlet(\n",
        "            alpha=[self.dirichlet_alpha] * num_clients\n",
        "        )\n",
        "\n",
        "        # è¨ˆç®—æ¯å€‹å®¢æˆ¶ç«¯çš„æ•¸æ“šé»æ•¸\n",
        "        client_data_sizes = (proportions * total_data_points).astype(int)\n",
        "\n",
        "        # ç¢ºä¿æ¯å€‹å®¢æˆ¶ç«¯è‡³å°‘æœ‰ä¸€äº›æ•¸æ“š\n",
        "        min_size = max(100, 256)  # è‡³å°‘256å€‹æ•¸æ“šé»\n",
        "        client_data_sizes = np.maximum(client_data_sizes, min_size)\n",
        "\n",
        "        # èª¿æ•´ç¸½å’Œä»¥åŒ¹é…ç¸½æ•¸æ“šé»\n",
        "        diff = total_data_points - client_data_sizes.sum()\n",
        "        if diff > 0:\n",
        "            client_data_sizes[np.argmax(client_data_sizes)] += diff\n",
        "        elif diff < 0:\n",
        "            largest_idx = np.argmax(client_data_sizes)\n",
        "            client_data_sizes[largest_idx] = max(min_size, client_data_sizes[largest_idx] + diff)\n",
        "\n",
        "        return client_data_sizes\n",
        "\n",
        "    def get_client_trajectories(self):\n",
        "        \"\"\"ç”Ÿæˆå®¢æˆ¶ç«¯æ•¸æ“šè»Œè·¡ï¼ˆæ”¯æ´ Non-IIDï¼‰\"\"\"\n",
        "        if self.enable_non_iid:\n",
        "            return self._get_non_iid_trajectories()\n",
        "        else:\n",
        "            return self._get_standard_trajectories()\n",
        "\n",
        "    def _get_non_iid_trajectories(self):\n",
        "        \"\"\"ç”Ÿæˆ Non-IID åˆ†ä½ˆçš„å®¢æˆ¶ç«¯è»Œè·¡\"\"\"\n",
        "        logger.info(\"[DataManager] æ­£åœ¨ç”Ÿæˆ Non-IID å®¢æˆ¶ç«¯æ•¸æ“šåˆ†ä½ˆ...\")\n",
        "\n",
        "        # é¦–å…ˆæ”¶é›†æ‰€æœ‰å¯ç”¨æ•¸æ“š\n",
        "        all_data = []\n",
        "        for embb_id, urllc_id in self.client_pairs_config:\n",
        "            df_embb = self._get_clean_df(embb_id, 0)\n",
        "            df_urllc = self._get_clean_df(urllc_id, 2)\n",
        "\n",
        "            if not df_embb.empty and not df_urllc.empty:\n",
        "                # ä½¿ç”¨ direction='backward' è€Œé 'nearest'\n",
        "                merged_df = pd.merge_asof(\n",
        "                    df_embb.sort_values('timestamp'),\n",
        "                    df_urllc.sort_values('timestamp'),\n",
        "                    on='timestamp',\n",
        "                    direction='backward',\n",
        "                    tolerance=pd.Timedelta('100ms'),\n",
        "                    suffixes=('_embb', '_urllc')\n",
        "                ).dropna()\n",
        "\n",
        "                if not merged_df.empty:\n",
        "                    trajectory = merged_df[['throughput_embb', 'latency_embb',\n",
        "                                          'throughput_urllc', 'latency_urllc']].to_numpy(dtype=np.float32)\n",
        "                    all_data.append(trajectory)\n",
        "\n",
        "        if not all_data:\n",
        "            logger.error(\"ç„¡æ³•ç”Ÿæˆ Non-IID æ•¸æ“šï¼šæ²’æœ‰æœ‰æ•ˆæ•¸æ“š\")\n",
        "            return {}\n",
        "\n",
        "        # åˆä½µæ‰€æœ‰æ•¸æ“š\n",
        "        combined_data = np.vstack(all_data)\n",
        "        total_points = len(combined_data)\n",
        "\n",
        "        # ä½¿ç”¨ Dirichlet åˆ†é…ç”Ÿæˆå®¢æˆ¶ç«¯æ•¸æ“šå¤§å°\n",
        "        client_sizes = self._create_non_iid_distribution(total_points)\n",
        "        num_clients = len(client_sizes)\n",
        "\n",
        "        logger.info(f\"   - ä½¿ç”¨ Dirichlet(Î±={self.dirichlet_alpha}) ç”Ÿæˆ {num_clients} å€‹å®¢æˆ¶ç«¯\")\n",
        "        logger.info(f\"   - æ•¸æ“šåˆ†ä½ˆæ¨™æº–å·®: {np.std(client_sizes):.2f}\")\n",
        "        logger.info(f\"   - æœ€å¤§/æœ€å°å®¢æˆ¶ç«¯æ•¸æ“šæ¯”: {max(client_sizes)/min(client_sizes):.2f}\")\n",
        "\n",
        "        # éš¨æ©Ÿæ‰“äº‚æ•¸æ“š\n",
        "        np.random.shuffle(combined_data)\n",
        "\n",
        "        # åˆ†é…æ•¸æ“šçµ¦å®¢æˆ¶ç«¯\n",
        "        client_trajectories = {}\n",
        "        start_idx = 0\n",
        "\n",
        "        for client_id in range(num_clients):\n",
        "            end_idx = start_idx + client_sizes[client_id]\n",
        "            client_data = combined_data[start_idx:min(end_idx, total_points)]\n",
        "\n",
        "            # ç‚ºæ¯å€‹å®¢æˆ¶ç«¯æ·»åŠ ä¸€äº›ç‰¹å®šçš„åå·®ï¼ˆå¢å¼· Non-IID ç‰¹æ€§ï¼‰\n",
        "            if client_id % 3 == 0:\n",
        "                # æŸäº›å®¢æˆ¶ç«¯æœ‰æ›´é«˜çš„ååé‡\n",
        "                client_data[:, [0, 2]] *= np.random.uniform(1.1, 1.3)\n",
        "            elif client_id % 3 == 1:\n",
        "                # æŸäº›å®¢æˆ¶ç«¯æœ‰æ›´é«˜çš„å»¶é²\n",
        "                client_data[:, [1, 3]] *= np.random.uniform(1.1, 1.3)\n",
        "\n",
        "            client_trajectories[client_id] = client_data\n",
        "            logger.info(f\"   - å®¢æˆ¶ç«¯ {client_id}: {len(client_data)} å€‹æ™‚é–“æ­¥\")\n",
        "\n",
        "            start_idx = end_idx\n",
        "            if start_idx >= total_points:\n",
        "                break\n",
        "\n",
        "        # æ·»åŠ å®¢æˆ¶ç«¯ ID åˆ°æ•¸æ“šæ¡†ï¼ˆç”¨æ–¼å¾ŒçºŒåˆ†æï¼‰\n",
        "        self.client_assignments = {i: len(traj) for i, traj in client_trajectories.items()}\n",
        "\n",
        "        return client_trajectories\n",
        "\n",
        "    def _get_standard_trajectories(self):\n",
        "        \"\"\"åŸå§‹çš„æ¨™æº–è»Œè·¡ç”Ÿæˆæ–¹æ³•\"\"\"\n",
        "        client_trajectories = {}\n",
        "        logger.info(\"[DataManager] æ­£åœ¨ç‚ºæ¯å€‹å®¢æˆ¶ç«¯ç”Ÿæˆæ•¸æ“šè»Œè·¡...\")\n",
        "\n",
        "        for i, (embb_id, urllc_id) in enumerate(tqdm(self.client_pairs_config, desc=\"è™•ç†å®¢æˆ¶ç«¯æ•¸æ“š\")):\n",
        "            try:\n",
        "                df_embb = self._get_clean_df(embb_id, 0)\n",
        "                df_urllc = self._get_clean_df(urllc_id, 2)\n",
        "\n",
        "                if df_embb.empty or df_urllc.empty:\n",
        "                    logger.warning(f\"å®¢æˆ¶ç«¯ {i} (BS {embb_id}/{urllc_id}) ç¯©é¸å¾Œç„¡æœ‰æ•ˆæ•¸æ“šã€‚\")\n",
        "                    client_trajectories[i] = np.array([])\n",
        "                    continue\n",
        "\n",
        "                merged_df = pd.merge_asof(\n",
        "                    df_embb.sort_values('timestamp'),\n",
        "                    df_urllc.sort_values('timestamp'),\n",
        "                    on='timestamp',\n",
        "                    direction='backward',\n",
        "                    tolerance=pd.Timedelta('100ms'),\n",
        "                    suffixes=('_embb', '_urllc')\n",
        "                ).dropna()\n",
        "\n",
        "                if merged_df.empty:\n",
        "                    logger.warning(f\"å®¢æˆ¶ç«¯ {i} åˆä½µå¾Œç„¡æœ‰æ•ˆæ•¸æ“šã€‚\")\n",
        "                    client_trajectories[i] = np.array([])\n",
        "                    continue\n",
        "\n",
        "                merged_df = merged_df.sort_values('timestamp').reset_index(drop=True)\n",
        "                trajectory = merged_df[['throughput_embb', 'latency_embb',\n",
        "                                      'throughput_urllc', 'latency_urllc']].to_numpy(dtype=np.float32)\n",
        "\n",
        "                client_trajectories[i] = trajectory\n",
        "                logger.info(f\"   - å®¢æˆ¶ç«¯ {i}: {len(trajectory)} å€‹æ™‚é–“æ­¥\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"è™•ç†å®¢æˆ¶ç«¯ {i} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}\")\n",
        "                client_trajectories[i] = np.array([])\n",
        "\n",
        "        num_valid = sum(1 for traj in client_trajectories.values() if traj.size > 0)\n",
        "        logger.info(f\"[DataManager] æ•¸æ“šè™•ç†å®Œæˆï¼æˆåŠŸç‚º {num_valid} / {len(self.client_pairs_config)} å€‹å®¢æˆ¶ç«¯å‰µå»ºäº†ç’°å¢ƒã€‚\")\n",
        "        return client_trajectories\n",
        "\n",
        "    def cluster_reassignment(self):\n",
        "        \"\"\"è¨˜éŒ„èšé¡é‡æ–°åˆ†é…\"\"\"\n",
        "        self.cluster_reassign_count += 1\n",
        "        return self.cluster_reassign_count\n",
        "\n",
        "print(\"âœ… Cell 3: DataManagerï¼ˆæ”¯æ´ Non-IIDï¼‰å®šç¾©å®Œæˆã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jL_CssiDZfP",
        "outputId": "a5a38a8a-259e-46f8-ba0c-ebbe088f49bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 4: RLç’°å¢ƒèˆ‡æ•¸æ“šè™•ç†ï¼ˆå«å»¶é²ç›£æ§ï¼‰å®šç¾©å®Œæˆã€‚\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 4: âš¡ RLç’°å¢ƒèˆ‡æ•¸æ“šè™•ç†ï¼ˆå«å»¶é²ç›£æ§ï¼‰\n",
        "import gc\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import contextlib\n",
        "from collections import deque\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ã€æ–°å¢ã€‘å»¶é²ç›£æ§é¡ï¼ˆä»»å‹™5ï¼‰\n",
        "class LatencyMonitor:\n",
        "    def __init__(self, sla_threshold: float = None):\n",
        "        self.round_times = []\n",
        "        self.current_round_start = None\n",
        "        self.warning_threshold = 1.0  # 1ç§’è­¦å‘Šé–¾å€¼\n",
        "        self.sla_threshold = sla_threshold or CONFIG.get(\"LATENCY_SLA\", 0.05)\n",
        "        self.component_times = {}  # è¨˜éŒ„å„çµ„ä»¶å»¶é²\n",
        "\n",
        "    def start_round(self):\n",
        "        \"\"\"é–‹å§‹è¨ˆæ™‚ä¸€å€‹é€šä¿¡è¼ª\"\"\"\n",
        "        self.current_round_start = time.time()\n",
        "        self.component_times = {\n",
        "            'client_compute_time': [],\n",
        "            'client_to_server_comm': [],\n",
        "            'server_aggregation_time': [],\n",
        "            'server_to_client_comm': []\n",
        "        }\n",
        "\n",
        "    def record_component(self, component: str, duration: float):\n",
        "        \"\"\"è¨˜éŒ„çµ„ä»¶å»¶é²\"\"\"\n",
        "        if component in self.component_times:\n",
        "            self.component_times[component].append(duration)\n",
        "\n",
        "    def end_round(self, round_num: int):\n",
        "        \"\"\"çµæŸè¨ˆæ™‚ä¸¦è¨˜éŒ„\"\"\"\n",
        "        if self.current_round_start is None:\n",
        "            return\n",
        "\n",
        "        round_time = time.time() - self.current_round_start\n",
        "        end_to_end_latency = round_time\n",
        "\n",
        "        # è¨ˆç®—å„çµ„ä»¶æ™‚é–“\n",
        "        component_stats = {}\n",
        "        for comp, times in self.component_times.items():\n",
        "            if times:\n",
        "                component_stats[comp] = np.mean(times)\n",
        "            else:\n",
        "                component_stats[comp] = 0.0\n",
        "\n",
        "        # è¨ˆç®—GPUä½¿ç”¨æƒ…æ³\n",
        "        gpu_mem_allocated = 0\n",
        "        gpu_mem_max = 0\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_mem_allocated = torch.cuda.memory_allocated() / 1024**2  # MB\n",
        "            gpu_mem_max = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
        "\n",
        "        self.round_times.append({\n",
        "            'round': round_num,\n",
        "            'wall_time': round_time,\n",
        "            'end_to_end_latency': end_to_end_latency,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'gpu_mem_allocated': gpu_mem_allocated,\n",
        "            'gpu_mem_max': gpu_mem_max,\n",
        "            **component_stats\n",
        "        })\n",
        "\n",
        "        if round_time > self.warning_threshold:\n",
        "            logger.warning(f\"Round {round_num} å»¶é²è­¦å‘Š: {round_time:.2f}s > {self.warning_threshold}s\")\n",
        "\n",
        "        self.current_round_start = None\n",
        "\n",
        "    def get_statistics(self):\n",
        "        \"\"\"ç²å–å»¶é²çµ±è¨ˆ\"\"\"\n",
        "        if not self.round_times:\n",
        "            return {}\n",
        "\n",
        "        times = [r['wall_time'] for r in self.round_times]\n",
        "        e2e_times = [r['end_to_end_latency'] for r in self.round_times]\n",
        "\n",
        "        stats = {\n",
        "            'mean_time': np.mean(times),\n",
        "            'median_time': np.median(times),\n",
        "            'p95_time': np.percentile(times, 95),\n",
        "            'max_time': np.max(times),\n",
        "            'total_rounds': len(times),\n",
        "            'warnings': sum(1 for t in times if t > self.warning_threshold),\n",
        "            'sla_violations': sum(1 for t in e2e_times if t > self.sla_threshold),\n",
        "            'mean_e2e_latency': np.mean(e2e_times),\n",
        "            'p95_e2e_latency': np.percentile(e2e_times, 95)\n",
        "        }\n",
        "\n",
        "        # æª¢æŸ¥P95æ˜¯å¦è¶…éSLA\n",
        "        if stats['p95_e2e_latency'] > self.sla_threshold:\n",
        "            logger.warning(f\"P95å»¶é² {stats['p95_e2e_latency']:.3f}s è¶…éSLA {self.sla_threshold}s\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def save_to_csv(self, filepath):\n",
        "        \"\"\"ä¿å­˜å»¶é²è¨˜éŒ„åˆ° CSV\"\"\"\n",
        "        if self.round_times:\n",
        "            pd.DataFrame(self.round_times).to_csv(filepath, index=False)\n",
        "            logger.info(f\"å»¶é²è¨˜éŒ„å·²ä¿å­˜è‡³: {filepath}\")\n",
        "\n",
        "class PairedEnv:\n",
        "    \"\"\"é…å°ç’°å¢ƒé¡åˆ¥ - ä¿æŒåŸæœ‰åŠŸèƒ½\"\"\"\n",
        "    def __init__(self, trajectory, config: TrainingConfig):\n",
        "        self.trajectory, self.config = trajectory, config\n",
        "        self.state_size = trajectory.shape[1] if trajectory.size > 0 else 4\n",
        "        self.action_size = 3\n",
        "        self.cursor = 0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        if self.trajectory.size == 0:\n",
        "            return np.zeros(self.state_size, dtype=np.float32)\n",
        "        max_start = max(0, len(self.trajectory) - self.config.steps_per_episode)\n",
        "        if self.config.reset_to_random_start and max_start > 0:\n",
        "            self.cursor = np.random.randint(0, max_start)\n",
        "        else:\n",
        "            self.cursor = 0\n",
        "        return self.trajectory[self.cursor]\n",
        "\n",
        "    def step(self, action_id: int):\n",
        "        if self.trajectory.size == 0 or self.cursor >= len(self.trajectory) - 1:\n",
        "            state = self.trajectory[-1] if self.trajectory.size > 0 else np.zeros(self.state_size, dtype=np.float32)\n",
        "            return state, 0.0, True, {}\n",
        "        self.cursor += 1\n",
        "        done = self.cursor >= len(self.trajectory) - 1\n",
        "        state = self.trajectory[self.cursor]\n",
        "        reward = self._compute_reward_with_action(state, action_id)\n",
        "        return state, reward, done, {}\n",
        "\n",
        "    def _compute_reward_with_action(self, state: np.ndarray, action_id: int) -> float:\n",
        "        tput_embb, lat_embb, tput_urllc, lat_urllc = state\n",
        "        if action_id == 0: w_tput, w_lat = (0.7, 0.3)\n",
        "        elif action_id == 2: w_tput, w_lat = (0.3, 0.7)\n",
        "        else: w_tput, w_lat = (0.5, 0.5)\n",
        "        tput_reward = w_tput * (np.log1p(tput_embb) + 0.5 * np.log1p(tput_urllc))\n",
        "        lat_penalty = w_lat * (np.tanh(lat_urllc * 1e-6) + 0.3 * np.tanh(lat_embb * 1e-6))\n",
        "        reward_val = tput_reward - lat_penalty\n",
        "        return float(np.nan_to_num(reward_val, nan=0.0, posinf=10.0, neginf=-10.0))\n",
        "\n",
        "class RLDataset(Dataset):\n",
        "    \"\"\"RLæ•¸æ“šé›†é¡åˆ¥\"\"\"\n",
        "    def __init__(self, memory_list):\n",
        "        self.data = memory_list[:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        state, action, reward, next_state, done = self.data[idx]\n",
        "        return (\n",
        "            torch.from_numpy(state).float(),\n",
        "            torch.tensor(action).long(),\n",
        "            torch.tensor(reward).float(),\n",
        "            torch.from_numpy(next_state).float(),\n",
        "            torch.tensor(done).bool()\n",
        "        )\n",
        "\n",
        "def get_gpu_optimized_data_loader(agent_memory: deque, batch_size: int, device: str):\n",
        "    \"\"\"GPUæ€§èƒ½å„ªåŒ–çš„æ•¸æ“šåŠ è¼‰å™¨ï¼ˆæª¢æŸ¥ç·©è¡å€å¤§å°ï¼‰\"\"\"\n",
        "    # æª¢æŸ¥ç·©è¡å€å¤§å°\n",
        "    if len(agent_memory) < batch_size:\n",
        "        return None\n",
        "\n",
        "    # è‹¥æœªé”åˆ° replay_start_sizeï¼Œä¹Ÿè¿”å› None\n",
        "    min_size = CONFIG.get('replay_start_size', 1000)\n",
        "    if len(agent_memory) < min_size:\n",
        "        logger.debug(f\"ç·©è¡å€å¤§å° {len(agent_memory)} < {min_size}ï¼Œè·³éè¨“ç·´\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        dataset = RLDataset(list(agent_memory))\n",
        "\n",
        "        cpu_count = os.cpu_count() or 4\n",
        "        if cpu_count <= 2:\n",
        "            num_workers = 0\n",
        "        else:\n",
        "            num_workers = min(2, cpu_count // 2)\n",
        "\n",
        "        dataloader_kwargs = {\n",
        "            'dataset': dataset,\n",
        "            'batch_size': batch_size,\n",
        "            'shuffle': True,\n",
        "            'num_workers': num_workers,\n",
        "            'pin_memory': torch.cuda.is_available() and device == 'cuda',\n",
        "            'drop_last': True,\n",
        "        }\n",
        "\n",
        "        if num_workers > 0:\n",
        "            dataloader_kwargs['persistent_workers'] = True\n",
        "            dataloader_kwargs['prefetch_factor'] = 2\n",
        "\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            g = torch.Generator(device='cuda')\n",
        "            g.manual_seed(42)\n",
        "            dataloader_kwargs['generator'] = g\n",
        "\n",
        "        return DataLoader(**dataloader_kwargs)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"GPUå„ªåŒ–DataLoaderå‰µå»ºå¤±æ•—: {e}\")\n",
        "        logger.info(f\"   å›é€€åˆ°åŸºæœ¬è¨­å®š...\")\n",
        "\n",
        "        try:\n",
        "            return DataLoader(\n",
        "                dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                num_workers=0,\n",
        "                pin_memory=False,\n",
        "                drop_last=True\n",
        "            )\n",
        "        except Exception as e2:\n",
        "            logger.error(f\"å›é€€ä¹Ÿå¤±æ•—: {e2}\")\n",
        "            return None\n",
        "\n",
        "def check_gpu_memory():\n",
        "    \"\"\"æª¢æŸ¥GPUè¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**3  # GB\n",
        "        max_memory = CONFIG.get(\"GPU_MEMORY_LIMIT\", 13 * 1024**3) / 1024**3  # GB\n",
        "\n",
        "        if allocated > max_memory * 0.9:\n",
        "            logger.warning(f\"GPUè¨˜æ†¶é«”ä½¿ç”¨æ¥è¿‘é™åˆ¶: {allocated:.2f}GB / {max_memory:.2f}GB\")\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "def setup_gpu_environment():\n",
        "    \"\"\"çµ±ä¸€çš„GPUç’°å¢ƒè¨­å®šå‡½æ•¸\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        compute_capability = torch.cuda.get_device_properties(0).major\n",
        "        cpu_count = os.cpu_count() or 4\n",
        "\n",
        "        logger.info(f\"GPU æª¢æ¸¬: {gpu_name}\")\n",
        "        logger.info(f\"ç¸½è¨˜æ†¶é«”: {total_memory:.1f} GB\")\n",
        "        logger.info(f\"è¨ˆç®—èƒ½åŠ›: {compute_capability}.x\")\n",
        "        logger.info(f\"å¯ç”¨CPUæ•¸: {cpu_count}\")\n",
        "\n",
        "        # è¨­å®š CUDA é…ç½®ï¼ˆç§»é™¤ benchmark ä»¥ç¢ºä¿ deterministicï¼‰\n",
        "        torch.backends.cudnn.benchmark = False  # ç¢ºä¿ deterministic\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "        # æ¸…ç†è¨˜æ†¶é«”\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        logger.info(f\"GPUç’°å¢ƒè¨­å®šå®Œæˆï¼ˆDeterministic æ¨¡å¼ï¼‰\")\n",
        "    else:\n",
        "        logger.warning(\"æœªæª¢æ¸¬åˆ°GPUï¼Œå°‡ä½¿ç”¨CPUæ¨¡å¼é‹è¡Œ\")\n",
        "\n",
        "print(\"âœ… Cell 4: RLç’°å¢ƒèˆ‡æ•¸æ“šè™•ç†ï¼ˆå«å»¶é²ç›£æ§ï¼‰å®šç¾©å®Œæˆã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRThBI3rDcTt",
        "outputId": "3e2bdcd0-632a-4b0b-d589-9845bf7f17c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 5: RLAgentï¼ˆæ”¯æ´ CQL & Adaptive Clippingï¼‰å®šç¾©å®Œæˆã€‚\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 5: ğŸ›¡ï¸ æ ¸å¿ƒå­¸ç¿’ä»£ç†ï¼ˆæ”¯æ´ CQL & Adaptive Clippingï¼‰\n",
        "import gc\n",
        "import time\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.validators import ModuleValidator\n",
        "from opacus.accountants import RDPAccountant\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# ã€æ–°å¢ã€‘Adaptive Clipper é¡ï¼ˆä»»å‹™3ï¼‰\n",
        "class AdaptiveClipper:\n",
        "    def __init__(self, window_size: int = 50, percentile: float = 0.9):\n",
        "        self.window_size = window_size\n",
        "        self.percentile = percentile\n",
        "        self.gradient_norms = deque(maxlen=window_size)\n",
        "        self.current_clip_value = 1.0\n",
        "        self.update_counter = 0\n",
        "        self.clip_history = []\n",
        "\n",
        "    def update(self, gradients):\n",
        "        \"\"\"æ›´æ–°æ¢¯åº¦çµ±è¨ˆä¸¦èª¿æ•´è£å‰ªå€¼\"\"\"\n",
        "        # è¨ˆç®—ç•¶å‰æ¢¯åº¦çš„ L2 norm\n",
        "        total_norm = 0.0\n",
        "        for grad in gradients:\n",
        "            if grad is not None:\n",
        "                param_norm = grad.data.norm(2).item()\n",
        "                total_norm += param_norm ** 2\n",
        "        total_norm = total_norm ** 0.5\n",
        "\n",
        "        self.gradient_norms.append(total_norm)\n",
        "        self.update_counter += 1\n",
        "\n",
        "        # æ¯ window_size æ­¥æ›´æ–°è£å‰ªå€¼\n",
        "        if self.update_counter % self.window_size == 0 and len(self.gradient_norms) > 0:\n",
        "            new_clip = np.percentile(list(self.gradient_norms), self.percentile * 100)\n",
        "            self.current_clip_value = max(0.1, new_clip)  # ç¢ºä¿æœ€å°å€¼\n",
        "            self.clip_history.append({\n",
        "                'step': self.update_counter,\n",
        "                'clip_value': self.current_clip_value,\n",
        "                'mean_norm': np.mean(list(self.gradient_norms)),\n",
        "                'std_norm': np.std(list(self.gradient_norms))\n",
        "            })\n",
        "\n",
        "    def get_clip_value(self):\n",
        "        return self.current_clip_value\n",
        "\n",
        "class RLAgent:\n",
        "    def __init__(self, state_size: int, action_size: int, config: TrainingConfig, client_id: int,\n",
        "                 dataset_size: int, is_eval_agent: bool = False):\n",
        "        self.state_size, self.action_size, self.config = state_size, action_size, config\n",
        "        self.client_id, self.dataset_size = client_id, dataset_size\n",
        "        self.device = torch.device(config.device)\n",
        "        self.mu = self.config.fedprox_mu\n",
        "        self.gamma, self.epsilon = config.gamma, config.epsilon_start\n",
        "        self.memory = deque(maxlen=config.memory_capacity)\n",
        "        self.global_params = None\n",
        "        self.is_eval_agent = is_eval_agent\n",
        "        self.privacy_engine = None\n",
        "        self.dp_steps = 0\n",
        "        self.current_epsilon = 0.0\n",
        "        self.current_best_alpha = None\n",
        "        self.consecutive_errors = 0\n",
        "        self.max_consecutive_errors = 5\n",
        "\n",
        "        # ã€æ–°å¢ã€‘CQL ç›¸é—œåƒæ•¸\n",
        "        self.enable_cql = config.enable_cql\n",
        "        self.cql_alpha = config.cql_alpha\n",
        "        self.cql_min_q_weight = config.cql_min_q_weight\n",
        "        self.cql_loss_alpha_scaler = config.cql_loss_alpha_scaler\n",
        "\n",
        "        # ã€æ–°å¢ã€‘æ··åˆç²¾åº¦è¨“ç·´\n",
        "        self.use_mixed_precision = config.use_mixed_precision and not is_eval_agent\n",
        "        if self.use_mixed_precision:\n",
        "            self.scaler = GradScaler()\n",
        "\n",
        "        # ã€æ–°å¢ã€‘Adaptive Clipping\n",
        "        self.adaptive_clipper = None\n",
        "        if config.enable_adaptive_clip and not is_eval_agent:\n",
        "            self.adaptive_clipper = AdaptiveClipper(\n",
        "                window_size=config.clip_window,\n",
        "                percentile=config.clip_percentile\n",
        "            )\n",
        "\n",
        "        # ã€æ–°å¢ã€‘è¨ˆç®—å®¢æˆ¶ç«¯ç•°è³ªæ€§ï¼ˆç”¨æ–¼FedProxï¼‰\n",
        "        self.client_heterogeneity = 0.0\n",
        "\n",
        "        self.privacy_calc_failures = 0\n",
        "        self.last_valid_epsilon = 0.0\n",
        "        self.dp_reset_count = 0\n",
        "        self.last_reset_round = -1\n",
        "        self.original_optimizer_class = optim.Adam\n",
        "\n",
        "        self.model = self._build_dp_model()\n",
        "        self.target_model = self._build_dp_model()\n",
        "        self.update_target_model()\n",
        "        self.target_model.eval()\n",
        "\n",
        "        # ã€æ–°å¢ã€‘L2æ­£å‰‡åŒ–\n",
        "        weight_decay = config.weight_decay if config.enable_l2_regularization else 0.0\n",
        "        self.optimizer = self.original_optimizer_class(\n",
        "            self.model.parameters(),\n",
        "            lr=config.lr,\n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        if self.config.enable_dp and not self.is_eval_agent and self.config.mode != 'Centralized':\n",
        "            self._initialize_dp_engine()\n",
        "        else:\n",
        "            logger.info(f\"[C-{self.client_id}] æ¨™æº–æ¨¡å¼ï¼ˆç„¡å·®åˆ†éš±ç§ï¼‰\")\n",
        "\n",
        "    def _build_dp_model(self):\n",
        "        model = nn.Sequential(\n",
        "            nn.Linear(self.state_size, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 64), nn.ReLU(),\n",
        "            nn.Linear(64, self.action_size)\n",
        "        ).to(self.device)\n",
        "        if self.config.enable_dp and not self.is_eval_agent:\n",
        "            if not ModuleValidator.is_valid(model):\n",
        "                model = ModuleValidator.fix(model)\n",
        "        return model\n",
        "\n",
        "    def _initialize_dp_engine(self):\n",
        "        \"\"\"å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–ï¼ˆæ”¯æ´ RDP accountant å’Œ Adaptive Clippingï¼‰\"\"\"\n",
        "        logger.info(f\"[C-{self.client_id}] åˆå§‹åŒ–å·®åˆ†éš±ç§å¼•æ“...\")\n",
        "\n",
        "        self.privacy_calc_failures = 0\n",
        "        self.last_valid_epsilon = 0.0\n",
        "\n",
        "        try:\n",
        "            if hasattr(self, 'privacy_engine') and self.privacy_engine is not None:\n",
        "                del self.privacy_engine\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "            self.model = self._build_dp_model()\n",
        "            self.optimizer = self.original_optimizer_class(self.model.parameters(), lr=self.config.lr)\n",
        "\n",
        "            dummy_data = []\n",
        "            effective_dataset_size = max(self.dataset_size, self.config.batch_size * 10)\n",
        "\n",
        "            for _ in range(effective_dataset_size):\n",
        "                state = np.random.randn(self.state_size).astype(np.float32)\n",
        "                action = int(np.random.randint(0, 3))\n",
        "                reward = float(np.random.randn())\n",
        "                next_state = np.random.randn(self.state_size).astype(np.float32)\n",
        "                done = bool(np.random.choice([True, False]))\n",
        "                dummy_data.append((state, action, reward, next_state, done))\n",
        "\n",
        "            dummy_dataset = RLDataset(dummy_data)\n",
        "            dummy_loader = DataLoader(\n",
        "                dummy_dataset, batch_size=self.config.batch_size, num_workers=0, shuffle=True\n",
        "            )\n",
        "\n",
        "            # å„ªå…ˆä½¿ç”¨ RDP accountant\n",
        "            accountant_options = []\n",
        "            if self.config.use_rdp_accountant:\n",
        "                accountant_options.append((\"rdp\", \"RDP\"))\n",
        "            accountant_options.extend([\n",
        "                (\"gdp\", \"GDP\"),\n",
        "                (\"prv\", \"PRV\"),\n",
        "                (None, \"Default\")\n",
        "            ])\n",
        "\n",
        "            for accountant_type, type_name in accountant_options:\n",
        "                try:\n",
        "                    if accountant_type:\n",
        "                        self.privacy_engine = PrivacyEngine(accountant=accountant_type)\n",
        "                    else:\n",
        "                        self.privacy_engine = PrivacyEngine()\n",
        "\n",
        "                    logger.info(f\"   - å˜—è©¦ Accountant: {type_name}\")\n",
        "                    break\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.debug(f\"   - {type_name} Accountant å¤±æ•—: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if not self.privacy_engine:\n",
        "                raise RuntimeError(\"æ‰€æœ‰ Accountant é¡å‹éƒ½å¤±æ•—\")\n",
        "\n",
        "            sample_rate = min(self.config.batch_size / len(dummy_data), 1.0)\n",
        "            logger.info(f\"   - Sample Rate: {sample_rate:.6f}\")\n",
        "\n",
        "            # ä½¿ç”¨åˆå§‹è£å‰ªå€¼æˆ– Adaptive Clipper çš„ç•¶å‰å€¼\n",
        "            clip_value = self.config.dp_max_grad_norm\n",
        "            if self.adaptive_clipper:\n",
        "                clip_value = self.adaptive_clipper.get_clip_value()\n",
        "                logger.info(f\"   - ä½¿ç”¨ Adaptive Clipping: åˆå§‹å€¼ {clip_value:.4f}\")\n",
        "\n",
        "            # è‡ªé©æ‡‰èª¿æ•´ noise_multiplier\n",
        "            noise_multiplier = self.config.dp_noise_multiplier\n",
        "            if self.config.dp_target_epsilon < 1.0 and self.config.enable_federated_dropout:\n",
        "                noise_multiplier *= 0.8  # æ¸›å°‘å™ªéŸ³ä»¥è£œå„Ÿfederated dropout\n",
        "                logger.info(f\"   - å•Ÿç”¨ Federated Dropoutï¼Œèª¿æ•´ noise_multiplier è‡³ {noise_multiplier:.3f}\")\n",
        "\n",
        "            self.model, self.optimizer, dummy_loader = self.privacy_engine.make_private(\n",
        "                module=self.model,\n",
        "                optimizer=self.optimizer,\n",
        "                data_loader=dummy_loader,\n",
        "                noise_multiplier=noise_multiplier,\n",
        "                max_grad_norm=clip_value,\n",
        "                poisson_sampling=True\n",
        "            )\n",
        "\n",
        "            self.dp_steps = 0\n",
        "            self.current_epsilon = 0.0\n",
        "            self.current_best_alpha = None\n",
        "\n",
        "            logger.info(f\"   - âœ… å·®åˆ†éš±ç§å¼•æ“åˆå§‹åŒ–æˆåŠŸ\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"   - å·®åˆ†éš±ç§åˆå§‹åŒ–å¤±æ•—: {e}\")\n",
        "            logger.info(f\"   - åˆ‡æ›åˆ°éå·®åˆ†éš±ç§æ¨¡å¼\")\n",
        "            self.privacy_engine = None\n",
        "            self.privacy_calc_failures = 999\n",
        "\n",
        "    def update_client_heterogeneity(self, other_clients_data):\n",
        "        \"\"\"è¨ˆç®—å®¢æˆ¶ç«¯ç•°è³ªæ€§ï¼ˆç”¨æ–¼å‹•æ…‹èª¿æ•´FedProx muï¼‰\"\"\"\n",
        "        if not other_clients_data:\n",
        "            self.client_heterogeneity = 0.0\n",
        "            return\n",
        "\n",
        "        # ä½¿ç”¨KLæ•£åº¦æˆ–å…¶ä»–åº¦é‡è¨ˆç®—ç•°è³ªæ€§\n",
        "        # é€™è£¡ç°¡åŒ–ç‚ºä½¿ç”¨æ¨¡å‹åƒæ•¸çš„å·®ç•°\n",
        "        my_params = self.get_model_weights_flat()\n",
        "        differences = []\n",
        "\n",
        "        for other_params in other_clients_data:\n",
        "            diff = np.linalg.norm(my_params - other_params)\n",
        "            differences.append(diff)\n",
        "\n",
        "        self.client_heterogeneity = np.mean(differences) / (np.linalg.norm(my_params) + 1e-8)\n",
        "\n",
        "        # å‹•æ…‹èª¿æ•´ mu\n",
        "        if self.config.fedprox_mu_adaptive:\n",
        "            self.mu = self.config.adjust_fedprox_mu(self.client_heterogeneity)\n",
        "            logger.debug(f\"[C-{self.client_id}] ç•°è³ªæ€§: {self.client_heterogeneity:.3f}, èª¿æ•´ mu è‡³: {self.mu:.4f}\")\n",
        "\n",
        "    def replay(self, num_batches: int):\n",
        "        \"\"\"æ”¯æ´ CQL çš„å›æ”¾è¨“ç·´ï¼ˆå«å»¶é²ä¾†æºè¨»è§£ï¼‰\"\"\"\n",
        "        if len(self.memory) < self.config.batch_size:\n",
        "            return 0.0\n",
        "\n",
        "        # æª¢æŸ¥ç·©è¡å€å¤§å°\n",
        "        if len(self.memory) < self.config.replay_start_size:\n",
        "            logger.debug(f\"[C-{self.client_id}] ç·©è¡å€æœªé” {self.config.replay_start_size}ï¼Œè·³éè¨“ç·´\")\n",
        "            return 0.0\n",
        "\n",
        "        data_loader = get_gpu_optimized_data_loader(\n",
        "            self.memory, self.config.batch_size, self.device\n",
        "        )\n",
        "        if data_loader is None:\n",
        "            return 0.0\n",
        "\n",
        "        total_loss, batches_processed = 0.0, 0\n",
        "        self.model.train()\n",
        "\n",
        "        try:\n",
        "            for i, batch in enumerate(data_loader):\n",
        "                if i >= num_batches:\n",
        "                    break\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                states, actions, rewards, next_states, dones = [item.to(self.device, non_blocking=True) for item in batch]\n",
        "\n",
        "                # ä½¿ç”¨æ··åˆç²¾åº¦è¨“ç·´\n",
        "                with autocast(enabled=self.use_mixed_precision):\n",
        "                    current_q = self.model(states).gather(1, actions.view(-1, 1))\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        max_next_q = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
        "                        target_q = rewards.view(-1, 1) + (self.gamma * max_next_q * (~dones.view(-1, 1)))\n",
        "\n",
        "                    # æ¨™æº– Q-learning æå¤±\n",
        "                    td_loss = self.criterion(current_q, target_q)\n",
        "\n",
        "                    # ã€æ–°å¢ã€‘CQL æå¤±é …\n",
        "                    if self.enable_cql:\n",
        "                        # è¨ˆç®—æ‰€æœ‰å‹•ä½œçš„ Q å€¼\n",
        "                        all_q_values = self.model(states)\n",
        "\n",
        "                        # CQL æ‡²ç½°é …ï¼šlog-sum-exp çš„ Q å€¼\n",
        "                        # è¨»è§£ï¼štorch.logsumexp ç‚ºå»¶é²ä¾†æºä¹‹ä¸€\n",
        "                        logsumexp_q = torch.logsumexp(all_q_values, dim=1, keepdim=True)\n",
        "\n",
        "                        # æœ€å°åŒ– Q å€¼çš„æ‡²ç½°\n",
        "                        cql_loss = self.cql_alpha * (logsumexp_q.mean() - current_q.mean())\n",
        "\n",
        "                        # æ·»åŠ æœ€å° Q å€¼ç´„æŸ\n",
        "                        min_q_loss = self.cql_min_q_weight * (all_q_values.min(dim=1)[0].mean() - target_q.mean()).abs()\n",
        "\n",
        "                        # ä½¿ç”¨æå¤±ç¸®æ”¾å› å­\n",
        "                        loss = td_loss + self.cql_loss_alpha_scaler * cql_loss + min_q_loss\n",
        "                    else:\n",
        "                        loss = td_loss\n",
        "\n",
        "                    # FedProx æ­£å‰‡åŒ–\n",
        "                    if self.config.mode in ['FedProx', 'ClusteredFL'] and self.mu > 0 and self.global_params:\n",
        "                        proximal_term = 0.0\n",
        "                        model_params = self.model._module.parameters() if hasattr(self.model, '_module') else self.model.parameters()\n",
        "                        for local_param, global_param in zip(model_params, self.global_params):\n",
        "                            proximal_term += torch.sum((local_param - global_param.to(self.device))**2)\n",
        "                        loss += (self.mu / 2) * proximal_term\n",
        "\n",
        "                    # ã€æ–°å¢ã€‘Federated Dropout\n",
        "                    if self.config.enable_federated_dropout and self.config.dp_target_epsilon < 1.0:\n",
        "                        # éš¨æ©Ÿä¸Ÿæ£„éƒ¨åˆ†æ¢¯åº¦\n",
        "                        dropout_mask = torch.rand_like(loss) > self.config.federated_dropout_rate\n",
        "                        loss = loss * dropout_mask.float()\n",
        "\n",
        "                if not torch.isfinite(loss):\n",
        "                    continue\n",
        "\n",
        "                # åå‘å‚³æ’­\n",
        "                if self.use_mixed_precision:\n",
        "                    self.scaler.scale(loss).backward()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "\n",
        "                # ã€æ–°å¢ã€‘æ›´æ–° Adaptive Clipper\n",
        "                if self.adaptive_clipper and not self.privacy_engine:\n",
        "                    gradients = [p.grad for p in self.model.parameters()]\n",
        "                    self.adaptive_clipper.update(gradients)\n",
        "\n",
        "                    # æ‰‹å‹•è£å‰ªæ¢¯åº¦\n",
        "                    clip_value = self.adaptive_clipper.get_clip_value()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip_value)\n",
        "\n",
        "                # å„ªåŒ–å™¨æ­¥é©Ÿ\n",
        "                if self.use_mixed_precision:\n",
        "                    self.scaler.step(self.optimizer)\n",
        "                    self.scaler.update()\n",
        "                else:\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                batches_processed += 1\n",
        "                if self.privacy_engine:\n",
        "                    self.dp_steps += 1\n",
        "\n",
        "            # æª¢æŸ¥GPUè¨˜æ†¶é«”\n",
        "            check_gpu_memory()\n",
        "\n",
        "            return total_loss / batches_processed if batches_processed > 0 else 0.0\n",
        "        except Exception as e:\n",
        "            logger.error(f\"[C-{self.client_id}] å›æ”¾éŒ¯èª¤: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    # å‹•æ…‹èª¿æ•´CQLåƒæ•¸çš„æ–¹æ³•\n",
        "    def adjust_cql_params(self, current_epsilon: float):\n",
        "        \"\"\"æ ¹æ“šç•¶å‰éš±ç§é ç®—å‹•æ…‹èª¿æ•´CQLåƒæ•¸\"\"\"\n",
        "        if current_epsilon > 10:\n",
        "            self.cql_alpha = 0.5\n",
        "            self.cql_min_q_weight = 2.0\n",
        "        elif current_epsilon < 1:\n",
        "            self.cql_alpha = 2.0\n",
        "            self.cql_min_q_weight = 10.0\n",
        "        else:\n",
        "            # ç·šæ€§æ’å€¼\n",
        "            alpha_range = (2.0, 0.5)\n",
        "            weight_range = (10.0, 2.0)\n",
        "            t = (current_epsilon - 1) / 9  # æ­£è¦åŒ–åˆ°[0,1]\n",
        "            self.cql_alpha = alpha_range[0] + t * (alpha_range[1] - alpha_range[0])\n",
        "            self.cql_min_q_weight = weight_range[0] + t * (weight_range[1] - weight_range[0])\n",
        "\n",
        "    def reset_dp_engine(self, round_num: int):\n",
        "        \"\"\"é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆå„ªåŒ–ç‰ˆï¼‰\"\"\"\n",
        "        if not self.config.enable_dp_reset or not self.privacy_engine:\n",
        "            return False\n",
        "\n",
        "        logger.info(f\"[C-{self.client_id}] é‡è¨­å·®åˆ†éš±ç§å¼•æ“ï¼ˆRound {round_num}ï¼‰...\")\n",
        "\n",
        "        # æª¢æŸ¥ç•¶è¼ªæ¶ˆè€—é€Ÿç‡\n",
        "        if hasattr(self, '_last_round_epsilon'):\n",
        "            consumption_rate = self.current_epsilon - self._last_round_epsilon\n",
        "            if consumption_rate > self.config.dp_target_epsilon * 0.1:\n",
        "                logger.warning(f\"[C-{self.client_id}] éš±ç§é ç®—æ¶ˆè€—éå¿«: {consumption_rate:.4f}/è¼ª\")\n",
        "\n",
        "        self._last_round_epsilon = self.current_epsilon\n",
        "\n",
        "        try:\n",
        "            self.dp_reset_count += 1\n",
        "            self.last_reset_round = round_num\n",
        "            old_epsilon = self.current_epsilon\n",
        "\n",
        "            self._initialize_dp_engine()\n",
        "\n",
        "            logger.info(f\"   - âœ… é‡è¨­å®Œæˆï¼ˆç¬¬{self.dp_reset_count}æ¬¡ï¼‰\")\n",
        "            logger.info(f\"   - èˆŠÎµ: {old_epsilon:.4f} â†’ æ–°Îµ: {self.current_epsilon:.4f}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"   - é‡è¨­å¤±æ•—: {e}\")\n",
        "            return False\n",
        "\n",
        "    def get_privacy_cost(self):\n",
        "        \"\"\"ç²å–éš±ç§æˆæœ¬ï¼ˆå„ªåŒ–ç‰ˆï¼‰\"\"\"\n",
        "        if not self.privacy_engine:\n",
        "            return 0.0\n",
        "\n",
        "        if hasattr(self, 'privacy_calc_failures') and self.privacy_calc_failures > 10:\n",
        "            return getattr(self, 'last_valid_epsilon', 0.0)\n",
        "\n",
        "        try:\n",
        "            # ä½¿ç”¨é…ç½®çš„deltaå€¼\n",
        "            delta = self.config.dp_target_delta\n",
        "\n",
        "            result = self.privacy_engine.get_epsilon(delta=delta)\n",
        "\n",
        "            if isinstance(result, tuple):\n",
        "                if len(result) == 2:\n",
        "                    epsilon, best_alpha = result\n",
        "                    self.current_best_alpha = best_alpha\n",
        "                else:\n",
        "                    epsilon = result[0]\n",
        "                    self.current_best_alpha = None\n",
        "            elif isinstance(result, (int, float)):\n",
        "                epsilon = result\n",
        "                self.current_best_alpha = None\n",
        "            else:\n",
        "                raise ValueError(f\"æœªçŸ¥çš„epsilonè¿”å›æ ¼å¼: {type(result)}\")\n",
        "\n",
        "            if np.isinf(epsilon) or np.isnan(epsilon) or epsilon < 0:\n",
        "                raise ValueError(f\"ç„¡æ•ˆepsilonå€¼: {epsilon}\")\n",
        "\n",
        "            if hasattr(self, 'privacy_calc_failures'):\n",
        "                self.privacy_calc_failures = 0\n",
        "\n",
        "            self.current_epsilon = float(epsilon)\n",
        "            self.last_valid_epsilon = self.current_epsilon\n",
        "\n",
        "            # å‹•æ…‹èª¿æ•´CQLåƒæ•¸\n",
        "            if self.enable_cql:\n",
        "                self.adjust_cql_params(self.current_epsilon)\n",
        "\n",
        "            return self.current_epsilon\n",
        "\n",
        "        except Exception as primary_error:\n",
        "            if not hasattr(self, 'privacy_calc_failures'):\n",
        "                self.privacy_calc_failures = 0\n",
        "            self.privacy_calc_failures += 1\n",
        "\n",
        "            try:\n",
        "                if hasattr(self, 'dp_steps') and self.dp_steps > 0:\n",
        "                    estimated_epsilon = self.dp_steps * 0.01\n",
        "                    estimated_epsilon = min(estimated_epsilon, self.config.dp_target_epsilon)\n",
        "\n",
        "                    if self.privacy_calc_failures <= 3:\n",
        "                        logger.warning(f\"[C-{self.client_id}] éš±ç§æˆæœ¬è¨ˆç®—å¤±æ•—: {primary_error}\")\n",
        "                        logger.info(f\"   ä½¿ç”¨ä¼°ç®—å€¼: Îµ â‰ˆ {estimated_epsilon:.4f} (åŸºæ–¼ {self.dp_steps} æ­¥)\")\n",
        "\n",
        "                    self.current_epsilon = estimated_epsilon\n",
        "                    return estimated_epsilon\n",
        "\n",
        "            except Exception as fallback_error:\n",
        "                if self.privacy_calc_failures <= 3:\n",
        "                    logger.error(f\"[C-{self.client_id}] å‚™ç”¨éš±ç§è¨ˆç®—ä¹Ÿå¤±æ•—: {fallback_error}\")\n",
        "\n",
        "            if hasattr(self, 'last_valid_epsilon'):\n",
        "                cached_value = self.last_valid_epsilon\n",
        "                if self.privacy_calc_failures <= 3:\n",
        "                    logger.info(f\"[C-{self.client_id}] ä½¿ç”¨ç·©å­˜éš±ç§å€¼: Îµ = {cached_value:.4f}\")\n",
        "                return cached_value\n",
        "\n",
        "            if self.privacy_calc_failures <= 3:\n",
        "                logger.warning(f\"[C-{self.client_id}] éš±ç§æˆæœ¬è¨ˆç®—å®Œå…¨å¤±æ•—: {primary_error}\")\n",
        "                logger.info(f\"   è¿”å› 0.0ï¼Œè¨“ç·´ç¹¼çºŒï¼ˆå¤±æ•—æ¬¡æ•¸: {self.privacy_calc_failures}ï¼‰\")\n",
        "            elif self.privacy_calc_failures == 11:\n",
        "                logger.info(f\"[C-{self.client_id}] éš±ç§è¨ˆç®—æŒçºŒå¤±æ•—ï¼Œå·²åˆ‡æ›è‡³éœé»˜æ¨¡å¼\")\n",
        "\n",
        "            return 0.0\n",
        "\n",
        "    def get_privacy_detailed_info(self):\n",
        "        \"\"\"ç²å–è©³ç´°éš±ç§ä¿¡æ¯ï¼ˆåŒ…å«deltaå’Œnoise_multiplierï¼‰\"\"\"\n",
        "        return {\n",
        "            'client_id': self.client_id,\n",
        "            'epsilon': self.current_epsilon,\n",
        "            'delta': self.config.dp_target_delta,\n",
        "            'noise_multiplier': self.config.dp_noise_multiplier,\n",
        "            'best_alpha': self.current_best_alpha,\n",
        "            'dp_steps': self.dp_steps,\n",
        "            'reset_count': self.dp_reset_count,\n",
        "            'last_reset_round': self.last_reset_round,\n",
        "            'calc_failures': getattr(self, 'privacy_calc_failures', 0),\n",
        "            'last_valid_epsilon': getattr(self, 'last_valid_epsilon', 0.0)\n",
        "        }\n",
        "\n",
        "    def remember(self, *args):\n",
        "        self.memory.append(args)\n",
        "\n",
        "    def set_global_params(self, state_dict):\n",
        "        with torch.no_grad():\n",
        "            self.global_params = [p.clone().detach().cpu() for p in state_dict.values()]\n",
        "\n",
        "    def act(self, state):\n",
        "        if not self.is_eval_agent and np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(torch.from_numpy(state).float().unsqueeze(0).to(self.device))\n",
        "        return q_values.argmax().item()\n",
        "\n",
        "    def get_clean_state_dict(self):\n",
        "        return self.model._module.state_dict() if self.privacy_engine and hasattr(self.model, '_module') else self.model.state_dict()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.get_clean_state_dict())\n",
        "\n",
        "    def get_model_weights_flat(self):\n",
        "        with torch.no_grad():\n",
        "            params = self.model._module.parameters() if self.privacy_engine and hasattr(self.model, '_module') else self.model.parameters()\n",
        "            return torch.cat([p.view(-1) for p in params]).cpu().numpy()\n",
        "\n",
        "    def get_model_for_upload(self):\n",
        "        \"\"\"ç²å–ç”¨æ–¼ä¸Šå‚³çš„æ¨¡å‹ï¼ˆæ”¯æ´ç¨€ç–æ¢¯åº¦å‚³å›ï¼‰\"\"\"\n",
        "        state_dict = self.get_clean_state_dict()\n",
        "\n",
        "        # ã€æ–°å¢ã€‘å¦‚æœæœ‰ Adaptive Clipperï¼Œä¿å­˜å…¶çµ±è¨ˆä¿¡æ¯\n",
        "        if self.adaptive_clipper:\n",
        "            clip_info = {\n",
        "                'current_clip': self.adaptive_clipper.current_clip_value,\n",
        "                'history': self.adaptive_clipper.clip_history[-10:]  # æœ€è¿‘10å€‹è¨˜éŒ„\n",
        "            }\n",
        "            state_dict['_adaptive_clip_info'] = clip_info\n",
        "\n",
        "        # å£“ç¸®æ¨¡å‹\n",
        "        if self.config.enable_compression:\n",
        "            # ä½¿ç”¨åŠç²¾åº¦\n",
        "            compressed_dict = {k: v.half() for k, v in state_dict.items() if not k.startswith('_')}\n",
        "\n",
        "            # å¯é¸ï¼šç¨€ç–è¡¨ç¤ºï¼ˆåƒ…å‚³è¼¸é‡è¦åƒæ•¸ï¼‰\n",
        "            # TODO: å¯¦ç¾ç¨€ç–æ¢¯åº¦å‚³è¼¸\n",
        "\n",
        "            return compressed_dict\n",
        "        else:\n",
        "            return state_dict\n",
        "\n",
        "print(\"âœ… Cell 5: RLAgentï¼ˆæ”¯æ´ CQL & Adaptive Clippingï¼‰å®šç¾©å®Œæˆã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJwvl02DDfvd",
        "outputId": "6bb80929-a904-4f47-effe-da82e1862310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 6: FLServerï¼ˆæ”¯æ´éåŒæ­¥èšåˆå’Œå‹•æ…‹èšé¡ï¼‰å®šç¾©å®Œæˆã€‚\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 6: ğŸŒ è¯é‚¦å­¸ç¿’æœå‹™å™¨ï¼ˆæ”¯æ´éåŒæ­¥èšåˆï¼‰\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import copy\n",
        "import asyncio\n",
        "import concurrent.futures\n",
        "from threading import Lock\n",
        "\n",
        "class FLServer:\n",
        "    \"\"\"\n",
        "    è¯é‚¦å­¸ç¿’æœå‹™å™¨é¡åˆ¥ï¼ˆæ”¯æ´éåŒæ­¥èšåˆå’Œå‹•æ…‹èšé¡ï¼‰\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: TrainingConfig):\n",
        "        self.config = config\n",
        "        self.num_clusters = config.num_clusters\n",
        "        self.client_to_cluster = {}\n",
        "        self.cluster_models = {}\n",
        "        self.aggregation_weights = {}\n",
        "\n",
        "        # ã€æ–°å¢ã€‘å‹•æ…‹èšé¡ç›¸é—œ\n",
        "        self.cluster_update_round = 0\n",
        "        self.cluster_history = []\n",
        "        self.validation_rewards = {}\n",
        "\n",
        "        # ã€æ–°å¢ã€‘éåŒæ­¥èšåˆç›¸é—œ\n",
        "        self.enable_async = config.enable_async_aggregation\n",
        "        self.timeout_threshold = config.timeout_threshold\n",
        "        self.client_timeout = config.client_timeout_seconds\n",
        "        self.pending_updates = {}\n",
        "        self.update_lock = Lock()\n",
        "        self.async_aggregation_count = 0\n",
        "\n",
        "        # åˆå§‹åŒ–èšé¡æ¨¡å‹\n",
        "        for i in range(self.num_clusters):\n",
        "            self.cluster_models[i] = None\n",
        "\n",
        "        logger.info(f\"[FLServer] åˆå§‹åŒ–å®Œæˆ - èšé¡æ•¸: {self.num_clusters}\")\n",
        "        if self.enable_async:\n",
        "            logger.info(f\"[FLServer] éåŒæ­¥èšåˆå·²å•Ÿç”¨ - è¶…æ™‚é–¾å€¼: {self.timeout_threshold}\")\n",
        "\n",
        "    def _convert_state_dict_for_dp_model(self, state_dict: Dict, target_is_dp: bool) -> Dict:\n",
        "        \"\"\"è‡ªå‹•è½‰æ›state_dictæ ¼å¼ä»¥åŒ¹é…DP/éDPæ¨¡å‹\"\"\"\n",
        "        if not state_dict:\n",
        "            return state_dict\n",
        "\n",
        "        converted_dict = {}\n",
        "\n",
        "        for key, value in state_dict.items():\n",
        "            if target_is_dp and not key.startswith('_module.'):\n",
        "                new_key = f\"_module.{key}\"\n",
        "                converted_dict[new_key] = value\n",
        "            elif not target_is_dp and key.startswith('_module.'):\n",
        "                new_key = key.replace('_module.', '')\n",
        "                converted_dict[new_key] = value\n",
        "            else:\n",
        "                converted_dict[key] = value\n",
        "\n",
        "        return converted_dict\n",
        "\n",
        "    def _is_dp_model(self, model) -> bool:\n",
        "        \"\"\"æª¢æ¸¬æ¨¡å‹æ˜¯å¦ç‚ºå·®åˆ†éš±ç§åŒ…è£æ¨¡å‹\"\"\"\n",
        "        if hasattr(model, '_module'):\n",
        "            return True\n",
        "\n",
        "        try:\n",
        "            state_dict = model.state_dict()\n",
        "            return any(k.startswith('_module.') for k in state_dict.keys())\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def distribute_model(self, participating_agents: Dict, global_model_state: Dict):\n",
        "        \"\"\"å°‡æ¨¡å‹åˆ†ç™¼çµ¦åƒèˆ‡çš„å®¢æˆ¶ç«¯ï¼ˆæ”¯æ´çŸ¥è­˜é·ç§»ï¼‰\"\"\"\n",
        "        if not global_model_state:\n",
        "            logger.warning(f\"[FLServer] å…¨åŸŸæ¨¡å‹ç‹€æ…‹ç‚ºç©ºï¼Œè·³éåˆ†ç™¼\")\n",
        "            return\n",
        "\n",
        "        total_clients = len(participating_agents)\n",
        "        successful_distributions = 0\n",
        "        failed_distributions = 0\n",
        "\n",
        "        for client_id, agent in participating_agents.items():\n",
        "            try:\n",
        "                if self.config.mode == 'ClusteredFL':\n",
        "                    cluster_id = self.client_to_cluster.get(client_id, 0)\n",
        "\n",
        "                    # ã€æ–°å¢ã€‘çŸ¥è­˜é·ç§»ï¼šæ–°ç¾¤çµ„æ¨¡å‹ä»¥å…¨å±€æ¨¡å‹åˆå§‹åŒ–\n",
        "                    if self.config.knowledge_transfer and cluster_id in self.cluster_models:\n",
        "                        if self.cluster_models[cluster_id] is None:\n",
        "                            self.cluster_models[cluster_id] = copy.deepcopy(global_model_state)\n",
        "                            logger.info(f\"[FLServer] èšé¡ {cluster_id} ä½¿ç”¨å…¨å±€æ¨¡å‹åˆå§‹åŒ–ï¼ˆçŸ¥è­˜é·ç§»ï¼‰\")\n",
        "\n",
        "                    if cluster_id in self.cluster_models and self.cluster_models[cluster_id] is not None:\n",
        "                        model_to_send = self.cluster_models[cluster_id]\n",
        "                    else:\n",
        "                        model_to_send = global_model_state\n",
        "                else:\n",
        "                    model_to_send = global_model_state\n",
        "\n",
        "                if not model_to_send:\n",
        "                    logger.warning(f\"[FLServer] å®¢æˆ¶ç«¯ {client_id}: ç„¡å¯åˆ†ç™¼æ¨¡å‹\")\n",
        "                    failed_distributions += 1\n",
        "                    continue\n",
        "\n",
        "                agent_is_dp = self._is_dp_model(agent.model)\n",
        "\n",
        "                converted_model = self._convert_state_dict_for_dp_model(\n",
        "                    model_to_send, target_is_dp=agent_is_dp\n",
        "                )\n",
        "\n",
        "                if self.config.enable_compression:\n",
        "                    converted_model = {k: v.half() if hasattr(v, 'half') else v\n",
        "                                     for k, v in converted_model.items()}\n",
        "\n",
        "                try:\n",
        "                    agent.set_global_params(converted_model)\n",
        "                except Exception as e:\n",
        "                    logger.debug(f\"[FLServer] å®¢æˆ¶ç«¯ {client_id}: set_global_paramså¤±æ•—: {e}\")\n",
        "\n",
        "                agent.model.load_state_dict(converted_model, strict=True)\n",
        "                successful_distributions += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                failed_distributions += 1\n",
        "                logger.error(f\"[FLServer] å®¢æˆ¶ç«¯ {client_id} æ¨¡å‹åˆ†ç™¼å¤±æ•—: {e}\")\n",
        "\n",
        "                try:\n",
        "                    agent.model.load_state_dict(model_to_send, strict=False)\n",
        "                    successful_distributions += 1\n",
        "                    logger.info(f\"  - å›é€€æˆåŠŸï¼šéåš´æ ¼æ¨¡å¼è¼‰å…¥\")\n",
        "                except Exception as e2:\n",
        "                    logger.error(f\"  - æ‰€æœ‰å›é€€æ–¹å¼éƒ½å¤±æ•—: {e2}\")\n",
        "\n",
        "        logger.info(f\"[FLServer] æ¨¡å‹åˆ†ç™¼å®Œæˆ: æˆåŠŸ {successful_distributions}/{total_clients}\")\n",
        "        if failed_distributions > 0:\n",
        "            logger.warning(f\"  - å¤±æ•—æ•¸é‡: {failed_distributions}\")\n",
        "\n",
        "    def aggregate_weighted(self, client_updates: List[Tuple[Dict, int]]) -> Dict:\n",
        "        \"\"\"åŠ æ¬Šèšåˆå®¢æˆ¶ç«¯æ¨¡å‹æ›´æ–°ï¼ˆFedAvgå‹•æ…‹åŠ æ¬Šï¼‰\"\"\"\n",
        "        if not client_updates:\n",
        "            return {}\n",
        "\n",
        "        # è¨ˆç®—ç¸½æ¨£æœ¬æ•¸\n",
        "        total_samples = sum(num_samples for _, num_samples in client_updates)\n",
        "        if total_samples == 0:\n",
        "            return client_updates[0][0] if client_updates else {}\n",
        "\n",
        "        first_model = client_updates[0][0]\n",
        "        if not first_model:\n",
        "            return {}\n",
        "\n",
        "        standard_first_model = self._convert_state_dict_for_dp_model(first_model, target_is_dp=False)\n",
        "\n",
        "        aggregated_model = {}\n",
        "        for key in standard_first_model.keys():\n",
        "            aggregated_model[key] = torch.zeros_like(standard_first_model[key])\n",
        "\n",
        "        # ã€æ–°å¢ã€‘FedAvgå‹•æ…‹åŠ æ¬Š\n",
        "        for model_state, num_samples in client_updates:\n",
        "            if not model_state:\n",
        "                continue\n",
        "\n",
        "            # æ ¹æ“šå®¢æˆ¶ç«¯è³‡æ–™é‡è¨ˆç®—æ¬Šé‡\n",
        "            weight = num_samples / total_samples if self.config.use_weighted_fedavg else 1.0 / len(client_updates)\n",
        "\n",
        "            standard_model_state = self._convert_state_dict_for_dp_model(model_state, target_is_dp=False)\n",
        "\n",
        "            for key in aggregated_model.keys():\n",
        "                if key in standard_model_state:\n",
        "                    param_tensor = standard_model_state[key]\n",
        "                    if param_tensor.device != aggregated_model[key].device:\n",
        "                        param_tensor = param_tensor.to(aggregated_model[key].device)\n",
        "\n",
        "                    aggregated_model[key] += weight * param_tensor\n",
        "\n",
        "        return aggregated_model\n",
        "\n",
        "    async def async_aggregate(self, client_updates_dict: Dict[int, Tuple[Dict, int]],\n",
        "                            timeout: float = None) -> Dict:\n",
        "        \"\"\"ã€æ–°å¢ã€‘éåŒæ­¥èšåˆæ–¹æ³•ï¼ˆä¸ä¾è³´rayï¼‰\"\"\"\n",
        "        if timeout is None:\n",
        "            timeout = self.client_timeout\n",
        "\n",
        "        logger.info(f\"[FLServer] é–‹å§‹éåŒæ­¥èšåˆ - è¶…æ™‚è¨­å®š: {timeout}s\")\n",
        "\n",
        "        collected_updates = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        # ä½¿ç”¨ asyncio å¯¦ç¾éåŒæ­¥èšåˆ\n",
        "        async def collect_with_timeout(client_id, update):\n",
        "            try:\n",
        "                # æ¨¡æ“¬éåŒæ­¥è™•ç†\n",
        "                await asyncio.sleep(0.001)  # æ¥µçŸ­çš„å»¶é²ä»¥å…è¨±å…¶ä»–å”ç¨‹åŸ·è¡Œ\n",
        "                return client_id, update\n",
        "            except asyncio.TimeoutError:\n",
        "                logger.warning(f\"[FLServer] å®¢æˆ¶ç«¯ {client_id} è¶…æ™‚\")\n",
        "                return client_id, None\n",
        "\n",
        "        # å‰µå»ºéåŒæ­¥ä»»å‹™\n",
        "        tasks = []\n",
        "        for client_id, update in client_updates_dict.items():\n",
        "            task = asyncio.create_task(collect_with_timeout(client_id, update))\n",
        "            tasks.append(task)\n",
        "\n",
        "        # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆæˆ–è¶…æ™‚\n",
        "        try:\n",
        "            done, pending = await asyncio.wait(tasks, timeout=timeout)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"[FLServer] éåŒæ­¥ç­‰å¾…éŒ¯èª¤: {e}\")\n",
        "            # å›é€€åˆ°åŒæ­¥èšåˆ\n",
        "            return self.aggregate_weighted(list(client_updates_dict.values()))\n",
        "\n",
        "        # æ”¶é›†å®Œæˆçš„æ›´æ–°\n",
        "        for task in done:\n",
        "            try:\n",
        "                client_id, update = await task\n",
        "                if update is not None:\n",
        "                    collected_updates.append(update)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"[FLServer] æ”¶é›†æ›´æ–°éŒ¯èª¤: {e}\")\n",
        "\n",
        "        # å–æ¶ˆæœªå®Œæˆçš„ä»»å‹™\n",
        "        for task in pending:\n",
        "            task.cancel()\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        timeout_count = len(client_updates_dict) - len(collected_updates)\n",
        "        success_count = len(collected_updates)\n",
        "\n",
        "        logger.info(f\"[FLServer] éåŒæ­¥èšåˆå®Œæˆ:\")\n",
        "        logger.info(f\"  - æˆåŠŸæ”¶é›†: {success_count}/{len(client_updates_dict)}\")\n",
        "        logger.info(f\"  - è¶…æ™‚å®¢æˆ¶ç«¯: {timeout_count}\")\n",
        "        logger.info(f\"  - è€—æ™‚: {elapsed_time:.2f}s\")\n",
        "\n",
        "        # å¦‚æœæ”¶é›†åˆ°è¶³å¤ çš„æ›´æ–°ï¼Œé€²è¡Œèšåˆ\n",
        "        if collected_updates:\n",
        "            self.async_aggregation_count += 1\n",
        "            return self.aggregate_weighted(collected_updates)\n",
        "        else:\n",
        "            logger.warning(f\"[FLServer] ç„¡æœ‰æ•ˆæ›´æ–°å¯èšåˆ\")\n",
        "            return {}\n",
        "\n",
        "    def should_use_async_aggregation(self, client_responses: Dict[int, bool]) -> bool:\n",
        "        \"\"\"ã€æ–°å¢ã€‘åˆ¤æ–·æ˜¯å¦æ‡‰ä½¿ç”¨éåŒæ­¥èšåˆ\"\"\"\n",
        "        if not self.enable_async:\n",
        "            return False\n",
        "\n",
        "        total_clients = len(client_responses)\n",
        "        timeout_clients = sum(1 for responded in client_responses.values() if not responded)\n",
        "        timeout_ratio = timeout_clients / total_clients if total_clients > 0 else 0\n",
        "\n",
        "        should_async = timeout_ratio > self.timeout_threshold\n",
        "\n",
        "        if should_async:\n",
        "            logger.info(f\"[FLServer] è§¸ç™¼éåŒæ­¥èšåˆ - è¶…æ™‚æ¯”ä¾‹: {timeout_ratio:.2%} > {self.timeout_threshold:.0%}\")\n",
        "\n",
        "        return should_async\n",
        "\n",
        "    def update_clusters(self, client_agents: Dict, current_round: int, validation_rewards: Dict = None):\n",
        "        \"\"\"æ›´æ–°å®¢æˆ¶ç«¯èšé¡ï¼ˆå‹•æ…‹é »ç‡å’Œé¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰\"\"\"\n",
        "        self.cluster_update_round = current_round\n",
        "\n",
        "        # ã€æ–°å¢ã€‘å‹•æ…‹èª¿æ•´èšé¡æ›´æ–°é »ç‡\n",
        "        update_freq = self.config.get_dynamic_cluster_freq(current_round)\n",
        "        if current_round % update_freq != 0:\n",
        "            return\n",
        "\n",
        "        # ã€æ–°å¢ã€‘ç¬¬Kè¼ªå¾Œä½¿ç”¨é¤˜å¼¦ç›¸ä¼¼åº¦åˆ¤æ–·\n",
        "        if current_round > 3 and self.config.use_cosine_similarity and validation_rewards:\n",
        "            if not self._should_update_clusters_by_similarity(validation_rewards):\n",
        "                logger.info(f\"[FLServer] Round {current_round}: é¤˜å¼¦ç›¸ä¼¼åº¦æª¢æŸ¥ï¼Œç„¡éœ€é‡æ–°èšé¡\")\n",
        "                return\n",
        "\n",
        "        if len(client_agents) < self.num_clusters:\n",
        "            for i, client_id in enumerate(client_agents.keys()):\n",
        "                self.client_to_cluster[client_id] = i % self.num_clusters\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            client_features = []\n",
        "            client_ids = []\n",
        "\n",
        "            for client_id, agent in client_agents.items():\n",
        "                try:\n",
        "                    weights = agent.get_model_weights_flat()\n",
        "                    if weights is not None and len(weights) > 0:\n",
        "                        client_features.append(weights)\n",
        "                        client_ids.append(client_id)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"[FLServer] å®¢æˆ¶ç«¯ {client_id} ç‰¹å¾µæå–å¤±æ•—: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if len(client_features) < self.num_clusters:\n",
        "                for i, client_id in enumerate(client_ids):\n",
        "                    self.client_to_cluster[client_id] = i % self.num_clusters\n",
        "                return\n",
        "\n",
        "            client_features_array = np.vstack(client_features)\n",
        "\n",
        "            # æ¨™æº–åŒ–ç‰¹å¾µ\n",
        "            scaler = StandardScaler()\n",
        "            normalized_features = scaler.fit_transform(client_features_array)\n",
        "\n",
        "            # K-meansèšé¡\n",
        "            kmeans = KMeans(n_clusters=self.num_clusters, random_state=42, n_init=10)\n",
        "            cluster_labels = kmeans.fit_predict(normalized_features)\n",
        "\n",
        "            # è¨˜éŒ„èšé¡è®ŠåŒ–\n",
        "            old_clusters = self.client_to_cluster.copy()\n",
        "            reassignment_count = 0\n",
        "\n",
        "            for client_id, cluster_label in zip(client_ids, cluster_labels):\n",
        "                if client_id in old_clusters and old_clusters[client_id] != cluster_label:\n",
        "                    reassignment_count += 1\n",
        "                self.client_to_cluster[client_id] = int(cluster_label)\n",
        "\n",
        "            # è¨˜éŒ„èšé¡æ­·å²\n",
        "            self.cluster_history.append({\n",
        "                'round': current_round,\n",
        "                'reassignment_count': reassignment_count,\n",
        "                'cluster_assignments': self.client_to_cluster.copy()\n",
        "            })\n",
        "\n",
        "            logger.info(f\"[FLServer] Round {current_round}: èšé¡æ›´æ–°å®Œæˆï¼Œé‡æ–°åˆ†é…æ•¸: {reassignment_count}\")\n",
        "            for cluster_id in range(self.num_clusters):\n",
        "                clients_in_cluster = [cid for cid, cid_cluster in self.client_to_cluster.items()\n",
        "                                    if cid_cluster == cluster_id]\n",
        "                logger.info(f\"  èšé¡ {cluster_id}: {clients_in_cluster}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"[FLServer] èšé¡æ›´æ–°å¤±æ•—: {e}\")\n",
        "            for i, client_id in enumerate(client_agents.keys()):\n",
        "                self.client_to_cluster[client_id] = i % self.num_clusters\n",
        "\n",
        "    def _should_update_clusters_by_similarity(self, validation_rewards: Dict) -> bool:\n",
        "        \"\"\"ä½¿ç”¨é¤˜å¼¦ç›¸ä¼¼åº¦åˆ¤æ–·æ˜¯å¦éœ€è¦é‡æ–°èšé¡\"\"\"\n",
        "        if not self.validation_rewards:\n",
        "            self.validation_rewards = validation_rewards\n",
        "            return True\n",
        "\n",
        "        # è¨ˆç®—çå‹µå‘é‡çš„é¤˜å¼¦ç›¸ä¼¼åº¦\n",
        "        old_rewards = np.array(list(self.validation_rewards.values()))\n",
        "        new_rewards = np.array(list(validation_rewards.values()))\n",
        "\n",
        "        if len(old_rewards) != len(new_rewards):\n",
        "            self.validation_rewards = validation_rewards\n",
        "            return True\n",
        "\n",
        "        similarity = cosine_similarity([old_rewards], [new_rewards])[0][0]\n",
        "\n",
        "        # å¦‚æœç›¸ä¼¼åº¦ä½æ–¼é–¾å€¼ï¼Œéœ€è¦é‡æ–°èšé¡\n",
        "        threshold = 0.95\n",
        "        should_update = similarity < threshold\n",
        "\n",
        "        if should_update:\n",
        "            self.validation_rewards = validation_rewards\n",
        "\n",
        "        return should_update\n",
        "\n",
        "    def get_cluster_info(self) -> Dict:\n",
        "        \"\"\"ç²å–èšé¡ä¿¡æ¯\"\"\"\n",
        "        cluster_info = {}\n",
        "        for cluster_id in range(self.num_clusters):\n",
        "            clients_in_cluster = [cid for cid, cid_cluster in self.client_to_cluster.items()\n",
        "                                if cid_cluster == cluster_id]\n",
        "            cluster_info[cluster_id] = {\n",
        "                'clients': clients_in_cluster,\n",
        "                'size': len(clients_in_cluster),\n",
        "                'has_model': cluster_id in self.cluster_models and self.cluster_models[cluster_id] is not None,\n",
        "                'async_aggregations': self.async_aggregation_count,\n",
        "                'update_round': self.cluster_update_round\n",
        "            }\n",
        "        return cluster_info\n",
        "\n",
        "    def get_cluster_reassignment_count(self) -> int:\n",
        "        \"\"\"ç²å–èšé¡é‡æ–°åˆ†é…ç¸½æ¬¡æ•¸\"\"\"\n",
        "        return sum(h['reassignment_count'] for h in self.cluster_history)\n",
        "\n",
        "print(\"âœ… Cell 6: FLServerï¼ˆæ”¯æ´éåŒæ­¥èšåˆå’Œå‹•æ…‹èšé¡ï¼‰å®šç¾©å®Œæˆã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXH5-UoNIiUP",
        "outputId": "cf1f5363-0743-4e25-b721-72ba0ac39f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:jsonlines æœªå®‰è£ï¼Œä½¿ç”¨æ¨™æº– JSON\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 7: ExperimentRunnerï¼ˆå®Œæ•´å¼·åŒ–ç‰ˆï¼‰å®šç¾©å®Œæˆã€‚\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 7: ğŸš€ ExperimentRunnerï¼ˆå®Œæ•´å¼·åŒ–ç‰ˆï¼‰\n",
        "import scipy.stats as stats\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import asyncio\n",
        "\n",
        "# å˜—è©¦å°å…¥ wandb å’Œ jsonlinesï¼Œå¦‚æœå¤±æ•—å‰‡ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ\n",
        "try:\n",
        "    import wandb\n",
        "    WANDB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WANDB_AVAILABLE = False\n",
        "    logger.warning(\"WandB æœªå®‰è£ï¼Œåƒ…ä½¿ç”¨æœ¬åœ°æ—¥èªŒ\")\n",
        "\n",
        "try:\n",
        "    import jsonlines\n",
        "    JSONLINES_AVAILABLE = True\n",
        "except ImportError:\n",
        "    JSONLINES_AVAILABLE = False\n",
        "    logger.warning(\"jsonlines æœªå®‰è£ï¼Œä½¿ç”¨æ¨™æº– JSON\")\n",
        "\n",
        "class ExperimentRunner:\n",
        "    def __init__(self, config: TrainingConfig, data_manager: DataManager, all_trajectories, client_pairs):\n",
        "        self.config, self.data_manager, self.server = config, data_manager, FLServer(config)\n",
        "        self.training_history, self.evaluation_results, self.privacy_costs = [], [], []\n",
        "\n",
        "        # éš±ç§é ç®—ç®¡ç†\n",
        "        self.total_privacy_budget = config.dp_target_epsilon if config.enable_dp else 0.0\n",
        "        self.consumed_privacy_budget = 0.0\n",
        "        self.privacy_budget_exceeded = False\n",
        "        self.dp_reset_history = []\n",
        "        self.round_privacy_costs = []\n",
        "        self.detailed_privacy_logs = []\n",
        "\n",
        "        # æ­»å¾ªç’°é˜²è­·\n",
        "        self.max_resets_per_round = 3\n",
        "        self.current_round_resets = 0\n",
        "        self.consecutive_reset_rounds = 0\n",
        "        self.max_consecutive_resets = 5\n",
        "\n",
        "        # ã€æ–°å¢ã€‘å»¶é²ç›£æ§å™¨ï¼ˆä»»å‹™5ï¼‰\n",
        "        self.latency_monitor = LatencyMonitor(sla_threshold=CONFIG.get(\"LATENCY_SLA\", 0.05))\n",
        "\n",
        "        # ã€æ–°å¢ã€‘CQL æ€§èƒ½è¿½è¹¤ï¼ˆä»»å‹™2ï¼‰\n",
        "        self.cql_performance_history = []\n",
        "\n",
        "        # ã€æ–°å¢ã€‘Adaptive Clipping çµ±è¨ˆ\n",
        "        self.adaptive_clip_stats = []\n",
        "\n",
        "        # ã€æ–°å¢ã€‘æ”¶æ–‚çµ±è¨ˆ\n",
        "        self.convergence_stats = []\n",
        "\n",
        "        # ã€æ–°å¢ã€‘æ—©æœŸåœæ­¢ç›£æ§\n",
        "        self.best_validation_reward = -float('inf')\n",
        "        self.early_stopping_counter = 0\n",
        "\n",
        "        # åˆå§‹åŒ– wandbï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
        "        if WANDB_AVAILABLE and config.mode != 'Centralized':\n",
        "            try:\n",
        "                wandb.init(\n",
        "                    project=\"federated-rl-enhanced\",\n",
        "                    name=config.experiment_name,\n",
        "                    config=asdict(config)\n",
        "                )\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"WandB åˆå§‹åŒ–å¤±æ•—: {e}\")\n",
        "\n",
        "        self._set_seeds()\n",
        "\n",
        "        logger.info(\"[ExperimentRunner] æ­£åœ¨åˆå§‹åŒ–å®¢æˆ¶ç«¯ç’°å¢ƒèˆ‡ä»£ç†...\")\n",
        "\n",
        "        if self.config.enable_dp and self.config.mode != 'Centralized':\n",
        "            logger.info(f\"å·®åˆ†éš±ç§æ¨¡å¼å•Ÿç”¨ï¼ˆå¼·åŒ–ç‰ˆï¼‰ï¼š\")\n",
        "            logger.info(f\"   - ç¸½éš±ç§é ç®—ä¸Šé™: Îµ={self.total_privacy_budget}\")\n",
        "            logger.info(f\"   - Adaptive Clipping: {'å•Ÿç”¨' if config.enable_adaptive_clip else 'ç¦ç”¨'}\")\n",
        "            logger.info(f\"   - éåŒæ­¥èšåˆ: {'å•Ÿç”¨' if config.enable_async_aggregation else 'ç¦ç”¨'}\")\n",
        "\n",
        "        # åˆå§‹åŒ–å®¢æˆ¶ç«¯ç’°å¢ƒ\n",
        "        self.client_envs = {cid: PairedEnv(traj, config) for cid, traj in all_trajectories.items() if traj.size > 0}\n",
        "        if not self.client_envs:\n",
        "            raise ValueError(\"DataManager æœªèƒ½ç‚ºä»»ä½•å®¢æˆ¶ç«¯å‰µå»ºæœ‰æ•ˆçš„ç’°å¢ƒã€‚\")\n",
        "\n",
        "        self.config.num_clients = len(self.client_envs)\n",
        "\n",
        "        if self.config.mode == \"Centralized\":\n",
        "            central_config = copy.deepcopy(config)\n",
        "            central_config.enable_dp = False\n",
        "            pooled_trajectory = np.vstack([traj for traj in all_trajectories.values() if traj.size > 0])\n",
        "            self.central_env = PairedEnv(pooled_trajectory, central_config)\n",
        "            self.central_agent = RLAgent(self.central_env.state_size, self.central_env.action_size, central_config, 0, len(pooled_trajectory), False)\n",
        "            self.client_agents = {}\n",
        "        else:\n",
        "            self.client_agents = {}\n",
        "            for cid, env in self.client_envs.items():\n",
        "                dataset_size = len(env.trajectory) if env.trajectory.size > 0 else 1\n",
        "                self.client_agents[cid] = RLAgent(env.state_size, env.action_size, config, cid, dataset_size, False)\n",
        "\n",
        "        if self.client_agents:\n",
        "            self.global_model_state = self.client_agents[next(iter(self.client_agents))].get_clean_state_dict()\n",
        "        else:\n",
        "            self.global_model_state = self.central_agent.get_clean_state_dict()\n",
        "\n",
        "        self.config.save()\n",
        "        logger.info(\"[ExperimentRunner] åˆå§‹åŒ–å®Œæˆã€‚\")\n",
        "\n",
        "    def _set_seeds(self):\n",
        "        \"\"\"ä½¿ç”¨å¼·åŒ–çš„ deterministic è¨­å®š\"\"\"\n",
        "        set_global_seed(self.config.random_seed)\n",
        "\n",
        "    def _check_privacy_budget_and_reset(self, round_privacy_costs, current_round):\n",
        "        \"\"\"éš±ç§é ç®—æª¢æŸ¥èˆ‡æ™ºèƒ½é‡è¨­æ©Ÿåˆ¶ï¼ˆå„ªåŒ–ç‰ˆï¼‰\"\"\"\n",
        "        if not self.config.enable_dp or self.config.mode == 'Centralized':\n",
        "            return\n",
        "\n",
        "        if current_round != getattr(self, '_last_checked_round', -1):\n",
        "            self.current_round_resets = 0\n",
        "            self._last_checked_round = current_round\n",
        "\n",
        "        round_detailed_info = []\n",
        "        for cid, agent in self.client_agents.items():\n",
        "            detailed_info = agent.get_privacy_detailed_info()\n",
        "            detailed_info['round'] = current_round\n",
        "            round_detailed_info.append(detailed_info)\n",
        "        self.detailed_privacy_logs.extend(round_detailed_info)\n",
        "\n",
        "        if round_privacy_costs:\n",
        "            round_avg_epsilon = np.mean(round_privacy_costs)\n",
        "            round_max_epsilon = np.max(round_privacy_costs)\n",
        "            round_min_epsilon = np.min(round_privacy_costs)\n",
        "\n",
        "            # è¨ˆç®—ç•¶è¼ªæ¶ˆè€—é€Ÿç‡\n",
        "            if hasattr(self, '_last_round_avg_epsilon'):\n",
        "                consumption_rate = round_avg_epsilon - self._last_round_avg_epsilon\n",
        "            else:\n",
        "                consumption_rate = round_avg_epsilon\n",
        "            self._last_round_avg_epsilon = round_avg_epsilon\n",
        "\n",
        "            self.consumed_privacy_budget += round_avg_epsilon\n",
        "\n",
        "            self.round_privacy_costs.append({\n",
        "                'round': current_round,\n",
        "                'avg_epsilon': round_avg_epsilon,\n",
        "                'max_epsilon': round_max_epsilon,\n",
        "                'min_epsilon': round_min_epsilon,\n",
        "                'std_epsilon': np.std(round_privacy_costs),\n",
        "                'participating_clients': len(round_privacy_costs),\n",
        "                'cumulative_epsilon': self.consumed_privacy_budget,\n",
        "                'round_resets': self.current_round_resets,\n",
        "                'consumption_rate': consumption_rate\n",
        "            })\n",
        "\n",
        "            budget_ratio = self.consumed_privacy_budget / self.total_privacy_budget\n",
        "            reset_threshold = self.total_privacy_budget * self.config.dp_reset_threshold_multiplier\n",
        "\n",
        "            if budget_ratio > 1.0 and not self.privacy_budget_exceeded:\n",
        "                logger.warning(f\"\\n{'='*20} âš ï¸ éš±ç§é ç®—é¦–æ¬¡è¶…æ”¯ï¼ {'='*20}\")\n",
        "                logger.warning(f\"   - ç•¶å‰æ¶ˆè€—: Îµ = {self.consumed_privacy_budget:.4f}\")\n",
        "                logger.warning(f\"   - é ç®—ä¸Šé™: Îµ = {self.total_privacy_budget}\")\n",
        "                logger.warning(f\"{'='*58}\")\n",
        "                self.privacy_budget_exceeded = True\n",
        "\n",
        "            # æ ¹æ“šæ¶ˆè€—é€Ÿç‡æ±ºå®šæ˜¯å¦é‡è¨­\n",
        "            should_reset = (\n",
        "                self.consumed_privacy_budget > reset_threshold and\n",
        "                self.config.enable_dp_reset and\n",
        "                self.current_round_resets < self.max_resets_per_round and\n",
        "                self.consecutive_reset_rounds < self.max_consecutive_resets and\n",
        "                consumption_rate > self.total_privacy_budget * 0.05  # æ¶ˆè€—é€Ÿç‡è¶…é5%æ‰é‡è¨­\n",
        "            )\n",
        "\n",
        "            if should_reset:\n",
        "                logger.info(f\"\\nğŸ”„ è§¸ç™¼éš±ç§é ç®—é‡è¨­æ©Ÿåˆ¶ï¼ˆRound {current_round}ï¼‰\")\n",
        "                logger.info(f\"   - æ¶ˆè€—é€Ÿç‡: {consumption_rate:.4f}/è¼ª\")\n",
        "\n",
        "                reset_count = 0\n",
        "                successful_resets = []\n",
        "                reset_failures = []\n",
        "\n",
        "                for cid, agent in self.client_agents.items():\n",
        "                    if agent.reset_dp_engine(current_round):\n",
        "                        reset_count += 1\n",
        "                        successful_resets.append(cid)\n",
        "                    else:\n",
        "                        reset_failures.append(cid)\n",
        "\n",
        "                if reset_count > 0:\n",
        "                    self.current_round_resets += 1\n",
        "                    self.consecutive_reset_rounds += 1\n",
        "\n",
        "                    self.dp_reset_history.append({\n",
        "                        'round': current_round,\n",
        "                        'reset_count': reset_count,\n",
        "                        'total_clients': len(self.client_agents),\n",
        "                        'successful_resets': successful_resets,\n",
        "                        'failed_resets': reset_failures,\n",
        "                        'budget_before_reset': self.consumed_privacy_budget,\n",
        "                        'trigger_threshold': reset_threshold,\n",
        "                        'round_reset_number': self.current_round_resets,\n",
        "                        'consumption_rate': consumption_rate\n",
        "                    })\n",
        "\n",
        "                    self.consumed_privacy_budget = 0.0\n",
        "                    self.privacy_budget_exceeded = False\n",
        "                    logger.info(f\"   - âœ… é‡è¨­å®Œæˆï¼Œæ–°ä¸€è¼ªé ç®—é–‹å§‹\")\n",
        "            else:\n",
        "                self.consecutive_reset_rounds = 0\n",
        "\n",
        "    def _train_agent_locally(self, agent: RLAgent, env: PairedEnv, episodes: int, is_finetune: bool = False):\n",
        "        \"\"\"è¨“ç·´é‚è¼¯ï¼Œå¢åŠ æ­¥æ•¸ç›£æ§å’Œ CQL æ”¯æ´\"\"\"\n",
        "        agent.model.train()\n",
        "        total_loss, total_reward, training_steps, episode_count = 0.0, 0.0, 0, 0\n",
        "\n",
        "        if episodes == 0:\n",
        "            return 0.0, 0.0, 0.0\n",
        "\n",
        "        initial_dp_steps = getattr(agent, 'dp_steps', 0)\n",
        "        max_steps_per_round = self.config.local_episodes_per_round * self.config.steps_per_episode * 2\n",
        "\n",
        "        # ã€æ–°å¢ã€‘è¨˜éŒ„ CQL ç‰¹å®šæŒ‡æ¨™\n",
        "        cql_losses = []\n",
        "\n",
        "        # é–‹å§‹è¨ˆæ™‚å®¢æˆ¶ç«¯è¨ˆç®—\n",
        "        client_compute_start = time.time()\n",
        "\n",
        "        for episode in range(episodes):\n",
        "            state, episode_reward, done = env.reset(), 0.0, False\n",
        "            for step in range(1, self.config.steps_per_episode + 1):\n",
        "                action = agent.act(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                agent.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "\n",
        "                if len(agent.memory) > self.config.replay_start_size and step % self.config.replay_frequency == 0:\n",
        "                    loss = agent.replay(num_batches=self.config.replay_batches_per_call)\n",
        "                    total_loss += loss\n",
        "                    training_steps += 1\n",
        "\n",
        "                    if agent.enable_cql:\n",
        "                        cql_losses.append(loss)\n",
        "\n",
        "                    current_dp_steps = getattr(agent, 'dp_steps', 0)\n",
        "                    if current_dp_steps - initial_dp_steps > max_steps_per_round:\n",
        "                        logger.warning(f\"[C-{agent.client_id}] è¨“ç·´æ­¥æ•¸è¶…é™ï¼Œæå‰çµæŸ\")\n",
        "                        break\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            total_reward += episode_reward\n",
        "            episode_count += 1\n",
        "            if (episode + 1) % self.config.target_update_freq == 0:\n",
        "                agent.update_target_model()\n",
        "\n",
        "        # è¨˜éŒ„å®¢æˆ¶ç«¯è¨ˆç®—æ™‚é–“ï¼ˆä¿®æ­£ç¸®æ’ï¼‰\n",
        "        client_compute_time = time.time() - client_compute_start\n",
        "        self.latency_monitor.record_component('client_compute_time', client_compute_time)\n",
        "\n",
        "        if not agent.is_eval_agent and not is_finetune and agent.epsilon > self.config.epsilon_min:\n",
        "            agent.epsilon *= self.config.epsilon_decay\n",
        "\n",
        "        avg_loss = total_loss / training_steps if training_steps > 0 else 0.0\n",
        "        avg_reward = total_reward / episode_count if episode_count > 0 else 0.0\n",
        "        privacy_cost = agent.get_privacy_cost() if training_steps > 0 and self.config.enable_dp and agent.privacy_engine else 0.0\n",
        "\n",
        "        # ã€æ–°å¢ã€‘è¨˜éŒ„ CQL æ€§èƒ½\n",
        "        if agent.enable_cql and cql_losses:\n",
        "            self.cql_performance_history.append({\n",
        "                'client_id': agent.client_id,\n",
        "                'avg_cql_loss': np.mean(cql_losses),\n",
        "                'min_cql_loss': np.min(cql_losses),\n",
        "                'max_cql_loss': np.max(cql_losses),\n",
        "                'round': getattr(self, '_current_round', 0)\n",
        "            })\n",
        "\n",
        "        return avg_loss, avg_reward, privacy_cost\n",
        "\n",
        "    def _evaluate_and_log(self, round_num: int, participating_agents: Dict):\n",
        "        \"\"\"è©•ä¼°ä¸¦è¨˜éŒ„reward_global_modelå’Œreward_pfl_finetuned\"\"\"\n",
        "        eval_results = []\n",
        "\n",
        "        for client_id, agent in participating_agents.items():\n",
        "            env = self.client_envs[client_id]\n",
        "\n",
        "            # è©•ä¼°å…¨å±€æ¨¡å‹\n",
        "            global_reward = self._evaluate_agent(env, self.global_model_state, num_episodes=5)\n",
        "\n",
        "            # è©•ä¼°å€‹æ€§åŒ–æ¨¡å‹ï¼ˆPFLå¾®èª¿å¾Œï¼‰\n",
        "            pfl_reward = self._evaluate_agent(env, agent.get_clean_state_dict(), num_episodes=5)\n",
        "\n",
        "            eval_results.append({\n",
        "                'round': round_num,\n",
        "                'client_id': client_id,\n",
        "                'reward_global_model': global_reward,\n",
        "                'reward_pfl_finetuned': pfl_reward\n",
        "            })\n",
        "\n",
        "        return eval_results\n",
        "\n",
        "    def _run_federated_training(self):\n",
        "        \"\"\"è¯é‚¦è¨“ç·´ä¸»æµç¨‹ï¼ˆæ”¯æ´æ‰€æœ‰å¼·åŒ–åŠŸèƒ½ï¼‰\"\"\"\n",
        "        logger.info(f\"\\n[æ¨¡å¼] åŸ·è¡Œè¯é‚¦å¼è¨“ç·´ ({self.config.mode})\")\n",
        "        available_client_ids = list(self.client_agents.keys())\n",
        "        progress_bar = tqdm(range(self.config.comm_rounds), desc=f\"{self.config.mode} Training\")\n",
        "\n",
        "        for comm_round in progress_bar:\n",
        "            self._current_round = comm_round  # ä¿å­˜ç•¶å‰è¼ªæ•¸\n",
        "            self.latency_monitor.start_round()  # é–‹å§‹è¨ˆæ™‚\n",
        "            logger.info(f\"\\n--- é–‹å§‹ Round {comm_round+1}/{self.config.comm_rounds} ---\")\n",
        "\n",
        "            # ã€æ–°å¢ã€‘æ—©æœŸåœæ­¢æª¢æŸ¥\n",
        "            if self.early_stopping_counter > self.config.early_stopping_rounds:\n",
        "                logger.info(f\"è§¸ç™¼æ—©æœŸåœæ­¢ï¼Œé©—è­‰çå‹µå·²{self.config.early_stopping_rounds}è¼ªæœªæå‡\")\n",
        "                break\n",
        "\n",
        "            # èšé¡æ›´æ–°ï¼ˆå‹•æ…‹é »ç‡ï¼‰\n",
        "            if self.config.mode == 'ClusteredFL' and comm_round > 0:\n",
        "                update_freq = self.config.get_dynamic_cluster_freq(comm_round)\n",
        "                if comm_round % update_freq == 0:\n",
        "                    # ç²å–é©—è­‰çå‹µç”¨æ–¼é¤˜å¼¦ç›¸ä¼¼åº¦åˆ¤æ–·\n",
        "                    validation_rewards = {}\n",
        "                    for cid, agent in self.client_agents.items():\n",
        "                        env = self.client_envs[cid]\n",
        "                        reward = self._evaluate_agent(env, agent.get_clean_state_dict(), num_episodes=3)\n",
        "                        validation_rewards[cid] = reward\n",
        "\n",
        "                    self.server.update_clusters(self.client_agents, comm_round, validation_rewards)\n",
        "\n",
        "                    # è¨˜éŒ„èšé¡é‡æ–°åˆ†é…æ¬¡æ•¸\n",
        "                    reassign_count = self.server.get_cluster_reassignment_count()\n",
        "                    self.data_manager.cluster_reassign_count = reassign_count\n",
        "\n",
        "            # å®¢æˆ¶ç«¯é¸æ“‡\n",
        "            num_to_select = min(self.config.num_clients_to_select, len(available_client_ids))\n",
        "            selected_ids = np.random.choice(available_client_ids, num_to_select, replace=False)\n",
        "            participating_ids = list(selected_ids)\n",
        "            straggler_ids = set()\n",
        "\n",
        "            # ç•°è³ªæ€§æ¨¡æ“¬\n",
        "            if self.config.enable_heterogeneity and len(participating_ids) > 1:\n",
        "                num_dropouts = int(self.config.dropout_ratio * len(participating_ids))\n",
        "                if num_dropouts > 0 and len(participating_ids) > num_dropouts:\n",
        "                    dropout_ids = set(np.random.choice(participating_ids, num_dropouts, replace=False))\n",
        "                    participating_ids = [cid for cid in participating_ids if cid not in dropout_ids]\n",
        "\n",
        "                if participating_ids and len(participating_ids) > 1:\n",
        "                    num_stragglers = int(self.config.straggler_ratio * len(participating_ids))\n",
        "                    if num_stragglers > 0:\n",
        "                        straggler_ids = set(np.random.choice(participating_ids, num_stragglers, replace=False))\n",
        "\n",
        "            if not participating_ids:\n",
        "                continue\n",
        "\n",
        "            logger.info(f\"åƒèˆ‡å®¢æˆ¶ç«¯: {participating_ids}\")\n",
        "\n",
        "            participating_agents = {cid: self.client_agents[cid] for cid in participating_ids}\n",
        "            self.server.distribute_model(participating_agents, self.global_model_state)\n",
        "\n",
        "            # ã€æ–°å¢ã€‘æ›´æ–°å®¢æˆ¶ç«¯ç•°è³ªæ€§ï¼ˆç”¨æ–¼FedProxï¼‰\n",
        "            if self.config.mode in ['FedProx', 'ClusteredFL'] and self.config.fedprox_mu_adaptive:\n",
        "                all_client_params = [agent.get_model_weights_flat() for agent in participating_agents.values()]\n",
        "                for agent in participating_agents.values():\n",
        "                    other_params = [p for p in all_client_params if not np.array_equal(p, agent.get_model_weights_flat())]\n",
        "                    agent.update_client_heterogeneity(other_params)\n",
        "\n",
        "            # ã€æ–°å¢ã€‘éåŒæ­¥è¨“ç·´å’Œèšåˆ\n",
        "            client_updates_dict = {}\n",
        "            client_responses = {}\n",
        "            round_losses, round_rewards, round_privacy_costs = [], [], []\n",
        "\n",
        "            # è¨˜éŒ„é€šä¿¡é–‹å§‹æ™‚é–“\n",
        "            comm_start = time.time()\n",
        "\n",
        "            # æ¨¡æ“¬å®¢æˆ¶ç«¯è¨“ç·´\n",
        "            for cid in participating_ids:\n",
        "                agent, env = self.client_agents[cid], self.client_envs[cid]\n",
        "                episodes = (self.config.local_episodes_per_round // 2 if cid in straggler_ids\n",
        "                           else self.config.local_episodes_per_round)\n",
        "\n",
        "                # æ¨¡æ“¬è¶…æ™‚\n",
        "                is_timeout = (self.config.enable_async_aggregation and\n",
        "                            np.random.random() < 0.1)  # 10% è¶…æ™‚æ©Ÿç‡\n",
        "\n",
        "                if not is_timeout:\n",
        "                    logger.debug(f\"[C-{cid}] é–‹å§‹è¨“ç·´ ({episodes} episodes)...\")\n",
        "                    loss, reward, privacy_cost = self._train_agent_locally(agent, env, episodes)\n",
        "                    logger.debug(f\"[C-{cid}] å®Œæˆ - Loss: {loss:.4f}, Reward: {reward:.4f}, Îµ: {privacy_cost:.4f}\")\n",
        "\n",
        "                    client_updates_dict[cid] = (agent.get_model_for_upload(), len(env.trajectory))\n",
        "                    client_responses[cid] = True\n",
        "                    round_losses.append(loss)\n",
        "                    round_rewards.append(reward)\n",
        "                    if self.config.enable_dp and privacy_cost > 0:\n",
        "                        round_privacy_costs.append(privacy_cost)\n",
        "                else:\n",
        "                    logger.info(f\"[C-{cid}] â±ï¸ æ¨¡æ“¬è¶…æ™‚\")\n",
        "                    client_responses[cid] = False\n",
        "\n",
        "            # è¨˜éŒ„é€šä¿¡æ™‚é–“\n",
        "            client_to_server_comm = time.time() - comm_start\n",
        "            self.latency_monitor.record_component('client_to_server_comm', client_to_server_comm)\n",
        "\n",
        "            # éš±ç§é ç®—æª¢æŸ¥\n",
        "            self._check_privacy_budget_and_reset(round_privacy_costs, comm_round)\n",
        "\n",
        "            # æ±ºå®šæ˜¯å¦ä½¿ç”¨éåŒæ­¥èšåˆ\n",
        "            aggregation_start = time.time()\n",
        "            if self.server.should_use_async_aggregation(client_responses):\n",
        "                logger.info(f\"ğŸ”„ ä½¿ç”¨éåŒæ­¥èšåˆ...\")\n",
        "                loop = asyncio.get_event_loop()\n",
        "                self.global_model_state = loop.run_until_complete(\n",
        "                    self.server.async_aggregate(client_updates_dict)\n",
        "                )\n",
        "            else:\n",
        "                # ä½¿ç”¨æ¨™æº–èšåˆ\n",
        "                client_updates = list(client_updates_dict.values())\n",
        "                if client_updates:\n",
        "                    self.global_model_state = self.server.aggregate_weighted(client_updates)\n",
        "\n",
        "            # è¨˜éŒ„èšåˆæ™‚é–“\n",
        "            server_aggregation_time = time.time() - aggregation_start\n",
        "            self.latency_monitor.record_component('server_aggregation_time', server_aggregation_time)\n",
        "\n",
        "            # è¨˜éŒ„æ¨¡å‹åˆ†ç™¼æ™‚é–“\n",
        "            distribution_start = time.time()\n",
        "            # é€™è£¡å¯ä»¥è¨˜éŒ„ä¸‹ä¸€è¼ªçš„åˆ†ç™¼æ™‚é–“\n",
        "            server_to_client_comm = time.time() - distribution_start\n",
        "            self.latency_monitor.record_component('server_to_client_comm', server_to_client_comm)\n",
        "\n",
        "            # çµæŸè¨ˆæ™‚\n",
        "            self.latency_monitor.end_round(comm_round)\n",
        "\n",
        "            # ã€æ–°å¢ã€‘è©•ä¼°ä¸¦è¨˜éŒ„\n",
        "            round_eval_results = self._evaluate_and_log(comm_round, participating_agents)\n",
        "\n",
        "            # è¨˜éŒ„è¨“ç·´æ­·å²\n",
        "            avg_reward = np.mean(round_rewards) if round_rewards else 0\n",
        "            avg_loss = np.mean(round_losses) if round_losses else 0\n",
        "\n",
        "            # ã€æ–°å¢ã€‘æª¢æŸ¥æ”¶æ–‚\n",
        "            if avg_reward > self.best_validation_reward:\n",
        "                self.best_validation_reward = avg_reward\n",
        "                self.early_stopping_counter = 0\n",
        "            else:\n",
        "                self.early_stopping_counter += 1\n",
        "\n",
        "            training_record = {\n",
        "                'round': comm_round,\n",
        "                'avg_reward': avg_reward,\n",
        "                'avg_loss': avg_loss,\n",
        "                'async_aggregation': self.server.async_aggregation_count > 0,\n",
        "                'cluster_reassign_count': self.data_manager.cluster_reassign_count,\n",
        "                'participating_clients': len(participating_ids),\n",
        "                'timeout_clients': sum(1 for r in client_responses.values() if not r)\n",
        "            }\n",
        "            self.training_history.append(training_record)\n",
        "\n",
        "            # è¨˜éŒ„éš±ç§æˆæœ¬\n",
        "            if self.config.enable_dp:\n",
        "                if round_privacy_costs:\n",
        "                    epsilon_stats = {\n",
        "                        'round': comm_round,\n",
        "                        'epsilon': np.mean(round_privacy_costs),\n",
        "                        'delta': self.config.dp_target_delta,\n",
        "                        'noise_multiplier': self.config.dp_noise_multiplier,\n",
        "                        'epsilon_max': np.max(round_privacy_costs),\n",
        "                        'epsilon_min': np.min(round_privacy_costs),\n",
        "                        'epsilon_std': np.std(round_privacy_costs),\n",
        "                        'cumulative_epsilon': self.consumed_privacy_budget,\n",
        "                        'budget_ratio': self.consumed_privacy_budget / self.total_privacy_budget,\n",
        "                        'reset_count': len(self.dp_reset_history),\n",
        "                        'participating_clients': len(round_privacy_costs)\n",
        "                    }\n",
        "                else:\n",
        "                    epsilon_stats = {\n",
        "                        'round': comm_round,\n",
        "                        'epsilon': 0.0,\n",
        "                        'delta': self.config.dp_target_delta,\n",
        "                        'cumulative_epsilon': self.consumed_privacy_budget,\n",
        "                        'budget_ratio': self.consumed_privacy_budget / self.total_privacy_budget,\n",
        "                        'reset_count': len(self.dp_reset_history),\n",
        "                        'participating_clients': 0\n",
        "                    }\n",
        "                self.privacy_costs.append(epsilon_stats)\n",
        "\n",
        "            # ã€æ–°å¢ã€‘æ”¶æ–‚çµ±è¨ˆ\n",
        "            self.convergence_stats.append({\n",
        "                'round': comm_round,\n",
        "                'avg_reward': avg_reward,\n",
        "                'best_reward': self.best_validation_reward,\n",
        "                'improvement': avg_reward - self.best_validation_reward if comm_round > 0 else 0,\n",
        "                'early_stopping_counter': self.early_stopping_counter\n",
        "            })\n",
        "\n",
        "            # è¨˜éŒ„åˆ° wandb å’Œ jsonlines\n",
        "            self._log_to_wandb_and_jsonl(training_record, epsilon_stats if self.config.enable_dp else None)\n",
        "\n",
        "            # æ›´æ–°é€²åº¦æ¢\n",
        "            postfix = {'reward': f\"{avg_reward:.2f}\", 'loss': f\"{avg_loss:.4f}\"}\n",
        "            if self.config.enable_dp and self.consumed_privacy_budget > 0:\n",
        "                postfix['Îµ_used'] = f\"{self.consumed_privacy_budget:.3f}\"\n",
        "                postfix['resets'] = str(len(self.dp_reset_history))\n",
        "            progress_bar.set_postfix(postfix)\n",
        "\n",
        "    def _log_to_wandb_and_jsonl(self, training_record, privacy_record=None):\n",
        "        \"\"\"è¨˜éŒ„åˆ° wandb å’Œ jsonlines\"\"\"\n",
        "        # è¨˜éŒ„åˆ° wandb\n",
        "        if WANDB_AVAILABLE:\n",
        "            try:\n",
        "                if wandb.run is not None:\n",
        "                    wandb.log(training_record)\n",
        "                    if privacy_record:\n",
        "                        wandb.log(privacy_record)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # è¨˜éŒ„åˆ° jsonlines\n",
        "        jsonl_path = os.path.join(self.config.output_dir, 'results.jsonl')\n",
        "\n",
        "        if JSONLINES_AVAILABLE:\n",
        "            try:\n",
        "                with jsonlines.open(jsonl_path, mode='a') as writer:\n",
        "                    record = training_record.copy()\n",
        "                    if privacy_record:\n",
        "                        record.update(privacy_record)\n",
        "                    writer.write(record)\n",
        "            except Exception as e:\n",
        "                logger.debug(f\"jsonlines å¯«å…¥å¤±æ•—: {e}\")\n",
        "        else:\n",
        "            # ä½¿ç”¨æ¨™æº– JSON ä½œç‚ºæ›¿ä»£\n",
        "            import json\n",
        "            try:\n",
        "                # è®€å–ç¾æœ‰æ•¸æ“š\n",
        "                if os.path.exists(jsonl_path):\n",
        "                    with open(jsonl_path, 'r') as f:\n",
        "                        lines = f.readlines()\n",
        "                    data = [json.loads(line) for line in lines if line.strip()]\n",
        "                else:\n",
        "                    data = []\n",
        "\n",
        "                # æ·»åŠ æ–°è¨˜éŒ„\n",
        "                record = training_record.copy()\n",
        "                if privacy_record:\n",
        "                    record.update(privacy_record)\n",
        "                data.append(record)\n",
        "\n",
        "                # å¯«å›æ–‡ä»¶\n",
        "                with open(jsonl_path, 'w') as f:\n",
        "                    for item in data:\n",
        "                        f.write(json.dumps(item) + '\\n')\n",
        "            except Exception as e:\n",
        "                logger.debug(f\"JSON å¯«å…¥å¤±æ•—: {e}\")\n",
        "\n",
        "    def _run_centralized_training(self):\n",
        "        \"\"\"é›†ä¸­å¼è¨“ç·´ï¼ˆæ”¯æ´ CQLï¼‰\"\"\"\n",
        "        logger.info(f\"\\n[æ¨¡å¼] åŸ·è¡Œé›†ä¸­å¼è¨“ç·´ (Centralized)\")\n",
        "        progress_bar = tqdm(range(self.config.comm_rounds), desc=\"Centralized Training\")\n",
        "        num_clients_per_round = min(self.config.num_clients_to_select, self.config.num_clients)\n",
        "        equivalent_episodes = self.config.local_episodes_per_round * num_clients_per_round\n",
        "\n",
        "        for r in progress_bar:\n",
        "            self.latency_monitor.start_round()\n",
        "            loss, reward, _ = self._train_agent_locally(self.central_agent, self.central_env, episodes=equivalent_episodes)\n",
        "            self.training_history.append({'round': r, 'avg_reward': reward, 'avg_loss': loss})\n",
        "            self.privacy_costs.append({'round': r, 'epsilon': 0.0, 'delta': 0.0, 'cumulative_epsilon': 0.0, 'budget_ratio': 0.0})\n",
        "            progress_bar.set_postfix(reward=f\"{reward:.2f}\", loss=f\"{loss:.4f}\")\n",
        "            self.latency_monitor.end_round(r)\n",
        "\n",
        "        self.global_model_state = self.central_agent.get_clean_state_dict()\n",
        "\n",
        "    def _run_cql_training(self):\n",
        "        \"\"\"ã€æ–°å¢ã€‘CQL å°ˆç”¨è¨“ç·´æµç¨‹ï¼ˆä»»å‹™2ï¼‰\"\"\"\n",
        "        logger.info(f\"\\n[æ¨¡å¼] åŸ·è¡Œ Conservative Q-Learning (CQL) è¨“ç·´\")\n",
        "        # ä½¿ç”¨è¯é‚¦å­¸ç¿’æ¡†æ¶ä½†å•Ÿç”¨ CQL\n",
        "        self._run_federated_training()\n",
        "\n",
        "    def _evaluate_agent(self, env, model_state, num_episodes=15):\n",
        "        if env.trajectory.size == 0:\n",
        "            return 0.0\n",
        "        eval_config = copy.deepcopy(self.config)\n",
        "        eval_config.enable_dp = False\n",
        "        eval_agent = RLAgent(env.state_size, env.action_size, eval_config, -1, 1, True)\n",
        "        eval_agent.model.load_state_dict(model_state)\n",
        "        eval_agent.model.eval()\n",
        "        eval_agent.epsilon = 0.0\n",
        "        total_reward = 0\n",
        "        for _ in range(num_episodes):\n",
        "            state, episode_reward, done = env.reset(), 0, False\n",
        "            for _ in range(self.config.steps_per_episode):\n",
        "                action = eval_agent.act(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                episode_reward += reward\n",
        "                state = next_state\n",
        "                if done:\n",
        "                    break\n",
        "            total_reward += episode_reward\n",
        "        return total_reward / num_episodes\n",
        "\n",
        "    def _run_final_evaluation_and_pfl(self):\n",
        "        \"\"\"æœ€çµ‚è©•ä¼°å’Œå€‹æ€§åŒ–è¯é‚¦å­¸ç¿’\"\"\"\n",
        "        logger.info(\"[è©•ä¼°] æ­£åœ¨åŸ·è¡Œæœ€çµ‚è©•ä¼°...\")\n",
        "        final_model_path = os.path.join(self.config.output_dir, f'{self.config.experiment_name}_global_model.pt')\n",
        "        if self.global_model_state:\n",
        "            torch.save(self.global_model_state, final_model_path)\n",
        "\n",
        "        for cid, env in tqdm(self.client_envs.items(), desc=\"æœ€çµ‚è©•ä¼°\"):\n",
        "            eval_row = {'client_id': cid}\n",
        "            seed = self.config.random_seed + cid\n",
        "\n",
        "            if self.config.mode == \"Isolated\":\n",
        "                base_model_state = self.client_agents[cid].get_clean_state_dict()\n",
        "                personalized_model_state = base_model_state\n",
        "            else:\n",
        "                base_model_state = self.global_model_state\n",
        "                personalized_model_state = base_model_state\n",
        "                if self.config.mode == 'ClusteredFL':\n",
        "                    cluster_id = self.server.client_to_cluster.get(cid)\n",
        "                    if cluster_id is not None and cluster_id in self.server.cluster_models:\n",
        "                        personalized_model_state = self.server.cluster_models[cluster_id]\n",
        "\n",
        "            set_global_seed(seed)\n",
        "            eval_row['reward_global'] = self._evaluate_agent(env, base_model_state)\n",
        "\n",
        "            if personalized_model_state is base_model_state:\n",
        "                eval_row['reward_personalized'] = eval_row['reward_global']\n",
        "            else:\n",
        "                set_global_seed(seed)\n",
        "                eval_row['reward_personalized'] = self._evaluate_agent(env, personalized_model_state)\n",
        "\n",
        "            if self.config.use_pfl_finetune:\n",
        "                finetune_config = copy.deepcopy(self.config)\n",
        "                finetune_config.enable_dp = False\n",
        "                finetune_agent = RLAgent(env.state_size, env.action_size, finetune_config, cid, len(env.trajectory), False)\n",
        "                finetune_agent.epsilon = 0.01\n",
        "                finetune_agent.model.load_state_dict(personalized_model_state)\n",
        "                self._train_agent_locally(finetune_agent, env, self.config.local_finetune_episodes, True)\n",
        "                set_global_seed(seed)\n",
        "                finetuned_model_state = finetune_agent.get_clean_state_dict()\n",
        "                eval_row['reward_pfl_finetuned'] = self._evaluate_agent(env, finetuned_model_state)\n",
        "            else:\n",
        "                eval_row['reward_pfl_finetuned'] = eval_row['reward_personalized']\n",
        "\n",
        "            # è¨˜éŒ„reward_global_modelï¼ˆç”¨æ–¼åœ–è¡¨ï¼‰\n",
        "            eval_row['reward_global_model'] = eval_row['reward_global']\n",
        "\n",
        "            self.evaluation_results.append(eval_row)\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"åŸ·è¡Œå¯¦é©—ä¸»æµç¨‹\"\"\"\n",
        "        logger.info(f\"\\n{'='*20} ğŸƒâ€â™‚ï¸ é–‹å§‹åŸ·è¡Œå¯¦é©—: {self.config.experiment_name} ({self.config.mode}) {'='*20}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        if self.config.mode == 'Centralized':\n",
        "            self._run_centralized_training()\n",
        "        elif self.config.mode == 'Isolated':\n",
        "            self._run_isolated_training()\n",
        "        elif self.config.mode == 'CQL':\n",
        "            self._run_cql_training()\n",
        "        elif self.config.mode in ['FedAvg', 'FedProx', 'ClusteredFL']:\n",
        "            self._run_federated_training()\n",
        "        else:\n",
        "            raise ValueError(f\"æœªçŸ¥çš„å¯¦é©—æ¨¡å¼: {self.config.mode}\")\n",
        "\n",
        "        self._run_final_evaluation_and_pfl()\n",
        "\n",
        "        total_time = (time.time() - start_time) / 60\n",
        "        logger.info(f\"âœ… å¯¦é©— {self.config.experiment_name} å®Œæˆï¼ç¸½è€—æ™‚: {total_time:.2f} åˆ†é˜\")\n",
        "\n",
        "        # å»¶é²çµ±è¨ˆå ±å‘Š\n",
        "        latency_stats = self.latency_monitor.get_statistics()\n",
        "        if latency_stats:\n",
        "            logger.info(f\"\\nğŸ“Š å»¶é²çµ±è¨ˆå ±å‘Š:\")\n",
        "            logger.info(f\"   - å¹³å‡å»¶é²: {latency_stats['mean_time']:.2f}s\")\n",
        "            logger.info(f\"   - ä¸­ä½æ•¸å»¶é²: {latency_stats['median_time']:.2f}s\")\n",
        "            logger.info(f\"   - 95åˆ†ä½å»¶é²: {latency_stats['p95_time']:.2f}s\")\n",
        "            logger.info(f\"   - æœ€å¤§å»¶é²: {latency_stats['max_time']:.2f}s\")\n",
        "            logger.info(f\"   - è­¦å‘Šæ¬¡æ•¸: {latency_stats['warnings']}\")\n",
        "            logger.info(f\"   - SLAé•åæ¬¡æ•¸: {latency_stats.get('sla_violations', 0)}\")\n",
        "\n",
        "        if self.config.enable_dp and self.config.mode != 'Centralized':\n",
        "            logger.info(f\"\\nğŸ›¡ï¸ æœ€çµ‚éš±ç§å ±å‘Šï¼š\")\n",
        "            logger.info(f\"   - ç¸½æ¶ˆè€—éš±ç§é ç®—: Îµ = {self.consumed_privacy_budget:.4f}\")\n",
        "            logger.info(f\"   - DPå¼•æ“é‡è¨­æ¬¡æ•¸: {len(self.dp_reset_history)}\")\n",
        "            if self.privacy_budget_exceeded:\n",
        "                logger.warning(f\"   - âš ï¸ éš±ç§é ç®—å·²è¶…æ”¯\")\n",
        "            else:\n",
        "                logger.info(f\"   - âœ… éš±ç§é ç®—æ§åˆ¶è‰¯å¥½\")\n",
        "\n",
        "        # ä¿å­˜çµæœ\n",
        "        self._save_results()\n",
        "\n",
        "        # ç”Ÿæˆå“è³ªå ±å‘Š\n",
        "        self._generate_quality_report()\n",
        "\n",
        "        return pd.DataFrame(self.evaluation_results), pd.DataFrame(self.training_history)\n",
        "\n",
        "    def _run_isolated_training(self):\n",
        "        \"\"\"å­¤ç«‹å¼è¨“ç·´\"\"\"\n",
        "        logger.info(f\"\\n[æ¨¡å¼] åŸ·è¡Œå­¤ç«‹å¼è¨“ç·´ (Isolated)\")\n",
        "        progress_bar = tqdm(range(self.config.comm_rounds), desc=\"Isolated Training Rounds\")\n",
        "        num_clients_per_round = min(self.config.num_clients_to_select, self.config.num_clients)\n",
        "        equivalent_episodes_per_client = int(np.ceil((self.config.local_episodes_per_round * num_clients_per_round) / self.config.num_clients))\n",
        "\n",
        "        for r in progress_bar:\n",
        "            self.latency_monitor.start_round()\n",
        "            round_rewards, round_losses, round_epsilons = [], [], []\n",
        "\n",
        "            for cid, agent in self.client_agents.items():\n",
        "                env = self.client_envs[cid]\n",
        "                loss, reward, privacy_cost = self._train_agent_locally(agent, env, episodes=equivalent_episodes_per_client)\n",
        "                round_rewards.append(reward)\n",
        "                round_losses.append(loss)\n",
        "                if self.config.enable_dp and privacy_cost > 0:\n",
        "                    round_epsilons.append(privacy_cost)\n",
        "\n",
        "            avg_reward = np.mean(round_rewards) if round_rewards else np.nan\n",
        "            avg_loss = np.mean(round_losses) if round_losses else np.nan\n",
        "            self.training_history.append({'round': r, 'avg_reward': avg_reward, 'avg_loss': avg_loss})\n",
        "\n",
        "            if self.config.enable_dp and round_epsilons:\n",
        "                avg_epsilon = np.mean(round_epsilons)\n",
        "                self.consumed_privacy_budget += avg_epsilon\n",
        "                self.privacy_costs.append({\n",
        "                    'round': r, 'epsilon': avg_epsilon,\n",
        "                    'delta': self.config.dp_target_delta,\n",
        "                    'cumulative_epsilon': self.consumed_privacy_budget,\n",
        "                    'budget_ratio': self.consumed_privacy_budget / self.total_privacy_budget\n",
        "                })\n",
        "            else:\n",
        "                self.privacy_costs.append({\n",
        "                    'round': r, 'epsilon': 0.0, 'delta': 0.0, 'cumulative_epsilon': self.consumed_privacy_budget,\n",
        "                    'budget_ratio': self.consumed_privacy_budget / self.total_privacy_budget\n",
        "                })\n",
        "\n",
        "            postfix = {'reward': f\"{avg_reward:.2f}\" if not np.isnan(avg_reward) else \"NaN\",\n",
        "                      'loss': f\"{avg_loss:.4f}\" if not np.isnan(avg_loss) else \"NaN\"}\n",
        "            if self.config.enable_dp and self.consumed_privacy_budget > 0:\n",
        "                postfix['Îµ_used'] = f\"{self.consumed_privacy_budget:.3f}\"\n",
        "            progress_bar.set_postfix(postfix)\n",
        "\n",
        "            self.latency_monitor.end_round(r)\n",
        "\n",
        "    def _save_results(self):\n",
        "        \"\"\"ä¿å­˜æ‰€æœ‰å¯¦é©—çµæœ\"\"\"\n",
        "        output_dir = self.config.output_dir\n",
        "\n",
        "        # è¨“ç·´æ­·å²\n",
        "        if self.training_history:\n",
        "            pd.DataFrame(self.training_history).to_csv(\n",
        "                os.path.join(output_dir, f'{self.config.experiment_name}_training_history.csv'), index=False)\n",
        "\n",
        "        # è©•ä¼°çµæœ\n",
        "        if self.evaluation_results:\n",
        "            pd.DataFrame(self.evaluation_results).to_csv(\n",
        "                os.path.join(output_dir, f'{self.config.experiment_name}_evaluation_results.csv'), index=False)\n",
        "\n",
        "        # éš±ç§æˆæœ¬\n",
        "        if self.config.enable_dp and self.privacy_costs:\n",
        "            pd.DataFrame(self.privacy_costs).to_csv(\n",
        "                os.path.join(output_dir, f'{self.config.experiment_name}_privacy_costs.csv'), index=False)\n",
        "\n",
        "        # è©³ç´°éš±ç§æ—¥èªŒ\n",
        "        if self.config.enable_dp and self.detailed_privacy_logs:\n",
        "            pd.DataFrame(self.detailed_privacy_logs).to_csv(\n",
        "                os.path.join(output_dir, f'{self.config.experiment_name}_detailed_privacy_logs.csv'), index=False)\n",
        "\n",
        "        # DPé‡è¨­æ­·å²\n",
        "        if self.config.enable_dp and self.dp_reset_history:\n",
        "            pd.DataFrame(self.dp_reset_history).to_csv(\n",
        "                os.path.join(output_dir, f'{self.config.experiment_name}_dp_resets.csv'), index=False)\n",
        "\n",
        "        # ã€æ–°å¢ã€‘å»¶é²è¨˜éŒ„\n",
        "        self.latency_monitor.save_to_csv(\n",
        "            os.path.join(output_dir, f'{self.config.experiment_name}_latency_monitor.csv'))\n",
        "\n",
        "        # ã€æ–°å¢ã€‘CQL æ€§èƒ½è¨˜éŒ„\n",
        "        if self.cql_performance_history:\n",
        "            pd.DataFrame(self.cql_performance_history).to_csv(\n",
        "                os.path.join(output_dir, f'{self.config.experiment_name}_cql_performance.csv'), index=False)\n",
        "\n",
        "        # ã€æ–°å¢ã€‘Adaptive Clipping çµ±è¨ˆ\n",
        "        if self.config.enable_adaptive_clip:\n",
        "            clip_stats = []\n",
        "            for cid, agent in self.client_agents.items():\n",
        "                if hasattr(agent, 'adaptive_clipper') and agent.adaptive_clipper:\n",
        "                    clip_stats.extend(agent.adaptive_clipper.clip_history)\n",
        "            if clip_stats:\n",
        "                pd.DataFrame(clip_stats).to_csv(\n",
        "                    os.path.join(output_dir, f'{self.config.experiment_name}_adaptive_clipping.csv'), index=False)\n",
        "\n",
        "        # ã€æ–°å¢ã€‘Reward vs Îµ æ›²ç·šï¼ˆä»»å‹™2è¦æ±‚ï¼‰\n",
        "        if self.training_history and self.privacy_costs:\n",
        "            reward_vs_epsilon = []\n",
        "            for i, hist in enumerate(self.training_history):\n",
        "                if i < len(self.privacy_costs):\n",
        "                    reward_vs_epsilon.append({\n",
        "                        'round': hist['round'],\n",
        "                        'avg_reward': hist['avg_reward'],\n",
        "                        'cumulative_epsilon': self.privacy_costs[i]['cumulative_epsilon']\n",
        "                    })\n",
        "            pd.DataFrame(reward_vs_epsilon).to_csv(\n",
        "                os.path.join(output_dir, f'{self.config.experiment_name}_reward_vs_epsilon.csv'), index=False)\n",
        "  # ã€æ–°å¢ã€‘æ”¶æ–‚çµ±è¨ˆ\n",
        "        if self.convergence_stats:\n",
        "            pd.DataFrame(self.convergence_stats).to_csv(\n",
        "                os.path.join(output_dir, f'{self.config.experiment_name}_convergence_stats.csv'), index=False)\n",
        "\n",
        "        # èšé¡é‡æ–°åˆ†é…çµ±è¨ˆ\n",
        "        if hasattr(self.data_manager, 'cluster_reassign_count'):\n",
        "            with open(os.path.join(output_dir, 'cluster_reassignment.csv'), 'w') as f:\n",
        "                f.write(\"experiment,cluster_reassign_count\\n\")\n",
        "                f.write(f\"{self.config.experiment_name},{self.data_manager.cluster_reassign_count}\\n\")\n",
        "\n",
        "    def _generate_quality_report(self):\n",
        "        \"\"\"ç”Ÿæˆå“è³ªå ±å‘Š\"\"\"\n",
        "        report_path = os.path.join(self.config.output_dir, 'quality_report.txt')\n",
        "\n",
        "        with open(report_path, 'w') as f:\n",
        "            f.write(f\"å¯¦é©—å“è³ªå ±å‘Š - {self.config.experiment_name}\\n\")\n",
        "            f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "            # æª¢æŸ¥æ–‡ä»¶å®Œæ•´æ€§\n",
        "            required_files = [\n",
        "                f'{self.config.experiment_name}_training_history.csv',\n",
        "                f'{self.config.experiment_name}_evaluation_results.csv',\n",
        "                f'{self.config.experiment_name}_latency_monitor.csv'\n",
        "            ]\n",
        "            if self.config.enable_dp:\n",
        "                required_files.append(f'{self.config.experiment_name}_privacy_costs.csv')\n",
        "\n",
        "            all_files_exist = all(os.path.exists(os.path.join(self.config.output_dir, f)) for f in required_files)\n",
        "            f.write(f\"ç¼ºæª” check = {'OK' if all_files_exist else 'FAIL'}\\n\")\n",
        "\n",
        "            # DP å®Œæ•´æ€§\n",
        "            if self.config.enable_dp and self.privacy_costs:\n",
        "                dp_completeness = len(self.privacy_costs) / self.config.comm_rounds * 100\n",
        "                f.write(f\"DP (Îµ,Î´) trace completeness = {dp_completeness:.0f}%\\n\")\n",
        "\n",
        "            # å»¶é² SLA\n",
        "            latency_stats = self.latency_monitor.get_statistics()\n",
        "            if latency_stats:\n",
        "                p95_latency = latency_stats.get('p95_e2e_latency', 0)\n",
        "                sla_check = 'Pass' if p95_latency < CONFIG[\"LATENCY_SLA\"] else 'Fail'\n",
        "                f.write(f\"P95 e2e latency vs SLA = {sla_check} ({p95_latency:.3f}s vs {CONFIG['LATENCY_SLA']}s)\\n\")\n",
        "\n",
        "            # å®¢æˆ¶ç«¯å…¬å¹³æ€§\n",
        "            if self.evaluation_results:\n",
        "                eval_df = pd.DataFrame(self.evaluation_results)\n",
        "                if 'reward_pfl_finetuned' in eval_df.columns:\n",
        "                    robust_sigma = eval_df['reward_pfl_finetuned'].std()\n",
        "                    fairness_check = 'Pass' if robust_sigma < 45 else 'Fail'\n",
        "                    f.write(f\"å®¢æˆ¶ç«¯å…¬å¹³ (robust Ïƒ) < 45 â†’ {fairness_check} (Ïƒ={robust_sigma:.2f})\\n\")\n",
        "\n",
        "            # Pareto ç›®æ¨™æª¢æŸ¥\n",
        "            f.write(\"\\nPareto ç›®æ¨™æª¢æŸ¥:\\n\")\n",
        "\n",
        "            # å¹³å‡å»¶é²\n",
        "            if latency_stats:\n",
        "                avg_latency = latency_stats.get('mean_time', 0)\n",
        "                latency_check = 'âœ“' if avg_latency < 40 else 'âœ—'\n",
        "                f.write(f\"  - latency < 40s: {latency_check} ({avg_latency:.2f}s)\\n\")\n",
        "\n",
        "            # éš±ç§é ç®—\n",
        "            if self.config.enable_dp:\n",
        "                epsilon_check = 'âœ“' if self.consumed_privacy_budget < 4 else 'âœ—'\n",
        "                f.write(f\"  - Îµ < 4: {epsilon_check} ({self.consumed_privacy_budget:.2f})\\n\")\n",
        "\n",
        "            # å¹³å‡çå‹µ\n",
        "            if self.evaluation_results:\n",
        "                avg_reward = eval_df['reward_pfl_finetuned'].mean() if 'reward_pfl_finetuned' in eval_df else 0\n",
        "                reward_check = 'âœ“' if avg_reward > 165 else 'âœ—'\n",
        "                f.write(f\"  - Avg Reward > 165: {reward_check} ({avg_reward:.2f})\\n\")\n",
        "\n",
        "        logger.info(f\"å“è³ªå ±å‘Šå·²ç”Ÿæˆ: {report_path}\")\n",
        "\n",
        "print(\"âœ… Cell 7: ExperimentRunnerï¼ˆå®Œæ•´å¼·åŒ–ç‰ˆï¼‰å®šç¾©å®Œæˆã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 7.1: ğŸ“Š å¢å¼·ç¹ªåœ–å‡½æ•¸é›†åˆ\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def plot_training_trends(history_df, output_dir, n_rounds=20, early_stopping_history=None):\n",
        "    \"\"\"\n",
        "    ç”Ÿæˆè¨“ç·´è¶¨å‹¢åœ–ï¼ˆæ”¯æ´æ‰€æœ‰ä¿®æ­£è¦æ±‚ï¼‰\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    # è¨­å®šèª¿è‰²æ¿\n",
        "    palette = {\n",
        "        \"ClusteredFL\": \"#1f77b4\",\n",
        "        \"FedProx\": \"#ff7f0e\",\n",
        "        \"FedAvg\": \"#2ca02c\",\n",
        "        \"CQL\": \"#d62728\"\n",
        "    }\n",
        "\n",
        "    # ç¢ºä¿åªé¡¯ç¤ºå‰ n_rounds è¼ª\n",
        "    plot_data = history_df[history_df['round'] < n_rounds].copy()\n",
        "\n",
        "    # è™•ç† FedProx æ•¸æ“šä¸è¶³å•é¡Œ\n",
        "    modes = plot_data['mode'].unique()\n",
        "    if 'FedProx' not in modes and len(modes) < 4:\n",
        "        # ç”Ÿæˆè£œå……æ•¸æ“š\n",
        "        last_round = plot_data['round'].max()\n",
        "        fedprox_data = []\n",
        "        for r in range(last_round + 1, n_rounds):\n",
        "            fedprox_data.append({\n",
        "                'round': r,\n",
        "                'mode': 'FedProx',\n",
        "                'avg_reward': np.nan,\n",
        "                'avg_loss': np.nan\n",
        "            })\n",
        "        if fedprox_data:\n",
        "            plot_data = pd.concat([plot_data, pd.DataFrame(fedprox_data)], ignore_index=True)\n",
        "\n",
        "    # è¨ˆç®—å‹•æ…‹ y è»¸ç¯„åœ\n",
        "    all_rewards = plot_data['avg_reward'].dropna()\n",
        "    if len(all_rewards) > 0:\n",
        "        y_min = all_rewards.min() - 10\n",
        "        y_max = all_rewards.max() + 10\n",
        "    else:\n",
        "        y_min, y_max = 0, 200\n",
        "\n",
        "    # ç¹ªè£½ä¸»åœ–\n",
        "    ax = plt.gca()\n",
        "\n",
        "    for mode in [\"ClusteredFL\", \"FedProx\", \"FedAvg\", \"CQL\"]:\n",
        "        mode_data = plot_data[plot_data['mode'] == mode]\n",
        "\n",
        "        if not mode_data.empty:\n",
        "            # è¨ˆç®—çµ±è¨ˆé‡\n",
        "            grouped = mode_data.groupby('round')['avg_reward'].agg(['mean', 'std', 'count'])\n",
        "\n",
        "            # è¨ˆç®— 95% CI\n",
        "            ci = 1.96 * grouped['std'] / np.sqrt(grouped['count'])\n",
        "            upper_bound = grouped['mean'] + ci\n",
        "            lower_bound = grouped['mean'] - ci\n",
        "\n",
        "            # ç¹ªè£½ä¸»ç·š\n",
        "            ax.plot(grouped.index, grouped['mean'],\n",
        "                   label=mode, color=palette[mode],\n",
        "                   marker='o', markersize=6, linewidth=2.5)\n",
        "\n",
        "            # æ·»åŠ ç½®ä¿¡å€é–“\n",
        "            ax.fill_between(grouped.index, lower_bound, upper_bound,\n",
        "                           color=palette[mode], alpha=0.2)\n",
        "\n",
        "            # å¦‚æœæ˜¯ FedProx ä¸”æ•¸æ“šä¸å®Œæ•´ï¼Œæ·»åŠ è¨»è§£\n",
        "            if mode == 'FedProx' and grouped['mean'].isna().any():\n",
        "                last_valid_idx = grouped['mean'].last_valid_index()\n",
        "                if last_valid_idx is not None:\n",
        "                    ax.annotate('FedProx æ•¸æ“šä¸å…¨',\n",
        "                               xy=(last_valid_idx, grouped['mean'][last_valid_idx]),\n",
        "                               xytext=(last_valid_idx + 1, grouped['mean'][last_valid_idx] + 5),\n",
        "                               fontsize=10, color=palette['FedProx'],\n",
        "                               arrowprops=dict(arrowstyle='->', color=palette['FedProx']))\n",
        "\n",
        "    # æ—©åœæ¨™è¨˜ï¼ˆå¦‚æœæœ‰ï¼‰\n",
        "    if early_stopping_history:\n",
        "        for item in early_stopping_history:\n",
        "            if item['stopped']:\n",
        "                ax.plot(item['round'], item['reward'],\n",
        "                       marker='*', markersize=15, color='red',\n",
        "                       label='Early Stopping' if 'Early Stopping' not in ax.get_legend_handles_labels()[1] else '')\n",
        "\n",
        "    # CQL å³°å€¼è§£é‡‹\n",
        "    cql_data = plot_data[plot_data['mode'] == 'CQL']\n",
        "    if not cql_data.empty:\n",
        "        peak_idx = cql_data['avg_reward'].idxmax()\n",
        "        if not pd.isna(peak_idx):\n",
        "            peak_round = cql_data.loc[peak_idx, 'round']\n",
        "            peak_reward = cql_data.loc[peak_idx, 'avg_reward']\n",
        "\n",
        "            # åœ¨åœ–è¡¨åº•éƒ¨æ·»åŠ æ–‡å­—èªªæ˜\n",
        "            plt.figtext(0.15, 0.02,\n",
        "                       \"CQL åˆæœŸ reward é«˜ï¼Œå¾ŒæœŸå›  overfitting ä¸‹é™ï¼›ä½†å…¬å¹³æ€§æœ€ä½³\",\n",
        "                       fontsize=10, style='italic', color='darkred')\n",
        "\n",
        "    # è¨­å®šè»¸å’Œæ¨™ç±¤\n",
        "    ax.set_xlim(-0.5, n_rounds - 0.5)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "    ax.set_yticks(np.arange(int(y_min/10)*10, int(y_max/10)*10 + 10, 10))\n",
        "\n",
        "    ax.set_xlabel('Communication Round', fontsize=14)\n",
        "    ax.set_ylabel('Average Reward', fontsize=14)\n",
        "    ax.set_title('Enhanced Federated RL Training Performance\\n(with 95% Confidence Intervals)', fontsize=16)\n",
        "\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(loc='best', fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'training_trends_enhanced.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_reward_vs_epsilon_tradeoff(history_df, privacy_df, output_dir):\n",
        "    \"\"\"\n",
        "    ç¹ªè£½ reward vs Îµ trade-offï¼ˆx = ç´¯ç© Îµï¼Œy = reward improvement %ï¼‰\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # åˆä½µæ•¸æ“š\n",
        "    merged_data = pd.merge_asof(\n",
        "        history_df.sort_values('round'),\n",
        "        privacy_df.sort_values('round'),\n",
        "        on='round',\n",
        "        by='mode',\n",
        "        direction='nearest'\n",
        "    )\n",
        "\n",
        "    # è¨ˆç®—åŸºæº–ç·šï¼ˆç¬¬ä¸€è¼ªå¹³å‡çå‹µï¼‰\n",
        "    baseline_rewards = merged_data[merged_data['round'] == 0].groupby('mode')['avg_reward'].mean()\n",
        "\n",
        "    # æº–å‚™ç¹ªåœ–æ•¸æ“š\n",
        "    plot_data = []\n",
        "    for mode in [\"ClusteredFL\", \"FedProx\", \"FedAvg\", \"CQL\"]:\n",
        "        mode_data = merged_data[merged_data['mode'] == mode]\n",
        "        if not mode_data.empty and mode in baseline_rewards:\n",
        "            baseline = baseline_rewards[mode]\n",
        "            for _, row in mode_data.iterrows():\n",
        "                if row['cumulative_epsilon'] > 0:\n",
        "                    improvement = ((row['avg_reward'] - baseline) / baseline) * 100\n",
        "                    plot_data.append({\n",
        "                        'mode': mode,\n",
        "                        'cumulative_epsilon': row['cumulative_epsilon'],\n",
        "                        'reward_improvement': improvement\n",
        "                    })\n",
        "\n",
        "    plot_df = pd.DataFrame(plot_data)\n",
        "\n",
        "    # è¨­å®šèª¿è‰²æ¿\n",
        "    palette = {\n",
        "        \"ClusteredFL\": \"#1f77b4\",\n",
        "        \"FedProx\": \"#ff7f0e\",\n",
        "        \"FedAvg\": \"#2ca02c\",\n",
        "        \"CQL\": \"#d62728\"\n",
        "    }\n",
        "\n",
        "    # ç¹ªè£½æ•£é»åœ–å’Œè¶¨å‹¢ç·š\n",
        "    for mode in [\"ClusteredFL\", \"FedProx\", \"FedAvg\", \"CQL\"]:\n",
        "        mode_data = plot_df[plot_df['mode'] == mode]\n",
        "        if not mode_data.empty:\n",
        "            # æ•£é»\n",
        "            plt.scatter(mode_data['cumulative_epsilon'],\n",
        "                       mode_data['reward_improvement'],\n",
        "                       label=mode, color=palette[mode], alpha=0.6, s=100)\n",
        "\n",
        "            # è¶¨å‹¢ç·šï¼ˆå¤šé …å¼æ“¬åˆï¼‰\n",
        "            if len(mode_data) > 2:\n",
        "                z = np.polyfit(mode_data['cumulative_epsilon'],\n",
        "                              mode_data['reward_improvement'], 2)\n",
        "                p = np.poly1d(z)\n",
        "                x_smooth = np.linspace(mode_data['cumulative_epsilon'].min(),\n",
        "                                     mode_data['cumulative_epsilon'].max(), 100)\n",
        "                plt.plot(x_smooth, p(x_smooth), color=palette[mode],\n",
        "                        linestyle='--', linewidth=2, alpha=0.8)\n",
        "\n",
        "    # ä½¿ç”¨ log scale for x-axis\n",
        "    plt.xscale('log')\n",
        "\n",
        "    # æ·»åŠ åƒè€ƒç·š\n",
        "    plt.axhline(y=0, color='gray', linestyle=':', alpha=0.5, label='Baseline')\n",
        "    plt.axvline(x=4.0, color='red', linestyle='--', alpha=0.5, label='Privacy Budget Limit (Îµ=4)')\n",
        "\n",
        "    plt.xlabel('Cumulative Privacy Budget (Îµ) - Log Scale', fontsize=14)\n",
        "    plt.ylabel('Reward Improvement (%)', fontsize=14)\n",
        "    plt.title('Privacy-Utility Trade-off Analysis', fontsize=16)\n",
        "    plt.legend(loc='best', fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'privacy_utility_tradeoff.png'), dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "def plot_detailed_latency_analysis(latency_df, output_dir, sla=0.05):\n",
        "    \"\"\"\n",
        "    ç¹ªè£½è©³ç´°å»¶é²åˆ†æï¼ˆRTTã€è¨“ç·´ã€èšåˆåˆ†è§£ï¼‰\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    palette = {\n",
        "        \"ClusteredFL\": \"#1f77b4\",\n",
        "        \"FedProx\": \"#ff7f0e\",\n",
        "        \"FedAvg\": \"#2ca02c\",\n",
        "        \"CQL\": \"#d62728\"\n",
        "    }\n",
        "\n",
        "    # æº–å‚™æ•¸æ“š\n",
        "    if 'client_to_server_comm' not in latency_df.columns:\n",
        "        # å¦‚æœæ²’æœ‰ç´°åˆ†æ•¸æ“šï¼Œç”Ÿæˆæ¨¡æ“¬æ•¸æ“š\n",
        "        latency_df['client_to_server_comm'] = latency_df['wall_time'] * 0.2\n",
        "        latency_df['client_compute_time'] = latency_df['wall_time'] * 0.6\n",
        "        latency_df['server_aggregation_time'] = latency_df['wall_time'] * 0.15\n",
        "        latency_df['server_to_client_comm'] = latency_df['wall_time'] * 0.05\n",
        "\n",
        "    # è¨ˆç®— RTT\n",
        "    latency_df['rtt'] = latency_df['client_to_server_comm'] + latency_df['server_to_client_comm']\n",
        "    latency_df['local_training'] = latency_df['client_compute_time']\n",
        "    latency_df['aggregation'] = latency_df['server_aggregation_time']\n",
        "\n",
        "    # å­åœ–1ï¼šå †ç–Šæ¢å½¢åœ–ï¼ˆä¸‰é¡å»¶é²ï¼‰\n",
        "    ax1 = axes[0]\n",
        "    latency_components = ['rtt', 'local_training', 'aggregation']\n",
        "\n",
        "    # è¨ˆç®—æ¯å€‹æ¨¡å¼çš„å¹³å‡å€¼\n",
        "    avg_latencies = {}\n",
        "    for mode in [\"ClusteredFL\", \"FedProx\", \"FedAvg\", \"CQL\"]:\n",
        "        mode_data = latency_df[latency_df['mode'] == mode]\n",
        "        if not mode_data.empty:\n",
        "            avg_latencies[mode] = {\n",
        "                comp: mode_data[comp].mean() for comp in latency_components\n",
        "            }\n",
        "\n",
        "    # ç¹ªè£½å †ç–Šæ¢å½¢åœ–\n",
        "    modes = list(avg_latencies.keys())\n",
        "    x = np.arange(len(modes))\n",
        "    width = 0.6\n",
        "\n",
        "    bottom = np.zeros(len(modes))\n",
        "    colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
        "\n",
        "    for i, component in enumerate(latency_components):\n",
        "        values = [avg_latencies[m].get(component, 0) for m in modes]\n",
        "        ax1.bar(x, values, width, bottom=bottom,\n",
        "               label=component.replace('_', ' ').title(), color=colors[i])\n",
        "        bottom += values\n",
        "\n",
        "    ax1.set_ylabel('Latency (seconds)')\n",
        "    ax1.set_title('Latency Breakdown by Component')\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(modes, rotation=45)\n",
        "    ax1.legend()\n",
        "    ax1.axhline(y=sla, color='red', linestyle='--', label=f'SLA ({sla}s)')\n",
        "\n",
        "    # å­åœ–2ï¼šç®±é¬šåœ–ï¼ˆç¸½å»¶é²åˆ†å¸ƒï¼‰\n",
        "    ax2 = axes[1]\n",
        "    sns.boxplot(data=latency_df, x='mode', y='wall_time',\n",
        "                order=[\"ClusteredFL\", \"FedProx\", \"FedAvg\", \"CQL\"],\n",
        "                palette=palette, ax=ax2)\n",
        "    ax2.axhline(y=sla, color='red', linestyle='--', label=f'SLA ({sla}s)')\n",
        "    ax2.set_title('End-to-End Latency Distribution')\n",
        "    ax2.set_ylabel('Wall Time (seconds)')\n",
        "    ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45)\n",
        "    ax2.legend()\n",
        "\n",
        "    # å­åœ–3ï¼šæ™‚é–“åºåˆ—ï¼ˆwall-timeè¶¨å‹¢ï¼‰\n",
        "    ax3 = axes[2]\n",
        "    for mode in [\"ClusteredFL\", \"FedProx\", \"FedAvg\", \"CQL\"]:\n",
        "        mode_data = latency_df[latency_df['mode'] == mode]\n",
        "        if not mode_data.empty:\n",
        "            ax3.plot(mode_data['round'], mode_data['wall_time'],\n",
        "                    label=mode, color=palette[mode], marker='o', markersize=4)\n",
        "\n",
        "    ax3.axhline(y=sla, color='red', linestyle='--', label=f'SLA ({sla}s)')\n",
        "    ax3.set_xlabel('Round')\n",
        "    ax3.set_ylabel('Wall Time (seconds)')\n",
        "    ax3.set_title('Latency Trend Over Rounds')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'detailed_latency_analysis.png'), dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "def plot_fairness_metrics(eval_df, history_df, output_dir, window_size=5):\n",
        "    \"\"\"\n",
        "    ç¹ªè£½å…¬å¹³æ€§æŒ‡æ¨™ï¼ˆJainã€Giniã€Theilï¼‰å’Œè¶¨å‹¢\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    palette = {\n",
        "        \"ClusteredFL\": \"#1f77b4\",\n",
        "        \"FedProx\": \"#ff7f0e\",\n",
        "        \"FedAvg\": \"#2ca02c\",\n",
        "        \"CQL\": \"#d62728\"\n",
        "    }\n",
        "\n",
        "    # è¨ˆç®—å…¬å¹³æ€§æŒ‡æ¨™\n",
        "    def calculate_jain_index(rewards):\n",
        "        n = len(rewards)\n",
        "        if n == 0:\n",
        "            return 0\n",
        "        sum_x = np.sum(rewards)\n",
        "        sum_x2 = np.sum(rewards ** 2)\n",
        "        if sum_x2 == 0:\n",
        "            return 0\n",
        "        return (sum_x ** 2) / (n * sum_x2)\n",
        "\n",
        "    def calculate_gini_coefficient(rewards):\n",
        "        n = len(rewards)\n",
        "        if n == 0:\n",
        "            return 0\n",
        "        sorted_rewards = np.sort(rewards)\n",
        "        cumsum = np.cumsum(sorted_rewards)\n",
        "        return (2 * np.sum((np.arange(1, n+1) * sorted_rewards))) / (n * cumsum[-1]) - (n + 1) / n\n",
        "\n",
        "    def calculate_theil_index(rewards):\n",
        "        n = len(rewards)\n",
        "        if n == 0:\n",
        "            return 0\n",
        "        mean_reward = np.mean(rewards)\n",
        "        if mean_reward == 0:\n",
        "            return 0\n",
        "        return np.mean(rewards / mean_reward * np.log(rewards / mean_reward + 1e-10))\n",
        "\n",
        "    # å­åœ–1ï¼šJain Index è¶¨å‹¢\n",
        "    ax1 = axes[0, 0]\n",
        "    for mode in [\"ClusteredFL\", \"FedProx\", \"FedAvg\", \"CQL\"]:\n",
        "        mode_eval = eval_df[eval_df['mode'] == mode]\n",
        "        if len(mode_eval) >= window_size:\n",
        "            jain_values = []\n",
        "            for i in range(len(mode_eval) - window_size + 1):\n",
        "                window_rewards = mode_eval['reward_pfl_finetuned'].iloc[i:i+window_size].values\n",
        "                jain_values.append(calculate_jain_index(window_rewards))\n",
        "\n",
        "            rounds = list(range(window_size-1, len(mode_eval)))\n",
        "            ax1.plot(rounds, jain_values, label=mode, color=palette[mode], marker='o')\n",
        "\n",
        "    ax1.set_xlabel('Client Index (Rolling Window)')\n",
        "    ax1.set_ylabel('Jain Fairness Index')\n",
        "    ax1.set_title(f'Jain Fairness Index (Window Size={window_size})')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim(0, 1)\n",
        "\n",
        "    # å­åœ–2ï¼šå…¬å¹³æ€§æŒ‡æ¨™æŸ±ç‹€åœ–ï¼ˆå«èª¤å·®æ¢ï¼‰\n",
        "    ax2 = axes[0, 1]\n",
        "    fairness_metrics = []\n",
        "\n",
        "    for mode in [\"ClusteredFL\", \"FedProx\", \"FedAvg\", \"CQL\"]:\n",
        "        mode_eval = eval_df[eval_df['mode'] == mode]\n",
        "        if not mode_eval.empty:\n",
        "            rewards = mode_eval['reward_pfl_finetuned'].values\n",
        "\n",
        "            # Bootstrap for confidence intervals\n",
        "            n_bootstrap = 1000\n",
        "            jain_bootstrap = []\n",
        "            gini_bootstrap = []\n",
        "            theil_bootstrap = []\n",
        "\n",
        "            for _ in range(n_bootstrap):\n",
        "                sample = np.random.choice(rewards, size=len(rewards), replace=True)\n",
        "                jain_bootstrap.append(calculate_jain_index(sample))\n",
        "                gini_bootstrap.append(calculate_gini_coefficient(sample))\n",
        "                theil_bootstrap.append(calculate_theil_index(sample))\n",
        "\n",
        "            fairness_metrics.append({\n",
        "                'mode': mode,\n",
        "                'jain': np.mean(jain_bootstrap),\n",
        "                'jain_ci': 1.96 * np.std(jain_bootstrap),\n",
        "                'gini': np.mean(gini_bootstrap),\n",
        "                'gini_ci': 1.96 * np.std(gini_bootstrap),\n",
        "                'theil': np.mean(theil_bootstrap),\n",
        "                'theil_ci': 1.96 * np.std(theil_bootstrap)\n",
        "            })\n",
        "\n",
        "    # ç¹ªè£½åˆ†çµ„æŸ±ç‹€åœ–\n",
        "    metrics_df = pd.DataFrame(fairness_metrics)\n",
        "    x = np.arange(len(metrics_df))\n",
        "    width = 0.25\n",
        "\n",
        "    bars1 = ax2.bar(x - width, metrics_df['jain'], width,\n",
        "                     yerr=metrics_df['jain_ci'], label='Jain Index', capsize=5)\n",
        "    bars2 = ax2.bar(x, metrics_df['gini'], width,\n",
        "                     yerr=metrics_df['gini_ci'], label='Gini Coefficient', capsize=5)\n",
        "    bars3 = ax2.bar(x + width, metrics_df['theil'], width,\n",
        "                     yerr=metrics_df['theil_ci'], label='Theil Index', capsize=5)\n",
        "\n",
        "    ax2.set_xlabel('Mode')\n",
        "    ax2.set_ylabel('Fairness Metric Value')\n",
        "    ax2.set_title('Fairness Metrics Comparison (with 95% CI)')\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels(metrics_df['mode'], rotation=45)\n",
        "    ax2.legend()\n",
        "\n",
        "    # å­åœ–3ï¼šRewardåˆ†å¸ƒå°æç´åœ–\n",
        "    ax3 = axes[1, 0]\n",
        "    sns.violinplot(data=eval_df, x='mode', y='reward_pfl_finetuned',\n",
        "                   order=[\"ClusteredFL\", \"FedProx\", \"FedAvg\", \"CQL\"],\n",
        "                   palette=palette, ax=ax3)\n",
        "    ax3.set_title('Client Reward Distribution (PFL Finetuned)')\n",
        "    ax3.set_ylabel('Reward')\n",
        "    ax3.set_xticklabels(ax3.get_xticklabels(), rotation=45)\n",
        "\n",
        "    # å­åœ–4ï¼šRobust Ïƒ æ¯”è¼ƒ\n",
        "    ax4 = axes[1, 1]\n",
        "    robust_sigmas = []\n",
        "    for mode in [\"ClusteredFL\", \"FedProx\", \"FedAvg\", \"CQL\"]:\n",
        "        mode_data = eval_df[eval_df['mode'] == mode]['reward_pfl_finetuned']\n",
        "        if not mode_data.empty:\n",
        "            q75, q25 = np.percentile(mode_data, [75, 25])\n",
        "            robust_sigma = (q75 - q25) / 1.349\n",
        "            robust_sigmas.append({'mode': mode, 'robust_sigma': robust_sigma})\n",
        "\n",
        "    sigma_df = pd.DataFrame(robust_sigmas)\n",
        "    bars = ax4.bar(sigma_df['mode'], sigma_df['robust_sigma'],\n",
        "                    color=[palette[m] for m in sigma_df['mode']])\n",
        "\n",
        "    # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    ax4.axhline(y=45, color='red', linestyle='--', label='Target Ïƒ < 45')\n",
        "    ax4.set_ylabel('Robust Standard Deviation')\n",
        "    ax4.set_title('Client Fairness (Robust Ïƒ = IQR/1.349)')\n",
        "    ax4.legend()\n",
        "    ax4.set_xticklabels(ax4.get_xticklabels(), rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'fairness_analysis_enhanced.png'), dpi=300)\n",
        "    plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "35GkVU42jBXr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# NEW-CELL: æ–°çš„ collect_metrics å‡½æ•¸\n",
        "def collect_enhanced_metrics(agents, client_envs, round_num, latency_monitor):\n",
        "    \"\"\"\n",
        "    æ”¶é›†å¢å¼·çš„æŒ‡æ¨™ï¼ˆæ‹†åˆ†å»¶é²çµ„ä»¶ï¼‰\n",
        "    \"\"\"\n",
        "    metrics = []\n",
        "\n",
        "    for client_id, agent in agents.items():\n",
        "        env = client_envs.get(client_id)\n",
        "        if env is None:\n",
        "            continue\n",
        "\n",
        "        # è©•ä¼°æ€§èƒ½\n",
        "        reward = evaluate_agent_performance(env, agent.get_clean_state_dict())\n",
        "\n",
        "        # ç²å–å»¶é²çµ„ä»¶\n",
        "        latency_stats = latency_monitor.component_times if hasattr(latency_monitor, 'component_times') else {}\n",
        "\n",
        "        # è¨ˆç®—å…¬å¹³æ€§æŒ‡æ¨™\n",
        "        all_rewards = [evaluate_agent_performance(e, agent.get_clean_state_dict())\n",
        "                      for e in client_envs.values()]\n",
        "\n",
        "        jain = calculate_jain_index(np.array(all_rewards))\n",
        "        gini = calculate_gini_coefficient(np.array(all_rewards))\n",
        "        theil = calculate_theil_index(np.array(all_rewards))\n",
        "\n",
        "        metrics.append({\n",
        "            'round': round_num,\n",
        "            'client_id': client_id,\n",
        "            'reward': reward,\n",
        "            'jain': jain,\n",
        "            'gini': gini,\n",
        "            'theil': theil,\n",
        "            'rtt': np.mean(latency_stats.get('client_to_server_comm', [0]) +\n",
        "                          latency_stats.get('server_to_client_comm', [0])),\n",
        "            'local_training': np.mean(latency_stats.get('client_compute_time', [0])),\n",
        "            'aggregation': np.mean(latency_stats.get('server_aggregation_time', [0]))\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(metrics)\n",
        "\n",
        "def evaluate_agent_performance(env, model_state, num_episodes=5):\n",
        "    \"\"\"Helper function to evaluate agent performance\"\"\"\n",
        "    if env.trajectory.size == 0:\n",
        "        return 0.0\n",
        "    # Simplified evaluation logic\n",
        "    return np.random.randn() * 10 + 150  # Placeholder\n",
        "\n",
        "def calculate_jain_index(rewards):\n",
        "    n = len(rewards)\n",
        "    if n == 0:\n",
        "        return 0\n",
        "    sum_x = np.sum(rewards)\n",
        "    sum_x2 = np.sum(rewards ** 2)\n",
        "    if sum_x2 == 0:\n",
        "        return 0\n",
        "    return (sum_x ** 2) / (n * sum_x2)\n",
        "\n",
        "def calculate_gini_coefficient(rewards):\n",
        "    n = len(rewards)\n",
        "    if n == 0:\n",
        "        return 0\n",
        "    sorted_rewards = np.sort(rewards)\n",
        "    cumsum = np.cumsum(sorted_rewards)\n",
        "    if cumsum[-1] == 0:\n",
        "        return 0\n",
        "    return (2 * np.sum((np.arange(1, n+1) * sorted_rewards))) / (n * cumsum[-1]) - (n + 1) / n\n",
        "\n",
        "def calculate_theil_index(rewards):\n",
        "    n = len(rewards)\n",
        "    if n == 0:\n",
        "        return 0\n",
        "    mean_reward = np.mean(rewards)\n",
        "    if mean_reward == 0:\n",
        "        return 0\n",
        "    return np.mean(rewards / mean_reward * np.log(rewards / mean_reward + 1e-10))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-vEs5j8yjXV5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# NEW-FILE: grid_search.py\n",
        "import itertools\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class HyperparameterSearch:\n",
        "    \"\"\"\n",
        "    è¶…åƒæ•¸ç¶²æ ¼æœç´¢å·¥å…·\n",
        "    \"\"\"\n",
        "    def __init__(self, base_config: dict, param_grid: dict, output_dir: str):\n",
        "        self.base_config = base_config\n",
        "        self.param_grid = param_grid\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    def generate_configs(self) -> List[Dict]:\n",
        "        \"\"\"ç”Ÿæˆæ‰€æœ‰åƒæ•¸çµ„åˆ\"\"\"\n",
        "        configs = []\n",
        "        grid = ParameterGrid(self.param_grid)\n",
        "\n",
        "        for params in grid:\n",
        "            config = self.base_config.copy()\n",
        "            config.update(params)\n",
        "            configs.append(config)\n",
        "\n",
        "        logger.info(f\"Generated {len(configs)} configurations for grid search\")\n",
        "        return configs\n",
        "\n",
        "    def run_experiment(self, config: dict, data_path: str) -> dict:\n",
        "        \"\"\"åŸ·è¡Œå–®å€‹å¯¦é©—ï¼ˆéœ€è¦å°å…¥ run_enhanced_experimentï¼‰\"\"\"\n",
        "        # é€™è£¡å‡è¨­ run_enhanced_experiment å·²å®šç¾©\n",
        "        from main import run_enhanced_experiment\n",
        "\n",
        "        success, result_info = run_enhanced_experiment(config, data_path)\n",
        "\n",
        "        if success and result_info:\n",
        "            return {\n",
        "                'config': config,\n",
        "                'success': True,\n",
        "                'avg_reward': result_info['avg_rewards']['pfl'],\n",
        "                'epsilon': result_info.get('privacy_stats', {}).get('consumed_epsilon', 0),\n",
        "                'latency': result_info.get('latency_stats', {}).get('mean_time', 0),\n",
        "                'execution_time': result_info['execution_time']\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'config': config,\n",
        "                'success': False\n",
        "            }\n",
        "\n",
        "    def search(self, data_path: str, n_parallel: int = 1) -> List[dict]:\n",
        "        \"\"\"åŸ·è¡Œç¶²æ ¼æœç´¢\"\"\"\n",
        "        configs = self.generate_configs()\n",
        "        results = []\n",
        "\n",
        "        # ç°¡åŒ–ç‰ˆï¼šé †åºåŸ·è¡Œ\n",
        "        for i, config in enumerate(configs):\n",
        "            logger.info(f\"Running experiment {i+1}/{len(configs)}\")\n",
        "            result = self.run_experiment(config, data_path)\n",
        "            results.append(result)\n",
        "\n",
        "            # ä¿å­˜ä¸­é–“çµæœ\n",
        "            self.save_results(results)\n",
        "\n",
        "        # æ‰¾å‡ºæœ€ä½³é…ç½®\n",
        "        best_result = self.find_best_result(results)\n",
        "        logger.info(f\"Best configuration found: {best_result['config']}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def find_best_result(self, results: List[dict]) -> dict:\n",
        "        \"\"\"æ‰¾å‡ºæœ€ä½³çµæœï¼ˆåŸºæ–¼ Pareto å„ªåŒ–ï¼‰\"\"\"\n",
        "        valid_results = [r for r in results if r['success']]\n",
        "\n",
        "        if not valid_results:\n",
        "            return None\n",
        "\n",
        "        # è¨ˆç®— Pareto score\n",
        "        for result in valid_results:\n",
        "            # æ¨™æº–åŒ–æŒ‡æ¨™\n",
        "            reward_score = result['avg_reward'] / 165  # ç›®æ¨™ > 165\n",
        "            epsilon_score = 1 - (result['epsilon'] / 4)  # ç›®æ¨™ < 4\n",
        "            latency_score = 1 - (result['latency'] / 40)  # ç›®æ¨™ < 40\n",
        "\n",
        "            # ç¶œåˆè©•åˆ†ï¼ˆå¯èª¿æ•´æ¬Šé‡ï¼‰\n",
        "            result['pareto_score'] = (\n",
        "                0.4 * reward_score +\n",
        "                0.3 * epsilon_score +\n",
        "                0.3 * latency_score\n",
        "            )\n",
        "\n",
        "        # è¿”å›æœ€é«˜åˆ†\n",
        "        return max(valid_results, key=lambda x: x['pareto_score'])\n",
        "\n",
        "    def save_results(self, results: List[dict]):\n",
        "        \"\"\"ä¿å­˜æœç´¢çµæœ\"\"\"\n",
        "        output_path = os.path.join(self.output_dir, 'grid_search_results.json')\n",
        "\n",
        "        # è½‰æ›ç‚ºå¯åºåˆ—åŒ–æ ¼å¼\n",
        "        serializable_results = []\n",
        "        for r in results:\n",
        "            result_copy = r.copy()\n",
        "            # å°‡ numpy é¡å‹è½‰æ›ç‚ºæ¨™æº– Python é¡å‹\n",
        "            for key in ['avg_reward', 'epsilon', 'latency', 'execution_time', 'pareto_score']:\n",
        "                if key in result_copy and isinstance(result_copy[key], np.number):\n",
        "                    result_copy[key] = float(result_copy[key])\n",
        "            serializable_results.append(result_copy)\n",
        "\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(serializable_results, f, indent=2)\n",
        "\n",
        "        logger.info(f\"Results saved to {output_path}\")\n",
        "\n",
        "# ç¤ºä¾‹åƒæ•¸ç¶²æ ¼\n",
        "EXAMPLE_PARAM_GRID = {\n",
        "    \"fedprox_mu\": [0, 0.01, 0.1],\n",
        "    \"num_clusters\": [2, 3, 4],\n",
        "    \"dp_noise_multiplier\": [0.3, 0.5, 0.7],\n",
        "    \"cql_alpha\": [0.5, 1.0, 2.0],\n",
        "    \"batch_size\": [64, 128, 256]\n",
        "}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iZsKzl_Ajado"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# NEW-CELL: ä¿®æ­£ simulate_frl_rounds çµæ§‹\n",
        "def run_single_round(agent, env, config):\n",
        "    \"\"\"åŸ·è¡Œå–®è¼ªè¨“ç·´\"\"\"\n",
        "    agent.model.train()\n",
        "    total_loss, total_reward = 0.0, 0.0\n",
        "    episodes = config.local_episodes_per_round\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0.0\n",
        "\n",
        "        for step in range(config.steps_per_episode):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            if len(agent.memory) > config.replay_start_size and step % config.replay_frequency == 0:\n",
        "                loss = agent.replay(num_batches=config.replay_batches_per_call)\n",
        "                total_loss += loss\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        total_reward += episode_reward\n",
        "\n",
        "        if (episode + 1) % config.target_update_freq == 0:\n",
        "            agent.update_target_model()\n",
        "\n",
        "    return total_loss / episodes, total_reward / episodes\n",
        "\n",
        "def aggregate_models(client_updates, server):\n",
        "    \"\"\"èšåˆæ¨¡å‹æ›´æ–°\"\"\"\n",
        "    if not client_updates:\n",
        "        return None\n",
        "\n",
        "    # ä½¿ç”¨æœå‹™å™¨çš„åŠ æ¬Šèšåˆæ–¹æ³•\n",
        "    aggregated_model = server.aggregate_weighted(client_updates)\n",
        "    return aggregated_model\n",
        "\n",
        "def log_round_metrics(round_num, metrics, logger):\n",
        "    \"\"\"è¨˜éŒ„è¼ªæ¬¡æŒ‡æ¨™\"\"\"\n",
        "    avg_reward = np.mean([m['reward'] for m in metrics])\n",
        "    avg_loss = np.mean([m['loss'] for m in metrics])\n",
        "\n",
        "    logger.info(f\"Round {round_num}: Avg Reward={avg_reward:.4f}, Avg Loss={avg_loss:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'round': round_num,\n",
        "        'avg_reward': avg_reward,\n",
        "        'avg_loss': avg_loss,\n",
        "        'num_clients': len(metrics)\n",
        "    }"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2SmAIRZ7jiwC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwLF6eFhEAgo",
        "outputId": "2d4e8434-f4dc-4220-d964-d418f5b149bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell 8A: å¼·åŒ–ç‰ˆå¯¦é©—åŸ·è¡Œå‡½æ•¸å·²è¼‰å…¥\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 8A: ğŸ¬ å¼·åŒ–ç‰ˆå¯¦é©—åŸ·è¡Œå‡½æ•¸\n",
        "import time\n",
        "import gc\n",
        "from datetime import datetime\n",
        "\n",
        "def run_enhanced_experiment(config_dict: dict, data_path: str):\n",
        "    \"\"\"\n",
        "    é‹è¡Œå¼·åŒ–ç‰ˆå¯¦é©—ï¼ˆæ”¯æ´æ‰€æœ‰æ–°åŠŸèƒ½ï¼‰\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    runner = None\n",
        "\n",
        "    try:\n",
        "        config = TrainingConfig(**config_dict)\n",
        "        logger.info(f\"\\n{'='*15} ğŸ”„ å¼·åŒ–ç‰ˆå¯¦é©—: {config.experiment_name} {'='*15}\")\n",
        "\n",
        "        # 1. æº–å‚™æ•¸æ“šï¼ˆæ”¯æ´ Non-IIDï¼‰\n",
        "        logger.info(f\"ğŸ“Š æ­£åœ¨æº–å‚™æ•¸æ“š...\")\n",
        "        data_manager = DataManager(\n",
        "            data_path,\n",
        "            config.base_client_pairs,\n",
        "            enable_non_iid=config.enable_non_iid,\n",
        "            dirichlet_alpha=config.dirichlet_alpha,\n",
        "            min_clients=config.min_clients_non_iid\n",
        "        )\n",
        "\n",
        "        # ç”Ÿæˆå®¢æˆ¶ç«¯è»Œè·¡\n",
        "        all_trajectories = data_manager.get_client_trajectories()\n",
        "\n",
        "        logger.info(f\"   - æ•¸æ“šåˆ†é…æ¨¡å¼: {'Non-IID' if config.enable_non_iid else 'IID'}\")\n",
        "        logger.info(f\"   - å®¢æˆ¶ç«¯æ•¸é‡: {len(all_trajectories)}\")\n",
        "\n",
        "        if config.enable_non_iid and hasattr(data_manager, 'client_assignments'):\n",
        "            # é¡¯ç¤º Non-IID åˆ†ä½ˆçµ±è¨ˆ\n",
        "            sizes = list(data_manager.client_assignments.values())\n",
        "            logger.info(f\"   - Non-IID åˆ†ä½ˆçµ±è¨ˆ:\")\n",
        "            logger.info(f\"     * å¹³å‡æ•¸æ“šé‡: {np.mean(sizes):.0f}\")\n",
        "            logger.info(f\"     * æ¨™æº–å·®: {np.std(sizes):.0f}\")\n",
        "            logger.info(f\"     * æœ€å¤§/æœ€å°æ¯”: {max(sizes)/min(sizes):.2f}\")\n",
        "\n",
        "        # 2. é…ç½®é©—è­‰\n",
        "        logger.info(f\"\\nğŸ”§ å¯¦é©—é…ç½®:\")\n",
        "        logger.info(f\"   - æ¨¡å¼: {config.mode}\")\n",
        "        logger.info(f\"   - CQL: {'å•Ÿç”¨' if config.enable_cql else 'ç¦ç”¨'}\")\n",
        "        logger.info(f\"   - Adaptive Clipping: {'å•Ÿç”¨' if config.enable_adaptive_clip else 'ç¦ç”¨'}\")\n",
        "        logger.info(f\"   - éåŒæ­¥èšåˆ: {'å•Ÿç”¨' if config.enable_async_aggregation else 'ç¦ç”¨'}\")\n",
        "        logger.info(f\"   - å·®åˆ†éš±ç§: {'å•Ÿç”¨' if config.enable_dp else 'ç¦ç”¨'}\")\n",
        "\n",
        "        # 3. å‰µå»ºä¸¦é‹è¡Œå¯¦é©—\n",
        "        logger.info(f\"\\nğŸš€ é–‹å§‹å¯¦é©—...\")\n",
        "        runner = ExperimentRunner(config, data_manager, all_trajectories, config.client_pairs)\n",
        "        eval_res, history_res = runner.run()\n",
        "\n",
        "        execution_time = (time.time() - start_time) / 60\n",
        "        logger.info(f\"\\nâ±ï¸ å¯¦é©—å®Œæˆæ™‚é–“: {execution_time:.2f} åˆ†é˜\")\n",
        "\n",
        "        # 4. çµæœåˆ†æ\n",
        "        logger.info(f\"\\nâœ… å¯¦é©—çµæœæ‘˜è¦:\")\n",
        "        if not eval_res.empty and len(eval_res) > 0:\n",
        "            avg_global_reward = eval_res['reward_global'].mean()\n",
        "            avg_personalized_reward = eval_res['reward_personalized'].mean()\n",
        "            avg_pfl_reward = eval_res['reward_pfl_finetuned'].mean()\n",
        "\n",
        "            logger.info(f\"   - å¹³å‡å…¨åŸŸçå‹µ: {avg_global_reward:.4f}\")\n",
        "            logger.info(f\"   - å¹³å‡å€‹äººåŒ–çå‹µ: {avg_personalized_reward:.4f}\")\n",
        "            logger.info(f\"   - å¹³å‡PFLå¾®èª¿çå‹µ: {avg_pfl_reward:.4f}\")\n",
        "            logger.info(f\"   - å€‹äººåŒ–æå‡: {((avg_personalized_reward/avg_global_reward-1)*100):.2f}%\")\n",
        "\n",
        "        # 5. å»¶é²åˆ†æ\n",
        "        if hasattr(runner, 'latency_monitor'):\n",
        "            latency_stats = runner.latency_monitor.get_statistics()\n",
        "            if latency_stats and latency_stats.get('total_rounds', 0) > 0:\n",
        "                logger.info(f\"\\nâ±ï¸ å»¶é²åˆ†æ:\")\n",
        "                logger.info(f\"   - å¹³å‡è¼ªå»¶é²: {latency_stats['mean_time']:.2f}s\")\n",
        "                logger.info(f\"   - 95åˆ†ä½å»¶é²: {latency_stats['p95_time']:.2f}s\")\n",
        "                if latency_stats['warnings'] > 0:\n",
        "                    logger.warning(f\"   - âš ï¸ è¶…é1ç§’è­¦å‘Š: {latency_stats['warnings']}æ¬¡\")\n",
        "\n",
        "        # 6. éš±ç§åˆ†æ\n",
        "        privacy_stats = None\n",
        "        if config.enable_dp and config.mode != 'Centralized' and runner is not None:\n",
        "            consumed_budget = getattr(runner, 'consumed_privacy_budget', 0.0)\n",
        "            dp_reset_history = getattr(runner, 'dp_reset_history', [])\n",
        "\n",
        "            logger.info(f\"\\nğŸ›¡ï¸ éš±ç§åˆ†æ:\")\n",
        "            logger.info(f\"   - æœ€çµ‚Îµæ¶ˆè€—: {consumed_budget:.4f}\")\n",
        "            logger.info(f\"   - é ç®—ä½¿ç”¨ç‡: {(consumed_budget/config.dp_target_epsilon*100):.1f}%\")\n",
        "            logger.info(f\"   - é‡è¨­æ¬¡æ•¸: {len(dp_reset_history)}\")\n",
        "\n",
        "            if config.enable_adaptive_clip:\n",
        "                logger.info(f\"   - Adaptive Clipping: âœ… å·²ä½¿ç”¨\")\n",
        "\n",
        "            privacy_stats = {\n",
        "                'consumed_epsilon': consumed_budget,\n",
        "                'reset_count': len(dp_reset_history)\n",
        "            }\n",
        "\n",
        "        # 7. CQL åˆ†æ\n",
        "        if config.mode == 'CQL' and hasattr(runner, 'cql_performance_history'):\n",
        "            if runner.cql_performance_history:\n",
        "                avg_cql_loss = np.mean([h['avg_cql_loss'] for h in runner.cql_performance_history])\n",
        "                logger.info(f\"\\nğŸ¯ CQL åˆ†æ:\")\n",
        "                logger.info(f\"   - å¹³å‡ CQL æå¤±: {avg_cql_loss:.4f}\")\n",
        "                logger.info(f\"   - ä¿å®ˆæ€§åƒæ•¸ Î±: {config.cql_alpha}\")\n",
        "\n",
        "        # 8. éåŒæ­¥èšåˆçµ±è¨ˆ\n",
        "        if config.enable_async_aggregation and hasattr(runner.server, 'async_aggregation_count'):\n",
        "            logger.info(f\"\\nğŸ”„ éåŒæ­¥èšåˆçµ±è¨ˆ:\")\n",
        "            logger.info(f\"   - éåŒæ­¥èšåˆæ¬¡æ•¸: {runner.server.async_aggregation_count}\")\n",
        "\n",
        "        # 9. æ¸…ç†è³‡æº\n",
        "        del runner, data_manager, all_trajectories\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # æª¢æŸ¥GPUè¨˜æ†¶é«”\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "            logger.info(f\"\\nğŸ’¾ æœ€çµ‚GPUè¨˜æ†¶é«”ä½¿ç”¨: {allocated:.2f} GB\")\n",
        "\n",
        "        return True, {\n",
        "            'execution_time': execution_time,\n",
        "            'avg_rewards': {\n",
        "                'global': avg_global_reward if 'avg_global_reward' in locals() else 0,\n",
        "                'personalized': avg_personalized_reward if 'avg_personalized_reward' in locals() else 0,\n",
        "                'pfl': avg_pfl_reward if 'avg_pfl_reward' in locals() else 0\n",
        "            },\n",
        "            'privacy_stats': privacy_stats,\n",
        "            'latency_stats': latency_stats if 'latency_stats' in locals() else None\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        execution_time = (time.time() - start_time) / 60\n",
        "        logger.error(f\"\\nâŒ å¯¦é©—å¤±æ•—: {config_dict.get('experiment_name')}\")\n",
        "        logger.error(f\"â±ï¸ å¤±æ•—æ™‚é–“: {execution_time:.2f} åˆ†é˜\")\n",
        "        logger.error(f\"ğŸ” éŒ¯èª¤è©³æƒ…: {str(e)}\")\n",
        "        import traceback\n",
        "        logger.error(f\"ğŸ“‹ éŒ¯èª¤å †ç–Š: {traceback.format_exc()}\")\n",
        "\n",
        "        # æ¸…ç†è³‡æº\n",
        "        if runner is not None:\n",
        "            del runner\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        return False, None\n",
        "\n",
        "print(\"âœ… Cell 8A: å¼·åŒ–ç‰ˆå¯¦é©—åŸ·è¡Œå‡½æ•¸å·²è¼‰å…¥\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rXdMvsOECy7",
        "outputId": "ffaab37a-85e9-46d4-fc34-186c92054e91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "âœ… ç’°å¢ƒè¨­å®šå®Œæˆï¼Œæº–å‚™åŸ·è¡Œå¯¦é©—\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 8B: ğŸ”§ ç’°å¢ƒè¨­å®šèˆ‡åˆå§‹åŒ–\n",
        "import os\n",
        "import time\n",
        "\n",
        "# GPUç’°å¢ƒè¨­å®š\n",
        "setup_gpu_environment()\n",
        "\n",
        "# ç¢ºä¿GPUè¢«æ­£ç¢ºä½¿ç”¨\n",
        "if torch.cuda.is_available():\n",
        "    logger.info(f\"ğŸ® GPU å¯ç”¨: {torch.cuda.get_device_name(0)}\")\n",
        "    logger.info(f\"ğŸ“Š GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    logger.warning(\"âš ï¸ GPU ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU æ¨¡å¼\")\n",
        "\n",
        "# ç’°å¢ƒè·¯å¾‘è¨­å®š\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_WORK_DIR = \"/content/drive/MyDrive/FRL_Slicing_Sim\"\n",
        "    logger.info(\"ğŸ”— Google Drive æ›è¼‰æˆåŠŸ\")\n",
        "except ImportError:\n",
        "    BASE_WORK_DIR = \"./FRL_Slicing_Sim\"\n",
        "    logger.info(\"ğŸ’» æœ¬åœ°ç’°å¢ƒæ¨¡å¼\")\n",
        "\n",
        "os.makedirs(BASE_WORK_DIR, exist_ok=True)\n",
        "DATA_PATH = os.path.join(BASE_WORK_DIR, \"kpi_traces_final_robust0.parquet\")\n",
        "BASE_OUTPUT_DIR = os.path.join(BASE_WORK_DIR, \"outputs_enhanced\")\n",
        "logger.info(f\"ğŸ“ æ•¸æ“šè·¯å¾‘: {DATA_PATH}\")\n",
        "logger.info(f\"ğŸ“ è¼¸å‡ºç›®éŒ„: {BASE_OUTPUT_DIR}\")\n",
        "\n",
        "# æª¢æŸ¥æ•¸æ“šæ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(f\"æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {DATA_PATH}\")\n",
        "\n",
        "# å¯¦é©—é…ç½®\n",
        "MODES_TO_RUN = [\"ClusteredFL\", \"FedProx\", \"FedAvg\", \"CQL\"]\n",
        "SEEDS = [42]  # ç°¡åŒ–ç‚ºå–®ä¸€ç¨®å­ä»¥ç¬¦åˆ5è¼ªç¤ºä¾‹è¦æ±‚\n",
        "\n",
        "logger.info(f\"\\nğŸ¯ å¯¦é©—è¨ˆåŠƒ:\")\n",
        "logger.info(f\"   - æ¸¬è©¦æ¨¡å¼: {MODES_TO_RUN}\")\n",
        "logger.info(f\"   - éš¨æ©Ÿç¨®å­: {SEEDS}\")\n",
        "logger.info(f\"   - ç¸½å¯¦é©—æ•¸: {len(MODES_TO_RUN) * len(SEEDS)}\")\n",
        "\n",
        "# å…¨å±€çµæœå­˜å„²\n",
        "experiment_results = []\n",
        "\n",
        "print(\"\\nâœ… ç’°å¢ƒè¨­å®šå®Œæˆï¼Œæº–å‚™åŸ·è¡Œå¯¦é©—\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ecd8a7f8456e4c35b5df0a9af5723473",
            "9dfcef1cb9bf477a909acb4dbd32b876",
            "9b86bbe8f7a147fa8efceae43a87cace",
            "c51e06324eeb4961bea60f024ce51caa",
            "2b7d7255fcbb4d79991aceaa6725f029",
            "29aaee57e48a4e45997aa13c19173434",
            "c387532806ac4822b14426fc28fa224c",
            "7c0b889c882940a7a89480155af8fef2",
            "ed458669f4794e1fa569d339786c00b8",
            "d73e469a9f1b4a2d9bfb52857c55f674",
            "ef0f31b3d8d44eac9d40840cb446b896",
            "557ac0fd04a74f1685f314bed8c9e51f",
            "c40456ba6dcd4547893223fbe4f1bb57",
            "2d1ce7facd044f7783bed3e1fa32f771",
            "6b6b42aaa43c43648f9e19adabb106e9",
            "dac32e58447844c5a1e8edc7eb8267c1",
            "40871663e605410981e68717c34f858c",
            "76db7a64f9344d9c964010767b1155ac",
            "8cd1140d0ba14c4182f2a66f283dbd83",
            "fe74e6ad794e4389b360f2416c4d7e3c",
            "dd99acf48aed4827851998254dbe3854",
            "959c2fdc4b4f4f30acbe523955f32394",
            "0bf8e89fa08f47eaaa8a9bde801ba958",
            "2a1d2e58a88e40599782c74db5eb4311",
            "66d5d9e6790e49efa4b905bb13afd4c2",
            "e1fababc1fdd425fa780d5d2001f9266",
            "bb7a30d997d841d1ab73a7bfeabf5ac6",
            "f5943860aa8a470eb1576ed0a0154231",
            "a1df46e11b174ad4be8bfa6cd2727837",
            "638d012626564f66b157b8a7b9bd6556",
            "b9726ca0064647d8a334f77644fe80d4",
            "9d42d62f5ecf4e1f95bb269a7f0b8164",
            "dcaf1b36f7f1474f9fc22a7c171601bb",
            "360e07cb76354723abe6d1675a5e50b5",
            "6dbc3bfb108e4afd84fe2b979dee838a",
            "dbe2602ffdb2419b86ea5ca44281bfcf",
            "3e59a9c2ba944826ac7093a55636fb1e",
            "e52325c596e8445a82b7d5e77a5e3963",
            "56b3251d52e8468ea61f25e9d409ca98",
            "aab2787947f04c62b1709cb623a7730f",
            "38f0f12acfd9497e90c0782e0b16b95a",
            "54427b901439429c87acf9732f9e39ba",
            "dc50fec87b4f4afbbd5acf9d1494354c",
            "04f4521bcb2d488f99f2950193a61341",
            "54e3f5c77e3b4d848bc7d032ea4eb83e",
            "9f06c7a6c9664d30ae2d24d677dcf92f",
            "949195bc442847a4aee1127841e90840",
            "8f0caef3d10b47c184350af8da00ea2d",
            "58264721e6794bf69b9a364436eb0d49",
            "47dd5f5856a247588aff1b57ac524f45",
            "f370dd06597245db8ec52b632f04dc6e",
            "2da9c207cce3490baf873d1057aea9d8",
            "cf54d44f3c4449d2949d9b919d72245d",
            "d7b3770ddd504f8781958bee42fa689f",
            "fb8f33b04b2d4d35b8097398416980e4",
            "3f5b5014fa934425aafbce2b61300ec2",
            "e747bdf56b6648e19c29b8edd23b0a3e",
            "be5d555a430548179c0fff086edd9851",
            "656ee897650f41b3afa90f974d426faf",
            "bd1f8bbf241942cd8fa1ec6983949c9f",
            "4a341d94632c4de289e6f7637c0fff19",
            "30c4303ff73c46b9bddedfe95ad6dd24",
            "e6c073e64fdf41d8bb73d70ba83b7712",
            "c5d983736bcd4c39b8c4d496e7fd1ba4",
            "1ac69829d08a46ed98b3d93525f41b19",
            "8d4cb6b47d0041248b799da32c9da160",
            "2fbb01d1a665496c90059b6eacb47968",
            "191a3cf032644c749a94149c3f290c83",
            "c5164b185b6e423fbaedd9ea79acd8aa",
            "c1ae23b61d4e432fba17a77d164f1fb1",
            "9d52cc4eb4564c8cb4d77802c7e4ac75",
            "781805d4fab242278684066c2ce35202",
            "7199d542a1ca44f19ddb80921861f7dc",
            "1588fa5a55f94c26841b6835a98199d3",
            "f620c927583f403389d4f5464e2f6246",
            "945cb2b898ba433a8309805168a834c3",
            "68eb3a9e25e548fe81b54a4c35befaec"
          ]
        },
        "id": "LbLBTZi-EFLt",
        "outputId": "8cd9c8c6-6bf9-453f-9cc9-dd326ec78fed"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ğŸš€ é–‹å§‹åŸ·è¡Œå¼·åŒ–ç‰ˆå¯¦é©—ï¼ˆ5è¼ªç¤ºä¾‹ï¼‰\n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "åŸ·è¡Œå¯¦é©—: ClusteredFL\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthc1006\u001b[0m (\u001b[33mthc1006-national-yang-ming-chiao-tung-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250709_112108-sk3cs3y8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/sk3cs3y8' target=\"_blank\">ClusteredFL_enhanced_s42</a></strong> to <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced' target=\"_blank\">https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/sk3cs3y8' target=\"_blank\">https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/sk3cs3y8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecd8a7f8456e4c35b5df0a9af5723473",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ClusteredFL Training:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:\n",
            "==================== âš ï¸ éš±ç§é ç®—é¦–æ¬¡è¶…æ”¯ï¼ ====================\n",
            "WARNING:__main__:   - ç•¶å‰æ¶ˆè€—: Îµ = 62.5732\n",
            "WARNING:__main__:   - é ç®—ä¸Šé™: Îµ = 8.0\n",
            "WARNING:__main__:==========================================================\n",
            "WARNING:__main__:Round 1 å»¶é²è­¦å‘Š: 13.93s > 1.0s\n",
            "WARNING:__main__:\n",
            "==================== âš ï¸ éš±ç§é ç®—é¦–æ¬¡è¶…æ”¯ï¼ ====================\n",
            "WARNING:__main__:   - ç•¶å‰æ¶ˆè€—: Îµ = 168.4542\n",
            "WARNING:__main__:   - é ç®—ä¸Šé™: Îµ = 8.0\n",
            "WARNING:__main__:==========================================================\n",
            "WARNING:__main__:[C-0] éš±ç§é ç®—æ¶ˆè€—éå¿«: 204.3816/è¼ª\n",
            "WARNING:__main__:[C-1] éš±ç§é ç®—æ¶ˆè€—éå¿«: 51.2418/è¼ª\n",
            "WARNING:__main__:[C-2] éš±ç§é ç®—æ¶ˆè€—éå¿«: 371.0294/è¼ª\n",
            "WARNING:__main__:[C-3] éš±ç§é ç®—æ¶ˆè€—éå¿«: 64.5039/è¼ª\n",
            "WARNING:__main__:[C-4] éš±ç§é ç®—æ¶ˆè€—éå¿«: 100.1608/è¼ª\n",
            "WARNING:__main__:[C-6] éš±ç§é ç®—æ¶ˆè€—éå¿«: 33.6189/è¼ª\n",
            "WARNING:__main__:Round 2 å»¶é²è­¦å‘Š: 54.69s > 1.0s\n",
            "WARNING:__main__:\n",
            "==================== âš ï¸ éš±ç§é ç®—é¦–æ¬¡è¶…æ”¯ï¼ ====================\n",
            "WARNING:__main__:   - ç•¶å‰æ¶ˆè€—: Îµ = 239.2194\n",
            "WARNING:__main__:   - é ç®—ä¸Šé™: Îµ = 8.0\n",
            "WARNING:__main__:==========================================================\n",
            "WARNING:__main__:[C-4] éš±ç§é ç®—æ¶ˆè€—éå¿«: 90.5991/è¼ª\n",
            "WARNING:__main__:[C-5] éš±ç§é ç®—æ¶ˆè€—éå¿«: 435.5333/è¼ª\n",
            "WARNING:__main__:[C-7] éš±ç§é ç®—æ¶ˆè€—éå¿«: 239.2409/è¼ª\n",
            "WARNING:__main__:[C-8] éš±ç§é ç®—æ¶ˆè€—éå¿«: 239.2409/è¼ª\n",
            "WARNING:__main__:[C-9] éš±ç§é ç®—æ¶ˆè€—éå¿«: 167.3015/è¼ª\n",
            "WARNING:__main__:Round 3 å»¶é²è­¦å‘Š: 83.57s > 1.0s\n",
            "WARNING:__main__:\n",
            "==================== âš ï¸ éš±ç§é ç®—é¦–æ¬¡è¶…æ”¯ï¼ ====================\n",
            "WARNING:__main__:   - ç•¶å‰æ¶ˆè€—: Îµ = 314.7510\n",
            "WARNING:__main__:   - é ç®—ä¸Šé™: Îµ = 8.0\n",
            "WARNING:__main__:==========================================================\n",
            "WARNING:__main__:[C-0] éš±ç§é ç®—æ¶ˆè€—éå¿«: 415.4959/è¼ª\n",
            "WARNING:__main__:[C-7] éš±ç§é ç®—æ¶ˆè€—éå¿«: 254.9252/è¼ª\n",
            "WARNING:__main__:[C-9] éš±ç§é ç®—æ¶ˆè€—éå¿«: 173.0723/è¼ª\n",
            "WARNING:__main__:Round 4 å»¶é²è­¦å‘Š: 110.01s > 1.0s\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "557ac0fd04a74f1685f314bed8c9e51f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "æœ€çµ‚è©•ä¼°:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:P95å»¶é² 104.724s è¶…éSLA 0.05s\n",
            "WARNING:__main__:P95å»¶é² 104.724s è¶…éSLA 0.05s\n",
            "WARNING:__main__:P95å»¶é² 104.724s è¶…éSLA 0.05s\n",
            "WARNING:__main__:   - âš ï¸ è¶…é1ç§’è­¦å‘Š: 4æ¬¡\n",
            "WARNING:__main__:   - âœ— å»¶é²è¶…æ¨™ (>40s)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "åŸ·è¡Œå¯¦é©—: FedProx\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>â–â–†â–ˆâ–†â–†</td></tr><tr><td>avg_reward</td><td>â–â–†â–…â–‡â–ˆ</td></tr><tr><td>budget_ratio</td><td>â–â–â–â–â–</td></tr><tr><td>cluster_reassign_count</td><td>â–â–â–â–â–</td></tr><tr><td>cumulative_epsilon</td><td>â–â–â–â–â–</td></tr><tr><td>delta</td><td>â–â–â–â–â–</td></tr><tr><td>epsilon</td><td>â–â–‚â–…â–†â–ˆ</td></tr><tr><td>epsilon_max</td><td>â–â–‡â–‡â–ˆ</td></tr><tr><td>epsilon_min</td><td>â–â–ƒâ–ˆâ–ˆ</td></tr><tr><td>epsilon_std</td><td>â–â–‡â–‡â–ˆ</td></tr><tr><td>noise_multiplier</td><td>â–â–â–â–</td></tr><tr><td>participating_clients</td><td>â–ˆâ–â–ˆâ–…â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>reset_count</td><td>â–â–ƒâ–…â–†â–ˆ</td></tr><tr><td>round</td><td>â–â–â–ƒâ–ƒâ–…â–…â–†â–†â–ˆâ–ˆ</td></tr><tr><td>timeout_clients</td><td>â–ˆâ–â–ƒâ–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>async_aggregation</td><td>True</td></tr><tr><td>avg_loss</td><td>56597422.74088</td></tr><tr><td>avg_reward</td><td>164.9491</td></tr><tr><td>budget_ratio</td><td>0</td></tr><tr><td>cluster_reassign_count</td><td>0</td></tr><tr><td>cumulative_epsilon</td><td>0</td></tr><tr><td>delta</td><td>1e-05</td></tr><tr><td>epsilon</td><td>314.75101</td></tr><tr><td>epsilon_max</td><td>494.16606</td></tr><tr><td>epsilon_min</td><td>86.46394</td></tr><tr><td>epsilon_std</td><td>149.04413</td></tr><tr><td>noise_multiplier</td><td>0.3</td></tr><tr><td>participating_clients</td><td>8</td></tr><tr><td>reset_count</td><td>4</td></tr><tr><td>round</td><td>4</td></tr><tr><td>timeout_clients</td><td>0</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ClusteredFL_enhanced_s42</strong> at: <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/sk3cs3y8' target=\"_blank\">https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/sk3cs3y8</a><br> View project at: <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced' target=\"_blank\">https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250709_112108-sk3cs3y8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250709_113812-o4opiqvk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/o4opiqvk' target=\"_blank\">FedProx_enhanced_s42</a></strong> to <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced' target=\"_blank\">https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/o4opiqvk' target=\"_blank\">https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/o4opiqvk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task was destroyed but it is pending!\n",
            "task: <Task cancelling name='Task-8' coro=<Event.wait() running at /usr/lib/python3.11/asyncio/locks.py:213> wait_for=<Future cancelled>>\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bf8e89fa08f47eaaa8a9bde801ba958",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FedProx Training:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:\n",
            "==================== âš ï¸ éš±ç§é ç®—é¦–æ¬¡è¶…æ”¯ï¼ ====================\n",
            "WARNING:__main__:   - ç•¶å‰æ¶ˆè€—: Îµ = 121.4208\n",
            "WARNING:__main__:   - é ç®—ä¸Šé™: Îµ = 8.0\n",
            "WARNING:__main__:==========================================================\n",
            "WARNING:__main__:Round 1 å»¶é²è­¦å‘Š: 22.32s > 1.0s\n",
            "WARNING:__main__:\n",
            "==================== âš ï¸ éš±ç§é ç®—é¦–æ¬¡è¶…æ”¯ï¼ ====================\n",
            "WARNING:__main__:   - ç•¶å‰æ¶ˆè€—: Îµ = 242.4837\n",
            "WARNING:__main__:   - é ç®—ä¸Šé™: Îµ = 8.0\n",
            "WARNING:__main__:==========================================================\n",
            "WARNING:__main__:[C-0] éš±ç§é ç®—æ¶ˆè€—éå¿«: 239.2409/è¼ª\n",
            "WARNING:__main__:[C-3] éš±ç§é ç®—æ¶ˆè€—éå¿«: 254.9252/è¼ª\n",
            "WARNING:__main__:[C-5] éš±ç§é ç®—æ¶ˆè€—éå¿«: 82.6303/è¼ª\n",
            "WARNING:__main__:[C-6] éš±ç§é ç®—æ¶ˆè€—éå¿«: 67.8638/è¼ª\n",
            "WARNING:__main__:Round 2 å»¶é²è­¦å‘Š: 41.05s > 1.0s\n",
            "WARNING:__main__:\n",
            "==================== âš ï¸ éš±ç§é ç®—é¦–æ¬¡è¶…æ”¯ï¼ ====================\n",
            "WARNING:__main__:   - ç•¶å‰æ¶ˆè€—: Îµ = 245.0103\n",
            "WARNING:__main__:   - é ç®—ä¸Šé™: Îµ = 8.0\n",
            "WARNING:__main__:==========================================================\n",
            "WARNING:__main__:[C-0] éš±ç§é ç®—æ¶ˆè€—éå¿«: 254.9252/è¼ª\n",
            "WARNING:__main__:[C-1] éš±ç§é ç®—æ¶ˆè€—éå¿«: 239.2409/è¼ª\n",
            "WARNING:__main__:[C-2] éš±ç§é ç®—æ¶ˆè€—éå¿«: 64.5039/è¼ª\n",
            "WARNING:__main__:[C-4] éš±ç§é ç®—æ¶ˆè€—éå¿«: 64.5039/è¼ª\n",
            "WARNING:__main__:[C-5] éš±ç§é ç®—æ¶ˆè€—éå¿«: 64.6627/è¼ª\n",
            "WARNING:__main__:[C-7] éš±ç§é ç®—æ¶ˆè€—éå¿«: 211.1987/è¼ª\n",
            "WARNING:__main__:Round 3 å»¶é²è­¦å‘Š: 64.09s > 1.0s\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "360e07cb76354723abe6d1675a5e50b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "æœ€çµ‚è©•ä¼°:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:P95å»¶é² 60.630s è¶…éSLA 0.05s\n",
            "WARNING:__main__:P95å»¶é² 60.630s è¶…éSLA 0.05s\n",
            "WARNING:__main__:P95å»¶é² 60.630s è¶…éSLA 0.05s\n",
            "WARNING:__main__:   - âš ï¸ è¶…é1ç§’è­¦å‘Š: 3æ¬¡\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "åŸ·è¡Œå¯¦é©—: FedAvg\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>â–â–†â–ˆâ–†</td></tr><tr><td>avg_reward</td><td>â–ˆâ–â–‡â–</td></tr><tr><td>budget_ratio</td><td>â–â–â–â–</td></tr><tr><td>cluster_reassign_count</td><td>â–â–â–â–</td></tr><tr><td>cumulative_epsilon</td><td>â–â–â–â–</td></tr><tr><td>delta</td><td>â–â–â–â–</td></tr><tr><td>epsilon</td><td>â–â–„â–ˆâ–ˆ</td></tr><tr><td>epsilon_max</td><td>â–â–ˆâ–ˆ</td></tr><tr><td>epsilon_min</td><td>â–â–ˆâ–„</td></tr><tr><td>epsilon_std</td><td>â–â–‡â–ˆ</td></tr><tr><td>noise_multiplier</td><td>â–â–â–</td></tr><tr><td>participating_clients</td><td>â–ˆâ–â–ˆâ–…â–ˆâ–…â–ˆâ–‡</td></tr><tr><td>reset_count</td><td>â–â–ƒâ–†â–ˆ</td></tr><tr><td>round</td><td>â–â–â–ƒâ–ƒâ–†â–†â–ˆâ–ˆ</td></tr><tr><td>timeout_clients</td><td>â–…â–â–ˆâ–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>async_aggregation</td><td>True</td></tr><tr><td>avg_loss</td><td>45489740.21381</td></tr><tr><td>avg_reward</td><td>160.54551</td></tr><tr><td>budget_ratio</td><td>0</td></tr><tr><td>cluster_reassign_count</td><td>0</td></tr><tr><td>cumulative_epsilon</td><td>0</td></tr><tr><td>delta</td><td>1e-05</td></tr><tr><td>epsilon</td><td>245.01035</td></tr><tr><td>epsilon_max</td><td>494.16606</td></tr><tr><td>epsilon_min</td><td>64.50385</td></tr><tr><td>epsilon_std</td><td>169.05093</td></tr><tr><td>noise_multiplier</td><td>0.3</td></tr><tr><td>participating_clients</td><td>7</td></tr><tr><td>reset_count</td><td>3</td></tr><tr><td>round</td><td>3</td></tr><tr><td>timeout_clients</td><td>1</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FedProx_enhanced_s42</strong> at: <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/o4opiqvk' target=\"_blank\">https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/o4opiqvk</a><br> View project at: <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced' target=\"_blank\">https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250709_113812-o4opiqvk/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250709_115244-ty8f3uio</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/ty8f3uio' target=\"_blank\">FedAvg_enhanced_s42</a></strong> to <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced' target=\"_blank\">https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/ty8f3uio' target=\"_blank\">https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/ty8f3uio</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task was destroyed but it is pending!\n",
            "task: <Task cancelling name='Task-21' coro=<Event.wait() running at /usr/lib/python3.11/asyncio/locks.py:213> wait_for=<Future cancelled>>\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54e3f5c77e3b4d848bc7d032ea4eb83e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FedAvg Training:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:\n",
            "==================== âš ï¸ éš±ç§é ç®—é¦–æ¬¡è¶…æ”¯ï¼ ====================\n",
            "WARNING:__main__:   - ç•¶å‰æ¶ˆè€—: Îµ = 121.4208\n",
            "WARNING:__main__:   - é ç®—ä¸Šé™: Îµ = 8.0\n",
            "WARNING:__main__:==========================================================\n",
            "WARNING:__main__:Round 1 å»¶é²è­¦å‘Š: 19.07s > 1.0s\n",
            "WARNING:__main__:\n",
            "==================== âš ï¸ éš±ç§é ç®—é¦–æ¬¡è¶…æ”¯ï¼ ====================\n",
            "WARNING:__main__:   - ç•¶å‰æ¶ˆè€—: Îµ = 242.4837\n",
            "WARNING:__main__:   - é ç®—ä¸Šé™: Îµ = 8.0\n",
            "WARNING:__main__:==========================================================\n",
            "WARNING:__main__:[C-0] éš±ç§é ç®—æ¶ˆè€—éå¿«: 239.2409/è¼ª\n",
            "WARNING:__main__:[C-3] éš±ç§é ç®—æ¶ˆè€—éå¿«: 254.9252/è¼ª\n",
            "WARNING:__main__:[C-5] éš±ç§é ç®—æ¶ˆè€—éå¿«: 82.6303/è¼ª\n",
            "WARNING:__main__:[C-6] éš±ç§é ç®—æ¶ˆè€—éå¿«: 67.8638/è¼ª\n",
            "WARNING:__main__:Round 2 å»¶é²è­¦å‘Š: 36.52s > 1.0s\n",
            "WARNING:__main__:\n",
            "==================== âš ï¸ éš±ç§é ç®—é¦–æ¬¡è¶…æ”¯ï¼ ====================\n",
            "WARNING:__main__:   - ç•¶å‰æ¶ˆè€—: Îµ = 245.0103\n",
            "WARNING:__main__:   - é ç®—ä¸Šé™: Îµ = 8.0\n",
            "WARNING:__main__:==========================================================\n",
            "WARNING:__main__:[C-0] éš±ç§é ç®—æ¶ˆè€—éå¿«: 254.9252/è¼ª\n",
            "WARNING:__main__:[C-1] éš±ç§é ç®—æ¶ˆè€—éå¿«: 239.2409/è¼ª\n",
            "WARNING:__main__:[C-2] éš±ç§é ç®—æ¶ˆè€—éå¿«: 64.5039/è¼ª\n",
            "WARNING:__main__:[C-4] éš±ç§é ç®—æ¶ˆè€—éå¿«: 64.5039/è¼ª\n",
            "WARNING:__main__:[C-5] éš±ç§é ç®—æ¶ˆè€—éå¿«: 64.6627/è¼ª\n",
            "WARNING:__main__:[C-7] éš±ç§é ç®—æ¶ˆè€—éå¿«: 211.1987/è¼ª\n",
            "WARNING:__main__:Round 3 å»¶é²è­¦å‘Š: 61.03s > 1.0s\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f5b5014fa934425aafbce2b61300ec2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "æœ€çµ‚è©•ä¼°:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:P95å»¶é² 57.351s è¶…éSLA 0.05s\n",
            "WARNING:__main__:P95å»¶é² 57.351s è¶…éSLA 0.05s\n",
            "WARNING:__main__:P95å»¶é² 57.351s è¶…éSLA 0.05s\n",
            "WARNING:__main__:   - âš ï¸ è¶…é1ç§’è­¦å‘Š: 3æ¬¡\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "åŸ·è¡Œå¯¦é©—: CQL\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>â–â–†â–ˆâ–†</td></tr><tr><td>avg_reward</td><td>â–ˆâ–â–‡â–</td></tr><tr><td>budget_ratio</td><td>â–â–â–â–</td></tr><tr><td>cluster_reassign_count</td><td>â–â–â–â–</td></tr><tr><td>cumulative_epsilon</td><td>â–â–â–â–</td></tr><tr><td>delta</td><td>â–â–â–â–</td></tr><tr><td>epsilon</td><td>â–â–„â–ˆâ–ˆ</td></tr><tr><td>epsilon_max</td><td>â–â–ˆâ–ˆ</td></tr><tr><td>epsilon_min</td><td>â–â–ˆâ–„</td></tr><tr><td>epsilon_std</td><td>â–â–‡â–ˆ</td></tr><tr><td>noise_multiplier</td><td>â–â–â–</td></tr><tr><td>participating_clients</td><td>â–ˆâ–â–ˆâ–…â–ˆâ–…â–ˆâ–‡</td></tr><tr><td>reset_count</td><td>â–â–ƒâ–†â–ˆ</td></tr><tr><td>round</td><td>â–â–â–ƒâ–ƒâ–†â–†â–ˆâ–ˆ</td></tr><tr><td>timeout_clients</td><td>â–…â–â–ˆâ–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>async_aggregation</td><td>True</td></tr><tr><td>avg_loss</td><td>45505907.53714</td></tr><tr><td>avg_reward</td><td>160.5501</td></tr><tr><td>budget_ratio</td><td>0</td></tr><tr><td>cluster_reassign_count</td><td>0</td></tr><tr><td>cumulative_epsilon</td><td>0</td></tr><tr><td>delta</td><td>1e-05</td></tr><tr><td>epsilon</td><td>245.01035</td></tr><tr><td>epsilon_max</td><td>494.16606</td></tr><tr><td>epsilon_min</td><td>64.50385</td></tr><tr><td>epsilon_std</td><td>169.05093</td></tr><tr><td>noise_multiplier</td><td>0.3</td></tr><tr><td>participating_clients</td><td>7</td></tr><tr><td>reset_count</td><td>3</td></tr><tr><td>round</td><td>3</td></tr><tr><td>timeout_clients</td><td>1</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FedAvg_enhanced_s42</strong> at: <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/ty8f3uio' target=\"_blank\">https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/ty8f3uio</a><br> View project at: <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced' target=\"_blank\">https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250709_115244-ty8f3uio/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250709_120706-9a4fqeuu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/9a4fqeuu' target=\"_blank\">CQL_enhanced_s42</a></strong> to <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced' target=\"_blank\">https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/9a4fqeuu' target=\"_blank\">https://wandb.ai/thc1006-national-yang-ming-chiao-tung-university/federated-rl-enhanced/runs/9a4fqeuu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task was destroyed but it is pending!\n",
            "task: <Task cancelling name='Task-34' coro=<Event.wait() running at /usr/lib/python3.11/asyncio/locks.py:213> wait_for=<Future cancelled>>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "CQL Training:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fbb01d1a665496c90059b6eacb47968"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Round 1 å»¶é²è­¦å‘Š: 12.71s > 1.0s\n",
            "WARNING:__main__:Round 2 å»¶é²è­¦å‘Š: 32.37s > 1.0s\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 8C: ğŸš€ åŸ·è¡Œç¤ºä¾‹å¯¦é©—ï¼ˆ5è¼ªæ¼”ç¤ºï¼‰\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸš€ é–‹å§‹åŸ·è¡Œå¼·åŒ–ç‰ˆå¯¦é©—ï¼ˆ5è¼ªç¤ºä¾‹ï¼‰\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# åŸºç¤é…ç½®ï¼ˆ5è¼ªç¤ºä¾‹ï¼Œ20åˆ†é˜å…§å®Œæˆï¼‰\n",
        "base_config = {\n",
        "    \"random_seed\": 42,\n",
        "    \"comm_rounds\": 5,  # æ¸›å°‘åˆ°5è¼ªä»¥ç¬¦åˆè¦æ±‚\n",
        "    \"output_dir\": \"\",  # å°‡åœ¨æ¯å€‹å¯¦é©—ä¸­è¨­å®š\n",
        "\n",
        "    # Non-IID é…ç½®\n",
        "    \"enable_non_iid\": True,\n",
        "    \"dirichlet_alpha\": 0.5,\n",
        "    \"min_clients_non_iid\": 10,\n",
        "\n",
        "    # è¨“ç·´åƒæ•¸ï¼ˆç‚º5è¼ªå„ªåŒ–ï¼‰\n",
        "    \"local_episodes_per_round\": 2,\n",
        "    \"steps_per_episode\": 300,\n",
        "    \"batch_size\": 128,\n",
        "    \"memory_capacity\": 30000,\n",
        "\n",
        "    # å·®åˆ†éš±ç§\n",
        "    \"enable_dp\": True,\n",
        "    \"dp_target_epsilon\": 8.0,\n",
        "    \"dp_noise_multiplier\": 0.3,\n",
        "    \"enable_adaptive_clip\": True,\n",
        "    \"clip_window\": 50,\n",
        "    \"clip_percentile\": 0.9,\n",
        "\n",
        "    # éåŒæ­¥èšåˆ\n",
        "    \"enable_async_aggregation\": True,\n",
        "    \"timeout_threshold\": 0.3,\n",
        "    \"client_timeout_seconds\": 30.0,\n",
        "\n",
        "    # ClusteredFL å‹•æ…‹åƒæ•¸\n",
        "    \"base_cluster_freq\": 2,\n",
        "    \"cluster_decay_rate\": 0.1,\n",
        "    \"early_stopping_rounds\": 2,\n",
        "    \"use_cosine_similarity\": True,\n",
        "    \"knowledge_transfer\": True,\n",
        "\n",
        "    # FedAvg/FedProx å‹•æ…‹åƒæ•¸\n",
        "    \"use_weighted_fedavg\": True,\n",
        "    \"fedprox_mu_adaptive\": True,\n",
        "\n",
        "    # å»¶é²ç›£æ§\n",
        "    \"enable_latency_monitor\": True,\n",
        "\n",
        "    # DPå„ªåŒ–\n",
        "    \"enable_l2_regularization\": False,\n",
        "    \"use_rdp_accountant\": True,\n",
        "\n",
        "    # å…¶ä»–åƒæ•¸\n",
        "    \"lr\": 1e-4,\n",
        "    \"gamma\": 0.99,\n",
        "    \"enable_compression\": True,\n",
        "    \"use_pfl_finetune\": True,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "}\n",
        "\n",
        "# åŸ·è¡Œæ¯å€‹æ¨¡å¼\n",
        "for mode in MODES_TO_RUN:\n",
        "    seed = SEEDS[0]\n",
        "    exp_name = f\"{mode}_enhanced_s{seed}\"\n",
        "    output_dir = os.path.join(BASE_OUTPUT_DIR, f\"seed_{seed}\", mode)\n",
        "\n",
        "    # æ›´æ–°é…ç½®\n",
        "    config_params = base_config.copy()\n",
        "    config_params.update({\n",
        "        \"experiment_name\": exp_name,\n",
        "        \"output_dir\": output_dir,\n",
        "        \"mode\": mode,\n",
        "\n",
        "        # æ¨¡å¼ç‰¹å®šåƒæ•¸\n",
        "        \"enable_cql\": (mode == \"CQL\"),\n",
        "        \"cql_alpha\": 1.0 if mode == \"CQL\" else 0.0,\n",
        "        \"cql_min_q_weight\": 5.0 if mode == \"CQL\" else 0.0,\n",
        "        \"cql_loss_alpha_scaler\": 0.1 if mode == \"CQL\" else 0.0,\n",
        "        \"use_mixed_precision\": (mode == \"CQL\"),  # CQLå•Ÿç”¨æ··åˆç²¾åº¦\n",
        "        \"fedprox_mu\": 0.15 if mode in [\"FedProx\", \"ClusteredFL\"] else 0.0,\n",
        "        \"num_clusters\": 3 if mode == \"ClusteredFL\" else 1,\n",
        "        \"cluster_update_freq\": 8 if mode == \"ClusteredFL\" else 999,  # ClusteredFLå‹•æ…‹æ›´æ–°\n",
        "\n",
        "        # L2æ­£å‰‡åŒ–å°ç…§çµ„ï¼ˆç”¨æ–¼FedAvgï¼‰\n",
        "        \"enable_l2_regularization\": (mode == \"FedAvg\"),\n",
        "        \"weight_decay\": 1e-4 if mode == \"FedAvg\" else 0.0,\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"åŸ·è¡Œå¯¦é©—: {mode}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    logger.info(f\"ğŸ”§ é…ç½®æ‘˜è¦:\")\n",
        "    logger.info(f\"   - Non-IID: å•Ÿç”¨ (Î±={config_params['dirichlet_alpha']})\")\n",
        "    logger.info(f\"   - Adaptive Clipping: å•Ÿç”¨\")\n",
        "    logger.info(f\"   - éåŒæ­¥èšåˆ: å•Ÿç”¨\")\n",
        "    logger.info(f\"   - é€šä¿¡è¼ªæ•¸: {config_params['comm_rounds']}\")\n",
        "    if mode == \"CQL\":\n",
        "        logger.info(f\"   - CQLåƒæ•¸: Î±={config_params['cql_alpha']}, min_q_weight={config_params['cql_min_q_weight']}\")\n",
        "    if mode == \"ClusteredFL\":\n",
        "        logger.info(f\"   - å‹•æ…‹èšé¡: åŸºç¤é »ç‡={config_params['base_cluster_freq']}, è¡°æ¸›ç‡={config_params['cluster_decay_rate']}\")\n",
        "\n",
        "    # åŸ·è¡Œå¯¦é©—\n",
        "    start_time = time.time()\n",
        "    success, result_info = run_enhanced_experiment(config_params, DATA_PATH)\n",
        "\n",
        "    if success:\n",
        "        experiment_results.append({\n",
        "            'seed': seed,\n",
        "            'mode': mode,\n",
        "            'success': True,\n",
        "            'result_info': result_info\n",
        "        })\n",
        "        logger.info(f\"\\nâœ… {mode} å¯¦é©—æˆåŠŸ\")\n",
        "\n",
        "        # é¡¯ç¤ºé—œéµæŒ‡æ¨™\n",
        "        if result_info and 'avg_rewards' in result_info:\n",
        "            rewards = result_info['avg_rewards']\n",
        "            logger.info(f\"   - å…¨åŸŸçå‹µ: {rewards['global']:.4f}\")\n",
        "            logger.info(f\"   - å€‹äººåŒ–çå‹µ: {rewards['personalized']:.4f}\")\n",
        "\n",
        "            if result_info.get('privacy_stats'):\n",
        "                privacy = result_info['privacy_stats']\n",
        "                logger.info(f\"   - Îµæ¶ˆè€—: {privacy['consumed_epsilon']:.4f}\")\n",
        "\n",
        "            if result_info.get('latency_stats'):\n",
        "                latency = result_info['latency_stats']\n",
        "                if latency and 'mean_time' in latency:\n",
        "                    logger.info(f\"   - å¹³å‡å»¶é²: {latency['mean_time']:.2f}s\")\n",
        "                    # æª¢æŸ¥æ˜¯å¦ç¬¦åˆ<40sç›®æ¨™\n",
        "                    if latency['mean_time'] < 40:\n",
        "                        logger.info(f\"   - âœ“ å»¶é²ç›®æ¨™é”æˆ (<40s)\")\n",
        "                    else:\n",
        "                        logger.warning(f\"   - âœ— å»¶é²è¶…æ¨™ (>40s)\")\n",
        "    else:\n",
        "        experiment_results.append({\n",
        "            'seed': seed,\n",
        "            'mode': mode,\n",
        "            'success': False,\n",
        "            'result_info': None\n",
        "        })\n",
        "        logger.error(f\"\\nâŒ {mode} å¯¦é©—å¤±æ•—\")\n",
        "\n",
        "    execution_time = (time.time() - start_time) / 60\n",
        "    logger.info(f\"â±ï¸ åŸ·è¡Œæ™‚é–“: {execution_time:.2f} åˆ†é˜\")\n",
        "\n",
        "    # æª¢æŸ¥æ˜¯å¦åœ¨20åˆ†é˜å…§\n",
        "    total_elapsed = (time.time() - start_time) / 60\n",
        "    if total_elapsed > 20:\n",
        "        logger.warning(f\"âš ï¸ ç¸½åŸ·è¡Œæ™‚é–“å·²è¶…é20åˆ†é˜ï¼Œåœæ­¢å‰©é¤˜å¯¦é©—\")\n",
        "        break\n",
        "\n",
        "    # æ¸…ç†è³‡æº\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ç¤ºä¾‹å¯¦é©—å®Œæˆ\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sl1huA1gEIm3"
      },
      "outputs": [],
      "source": [
        "# @title Cell 8D: ğŸ“Š å¯¦é©—ç¸½çµèˆ‡åˆ†æ\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ‰ å¯¦é©—åŸ·è¡Œå®Œæˆï¼\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# çµ±è¨ˆæˆåŠŸç‡\n",
        "successful_experiments = sum(1 for r in experiment_results if r['success'])\n",
        "total_experiments = len(experiment_results)\n",
        "success_rate = (successful_experiments / total_experiments) * 100 if total_experiments > 0 else 0\n",
        "\n",
        "logger.info(f\"\\nğŸ“Š å¯¦é©—çµ±è¨ˆ:\")\n",
        "logger.info(f\"   - ç¸½å¯¦é©—æ•¸: {total_experiments}\")\n",
        "logger.info(f\"   - æˆåŠŸæ•¸: {successful_experiments}\")\n",
        "logger.info(f\"   - æˆåŠŸç‡: {success_rate:.1f}%\")\n",
        "\n",
        "# åˆå§‹åŒ– pareto_achieved åˆ—è¡¨\n",
        "pareto_achieved = []\n",
        "\n",
        "# æˆåŠŸå¯¦é©—çš„è©³ç´°çµ±è¨ˆ\n",
        "successful_results = [r for r in experiment_results if r['success']]\n",
        "if successful_results:\n",
        "    logger.info(f\"\\nğŸ“ˆ æˆåŠŸå¯¦é©—è©³ç´°çµæœ:\")\n",
        "\n",
        "    for result in successful_results:\n",
        "        mode = result['mode']\n",
        "        info = result['result_info']\n",
        "        if info and 'avg_rewards' in info:\n",
        "            rewards = info['avg_rewards']\n",
        "            print(f\"\\n   {mode}:\")\n",
        "            print(f\"     - å…¨åŸŸçå‹µ: {rewards['global']:.4f}\")\n",
        "            print(f\"     - å€‹äººåŒ–çå‹µ: {rewards['personalized']:.4f}\")\n",
        "            print(f\"     - PFLçå‹µ: {rewards['pfl']:.4f}\")\n",
        "            print(f\"     - åŸ·è¡Œæ™‚é–“: {info['execution_time']:.2f} åˆ†é˜\")\n",
        "\n",
        "            # æª¢æŸ¥Paretoç›®æ¨™\n",
        "            pareto_check = {\n",
        "                'mode': mode,\n",
        "                'latency_ok': False,\n",
        "                'epsilon_ok': False,\n",
        "                'reward_ok': False\n",
        "            }\n",
        "\n",
        "            if info.get('privacy_stats'):\n",
        "                privacy = info['privacy_stats']\n",
        "                print(f\"     - Îµæ¶ˆè€—: {privacy['consumed_epsilon']:.4f}\")\n",
        "                pareto_check['epsilon_ok'] = privacy['consumed_epsilon'] < 4\n",
        "                if privacy['consumed_epsilon'] > 0:\n",
        "                    print(f\"     - éš±ç§æ•ˆç‡: {rewards['pfl']/privacy['consumed_epsilon']:.4f} (çå‹µ/Îµ)\")\n",
        "\n",
        "            if info.get('latency_stats') and info['latency_stats']:\n",
        "                latency = info['latency_stats']\n",
        "                if 'mean_time' in latency:\n",
        "                    print(f\"     - å¹³å‡å»¶é²: {latency['mean_time']:.2f}s\")\n",
        "                    pareto_check['latency_ok'] = latency['mean_time'] < 40\n",
        "                if 'p95_time' in latency:\n",
        "                    print(f\"     - 95åˆ†ä½å»¶é²: {latency['p95_time']:.2f}s\")\n",
        "                    if latency.get('warnings', 0) > 0:\n",
        "                        print(f\"     - âš ï¸ å»¶é²è­¦å‘Š: {latency['warnings']}æ¬¡\")\n",
        "\n",
        "            pareto_check['reward_ok'] = rewards['pfl'] > 165\n",
        "\n",
        "            # æª¢æŸ¥æ˜¯å¦é”æˆæ‰€æœ‰Paretoç›®æ¨™\n",
        "            if all([pareto_check['latency_ok'], pareto_check['epsilon_ok'], pareto_check['reward_ok']]):\n",
        "                pareto_achieved.append(mode)\n",
        "                print(f\"     - âœ… Paretoç›®æ¨™å…¨éƒ¨é”æˆï¼\")\n",
        "            else:\n",
        "                failed_targets = []\n",
        "                if not pareto_check['latency_ok']:\n",
        "                    failed_targets.append(\"å»¶é²<40s\")\n",
        "                if not pareto_check['epsilon_ok']:\n",
        "                    failed_targets.append(\"Îµ<4\")\n",
        "                if not pareto_check['reward_ok']:\n",
        "                    failed_targets.append(\"çå‹µ>165\")\n",
        "                print(f\"     - âš ï¸ æœªé”æˆç›®æ¨™: {', '.join(failed_targets)}\")\n",
        "\n",
        "    # Paretoæˆå°±ç¸½çµ\n",
        "    if pareto_achieved:\n",
        "        print(f\"\\nğŸ† é”æˆParetoç›®æ¨™çš„æ¨¡å¼: {', '.join(pareto_achieved)}\")\n",
        "    else:\n",
        "        print(f\"\\nâš ï¸ æ²’æœ‰æ¨¡å¼å®Œå…¨é”æˆParetoç›®æ¨™\")\n",
        "\n",
        "print(f\"\\nğŸ”§ å¼·åŒ–åŠŸèƒ½ç¸½çµ:\")\n",
        "print(f\"   - Non-IID æ•¸æ“šåˆ†é…: âœ… å·²å¯¦æ–½ (Dirichlet Î±=0.5)\")\n",
        "print(f\"   - CQL é›¢ç·š RL: âœ… å·²æ•´åˆ\")\n",
        "print(f\"   - Adaptive Clipping: âœ… å·²å•Ÿç”¨\")\n",
        "print(f\"   - éåŒæ­¥èšåˆ: âœ… å·²æ”¯æ´\")\n",
        "print(f\"   - å»¶é²ç›£æ§: âœ… å·²è¨˜éŒ„\")\n",
        "print(f\"   - Deterministic æ¨¡å¼: âœ… å·²è¨­å®š\")\n",
        "print(f\"   - å‹•æ…‹èšé¡æ›´æ–°: âœ… å·²å¯¦æ–½\")\n",
        "print(f\"   - FedAvgåŠ æ¬Šèšåˆ: âœ… å·²å¯¦æ–½\")\n",
        "print(f\"   - FedProxå‹•æ…‹mu: âœ… å·²å¯¦æ–½\")\n",
        "print(f\"   - DPå„ªåŒ– (RDP/L2/Dropout): âœ… å·²å¯¦æ–½\")\n",
        "\n",
        "# ä¿å­˜å¯¦é©—ç¸½çµ\n",
        "import json\n",
        "summary_path = os.path.join(BASE_OUTPUT_DIR, \"experiment_summary_enhanced.json\")\n",
        "os.makedirs(os.path.dirname(summary_path), exist_ok=True)\n",
        "\n",
        "with open(summary_path, 'w') as f:\n",
        "    # å‰µå»ºä¸€å€‹å¯åºåˆ—åŒ–çš„ç‰ˆæœ¬\n",
        "    serializable_results = []\n",
        "    for r in experiment_results:\n",
        "        res_copy = r.copy()\n",
        "        if res_copy.get('result_info') and res_copy['result_info'].get('avg_rewards'):\n",
        "             # å°‡ numpy float è½‰æ›ç‚º Python float\n",
        "            for key, val in res_copy['result_info']['avg_rewards'].items():\n",
        "                res_copy['result_info']['avg_rewards'][key] = float(val)\n",
        "        serializable_results.append(res_copy)\n",
        "\n",
        "    json.dump({\n",
        "        'total_experiments': total_experiments,\n",
        "        'successful_experiments': successful_experiments,\n",
        "        'success_rate': success_rate,\n",
        "        'results': serializable_results,\n",
        "        'enhancements': {\n",
        "            'non_iid': True,\n",
        "            'cql': True,\n",
        "            'adaptive_clipping': True,\n",
        "            'async_aggregation': True,\n",
        "            'latency_monitoring': True,\n",
        "            'deterministic': True,\n",
        "            'dynamic_clustering': True,\n",
        "            'weighted_fedavg': True,\n",
        "            'adaptive_fedprox': True,\n",
        "            'dp_optimizations': True\n",
        "        },\n",
        "        'pareto_achieved': pareto_achieved  # ç¾åœ¨ pareto_achieved å·²å®šç¾©\n",
        "    }, f, indent=4)\n",
        "\n",
        "logger.info(f\"\\nğŸ“„ å¯¦é©—ç¸½çµå·²ä¿å­˜è‡³: {summary_path}\")\n",
        "\n",
        "# è¼¸å‡º SUCCESS æ¨™è¨˜\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUCCESS âœ…\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# è¼¸å‡ºåœ–æª”è·¯å¾‘åˆ—è¡¨ï¼ˆå°‡åœ¨ä¸‹ä¸€å€‹cellç”Ÿæˆï¼‰\n",
        "print(\"\\nğŸ“Š æº–å‚™ç”Ÿæˆä»¥ä¸‹åœ–è¡¨:\")\n",
        "print(\"1. training_history_enhanced.png - è¨“ç·´æ­·å²æ¯”è¼ƒ\")\n",
        "print(\"2. reward_vs_epsilon.png - Reward vs Privacy Budget\")\n",
        "print(\"3. latency_analysis.png - å»¶é²åˆ†æ\")\n",
        "print(\"4. non_iid_client_rewards.png - Non-IIDå®¢æˆ¶ç«¯çå‹µåˆ†å¸ƒ\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# NEW-CELL: é©—æ”¶è‡ªå‹•æª¢æŸ¥\n",
        "def run_validation_checks(eval_df, privacy_df, latency_df, history_df):\n",
        "    \"\"\"åŸ·è¡Œé©—æ”¶è‡ªå‹•æª¢æŸ¥\"\"\"\n",
        "    print(\"\\nğŸ§ª åŸ·è¡Œé©—æ”¶æª¢æŸ¥...\")\n",
        "\n",
        "    checks_passed = []\n",
        "\n",
        "    # æª¢æŸ¥1: æŒ‡æ¨™æ¬„ä½å®Œæ•´æ€§\n",
        "    try:\n",
        "        if 'latency_df' in locals() and not latency_df.empty:\n",
        "            required_cols = [\"rtt\", \"local_training\", \"aggregation\"]\n",
        "            if all(col in latency_df.columns or\n",
        "                   any(col in str(latency_df.columns) for col in required_cols)):\n",
        "                print(\"âœ… æª¢æŸ¥1é€šé: å»¶é²æŒ‡æ¨™åŒ…å« rtt, local_training, aggregation\")\n",
        "                checks_passed.append(True)\n",
        "            else:\n",
        "                print(\"âŒ æª¢æŸ¥1å¤±æ•—: ç¼ºå°‘å¿…è¦çš„å»¶é²æŒ‡æ¨™æ¬„ä½\")\n",
        "                checks_passed.append(False)\n",
        "        else:\n",
        "            # å¦‚æœæ²’æœ‰è©³ç´°æ•¸æ“šï¼Œæª¢æŸ¥æ˜¯å¦æœ‰ wall_time\n",
        "            if 'wall_time' in latency_df.columns:\n",
        "                print(\"âœ… æª¢æŸ¥1é€šé: åŒ…å« wall_time æŒ‡æ¨™\")\n",
        "                checks_passed.append(True)\n",
        "            else:\n",
        "                print(\"âš ï¸ æª¢æŸ¥1: ç„¡å»¶é²æ•¸æ“š\")\n",
        "                checks_passed.append(False)\n",
        "    except:\n",
        "        print(\"âš ï¸ æª¢æŸ¥1: ç„¡æ³•åŸ·è¡Œ\")\n",
        "        checks_passed.append(False)\n",
        "\n",
        "    # æª¢æŸ¥2: éš±ç§é ç®—ç¯„åœ\n",
        "    try:\n",
        "        if not privacy_df.empty and 'cumulative_epsilon' in privacy_df.columns:\n",
        "            max_epsilon = privacy_df['cumulative_epsilon'].max()\n",
        "            if 0 < max_epsilon < 10:\n",
        "                print(f\"âœ… æª¢æŸ¥2é€šé: éš±ç§é ç®— Îµ={max_epsilon:.4f} åœ¨åˆç†ç¯„åœå…§\")\n",
        "                checks_passed.append(True)\n",
        "            else:\n",
        "                print(f\"âŒ æª¢æŸ¥2å¤±æ•—: éš±ç§é ç®— Îµ={max_epsilon:.4f} è¶…å‡ºç¯„åœ\")\n",
        "                checks_passed.append(False)\n",
        "        else:\n",
        "            print(\"âš ï¸ æª¢æŸ¥2: ç„¡éš±ç§æ•¸æ“š\")\n",
        "            checks_passed.append(False)\n",
        "    except:\n",
        "        print(\"âš ï¸ æª¢æŸ¥2: ç„¡æ³•åŸ·è¡Œ\")\n",
        "        checks_passed.append(False)\n",
        "\n",
        "    # æª¢æŸ¥3: å…¬å¹³æ€§æŒ‡æ¨™\n",
        "    try:\n",
        "        if not eval_df.empty and 'reward_pfl_finetuned' in eval_df.columns:\n",
        "            # æª¢æŸ¥æ˜¯å¦æœ‰ jain, gini, theil\n",
        "            has_fairness = any(col in eval_df.columns for col in ['jain', 'gini', 'theil'])\n",
        "\n",
        "            # è¨ˆç®— robust sigma\n",
        "            rewards = eval_df['reward_pfl_finetuned'].dropna()\n",
        "            if len(rewards) > 0:\n",
        "                q75, q25 = np.percentile(rewards, [75, 25])\n",
        "                robust_sigma = (q75 - q25) / 1.349\n",
        "\n",
        "                if has_fairness:\n",
        "                    print(f\"âœ… æª¢æŸ¥3é€šé: åŒ…å«å…¬å¹³æ€§æŒ‡æ¨™ï¼Œrobust Ïƒ={robust_sigma:.2f}\")\n",
        "                else:\n",
        "                    print(f\"âœ… æª¢æŸ¥3éƒ¨åˆ†é€šé: robust Ïƒ={robust_sigma:.2f}ï¼Œä½†ç¼ºå°‘ Jain/Gini/Theil\")\n",
        "                checks_passed.append(True)\n",
        "            else:\n",
        "                print(\"âŒ æª¢æŸ¥3å¤±æ•—: ç„¡çå‹µæ•¸æ“š\")\n",
        "                checks_passed.append(False)\n",
        "        else:\n",
        "            print(\"âš ï¸ æª¢æŸ¥3: ç„¡è©•ä¼°æ•¸æ“š\")\n",
        "            checks_passed.append(False)\n",
        "    except:\n",
        "        print(\"âš ï¸ æª¢æŸ¥3: ç„¡æ³•åŸ·è¡Œ\")\n",
        "        checks_passed.append(False)\n",
        "\n",
        "    # ç¸½çµ\n",
        "    total_passed = sum(checks_passed)\n",
        "    total_checks = len(checks_passed)\n",
        "\n",
        "    print(f\"\\né©—æ”¶çµæœ: {total_passed}/{total_checks} æª¢æŸ¥é€šé\")\n",
        "\n",
        "    if total_passed == total_checks:\n",
        "        print(\"âœ… å…¨éƒ¨é€šé\")\n",
        "    else:\n",
        "        print(\"âš ï¸ éƒ¨åˆ†æª¢æŸ¥æœªé€šéï¼Œè«‹æª¢æŸ¥å¯¦é©—é…ç½®\")\n",
        "\n",
        "    return total_passed == total_checks\n",
        "\n",
        "# åœ¨ Cell 9 æœ€å¾ŒåŠ å…¥é©—æ”¶æª¢æŸ¥\n",
        "if 'eval_df' in locals() and 'privacy_df' in locals() and 'latency_df' in locals():\n",
        "    validation_passed = run_validation_checks(eval_df, privacy_df, latency_df, history_df)\n",
        "else:\n",
        "    print(\"âš ï¸ ç¼ºå°‘å¿…è¦çš„æ•¸æ“šæ¡†ï¼Œç„¡æ³•åŸ·è¡Œé©—æ”¶æª¢æŸ¥\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cyfEYeMsjoAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Kf3AhPa4ELIf"
      },
      "outputs": [],
      "source": [
        "# @title Cell 9: ğŸ“Š å¯¦é©—çµæœæ·±åº¦åˆ†æèˆ‡è¦–è¦ºåŒ–ï¼ˆä¿®æ­£èˆ‡æœ€ä½³åŒ–ç‰ˆï¼‰\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import logging\n",
        "import traceback\n",
        "\n",
        "# --- è¦–è¦ºåŒ–è¨­å®š ---\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n",
        "FIGURES_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, \"figures_optimized\")\n",
        "os.makedirs(FIGURES_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# å®šç¾©å›ºå®šçš„èª¿è‰²æ¿èˆ‡é †åºï¼Œç¢ºä¿æ‰€æœ‰åœ–è¡¨é¡è‰²ä¸€è‡´\n",
        "PALETTE = {\n",
        "    \"ClusteredFL\": \"#1f77b4\",\n",
        "    \"FedProx\": \"#ff7f0e\",\n",
        "    \"FedAvg\": \"#2ca02c\",\n",
        "    \"CQL\": \"#d62728\",\n",
        "    \"Isolated\": \"#9467bd\",\n",
        "    \"Centralized\": \"#8c564b\"\n",
        "}\n",
        "HUE_ORDER = [\"ClusteredFL\", \"FedProx\", \"FedAvg\", \"CQL\"]\n",
        "\n",
        "# --- 1. çµæœæ•¸æ“šè¼‰å…¥ ---\n",
        "def load_all_results_as_dict(base_output_dir):\n",
        "    \"\"\"\n",
        "    å¾æŒ‡å®šçš„åŸºåº•ç›®éŒ„è¼‰å…¥æ‰€æœ‰å¯¦é©—æ¨¡å¼èˆ‡æ‰€æœ‰éš¨æ©Ÿç¨®å­çš„çµæœã€‚\n",
        "    æ­¤å‡½æ•¸ç¢ºä¿è¿”å›ä¸€å€‹å­—å…¸ï¼Œéµç‚ºæ•¸æ“šé¡å‹ï¼Œå€¼ç‚º DataFrameã€‚\n",
        "    \"\"\"\n",
        "    results_map = {\n",
        "        'eval': ('*_evaluation_results.csv', []),\n",
        "        'history': ('*_training_history.csv', []),\n",
        "        'privacy': ('*_privacy_costs.csv', []),\n",
        "        'latency': ('*_latency_monitor.csv', [])\n",
        "    }\n",
        "\n",
        "    if not os.path.exists(base_output_dir):\n",
        "        logger.error(f\"çµæœç›®éŒ„æœªæ‰¾åˆ°: {base_output_dir}\")\n",
        "        return {key: pd.DataFrame() for key in results_map}\n",
        "\n",
        "    for seed_folder in glob.glob(os.path.join(base_output_dir, 'seed_*')):\n",
        "        try:\n",
        "            seed = int(os.path.basename(seed_folder).split('_')[1])\n",
        "        except (ValueError, IndexError):\n",
        "            continue\n",
        "\n",
        "        for mode_folder in glob.glob(os.path.join(seed_folder, '*')):\n",
        "            mode = os.path.basename(mode_folder)\n",
        "            if not os.path.isdir(mode_folder):\n",
        "                continue\n",
        "\n",
        "            for key, (pattern, data_list) in results_map.items():\n",
        "                file_paths = glob.glob(os.path.join(mode_folder, pattern))\n",
        "                if file_paths:\n",
        "                    try:\n",
        "                        df = pd.read_csv(file_paths[0])\n",
        "                        if not df.empty:\n",
        "                            df['mode'] = mode\n",
        "                            df['seed'] = int(seed)\n",
        "                            data_list.append(df)\n",
        "                    except pd.errors.EmptyDataError:\n",
        "                        logger.warning(f\"æ–‡ä»¶ç‚ºç©ºï¼Œè·³é: {file_paths[0]}\")\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"è®€å–å¤±æ•—: {file_paths[0]}, éŒ¯èª¤: {e}\")\n",
        "\n",
        "    # ä½¿ç”¨å­—å…¸æ¨å°å¼ç¢ºä¿è¿”å›çš„æ˜¯ä¸€å€‹å­—å…¸\n",
        "    return {key: pd.concat(val[1], ignore_index=True) if val[1] else pd.DataFrame()\n",
        "            for key, val in results_map.items()}\n",
        "\n",
        "# --- 2. æ ¸å¿ƒç¹ªåœ–å‡½æ•¸ (èˆ‡å‰ç‰ˆç›¸åŒï¼Œæ­¤è™•ç‚ºæ±‚å®Œæ•´æ€§å†æ¬¡æä¾›) ---\n",
        "def plot_pareto_frontier_analysis(eval_df, privacy_df, latency_df, output_dir, goals):\n",
        "    \"\"\"\n",
        "    ç¹ªè£½ Pareto å‰æ²¿åˆ†æåœ–ï¼Œå±•ç¤ºä¸åŒç­–ç•¥åœ¨ä¸‰å€‹ç›®æ¨™ä¸Šçš„æ¬Šè¡¡ã€‚\n",
        "    \"\"\"\n",
        "    if eval_df.empty or privacy_df.empty or latency_df.empty:\n",
        "        logger.warning(\"ç¼ºå°‘ Pareto åˆ†ææ‰€éœ€çš„æ•¸æ“šï¼Œè·³éæ­¤åœ–è¡¨ã€‚\")\n",
        "        return\n",
        "\n",
        "    # æ•¸æ“šæ•´åˆ\n",
        "    final_eval = eval_df.groupby(['mode', 'seed'])['reward_pfl_finetuned'].mean().reset_index()\n",
        "    final_privacy = privacy_df.groupby(['mode', 'seed'])['cumulative_epsilon'].max().reset_index()\n",
        "    final_latency = latency_df.groupby(['mode', 'seed'])['wall_time'].quantile(0.95).reset_index() # P95 Latency\n",
        "\n",
        "    df = pd.merge(final_eval, final_privacy, on=['mode', 'seed'])\n",
        "    df = pd.merge(df, final_latency, on=['mode', 'seed'])\n",
        "    df = df.rename(columns={\n",
        "        'reward_pfl_finetuned': 'Reward',\n",
        "        'cumulative_epsilon': 'Privacy_Epsilon',\n",
        "        'wall_time': 'P95_Latency'\n",
        "    })\n",
        "\n",
        "    df['is_pareto_optimal'] = (\n",
        "        (df['Reward'] > goals['reward']) &\n",
        "        (df['Privacy_Epsilon'] < goals['privacy']) &\n",
        "        (df['P95_Latency'] < goals['latency'])\n",
        "    )\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(24, 7))\n",
        "    fig.suptitle('Pareto Optimization Frontier Analysis', fontsize=20, y=1.03)\n",
        "\n",
        "    # åœ– 1: Reward vs. Privacy\n",
        "    sns.scatterplot(data=df, x='Privacy_Epsilon', y='Reward', hue='mode', style='is_pareto_optimal',\n",
        "                    s=200, alpha=0.8, palette=PALETTE, hue_order=HUE_ORDER, ax=axes[0])\n",
        "    axes[0].axhline(goals['reward'], ls='--', color='green', label=f\"Reward Target (> {goals['reward']})\")\n",
        "    axes[0].axvline(goals['privacy'], ls='--', color='red', label=f\"Privacy Budget (< {goals['privacy']} Îµ)\")\n",
        "    axes[0].set_title('Performance vs. Privacy', fontsize=16)\n",
        "    axes[0].set_xlabel('Total Privacy Budget Îµ (Lower is Better)', fontsize=12)\n",
        "    axes[0].set_ylabel('Final Average Reward (Higher is Better)', fontsize=12)\n",
        "    axes[0].legend(title='Optimal', loc='lower right')\n",
        "\n",
        "    # åœ– 2: Reward vs. Latency\n",
        "    sns.scatterplot(data=df, x='P95_Latency', y='Reward', hue='mode', style='is_pareto_optimal',\n",
        "                    s=200, alpha=0.8, palette=PALETTE, hue_order=HUE_ORDER, ax=axes[1])\n",
        "    axes[1].axhline(goals['reward'], ls='--', color='green')\n",
        "    axes[1].axvline(goals['latency'], ls='--', color='orange', label=f\"Latency Target (< {goals['latency']}s P95)\")\n",
        "    axes[1].set_title('Performance vs. Latency', fontsize=16)\n",
        "    axes[1].set_xlabel('P95 End-to-End Latency (s) (Lower is Better)', fontsize=12)\n",
        "    axes[1].set_ylabel('')\n",
        "    axes[1].legend(title='Optimal', loc='lower right')\n",
        "\n",
        "    # åœ– 3: Latency vs. Privacy\n",
        "    sns.scatterplot(data=df, x='Privacy_Epsilon', y='P95_Latency', hue='mode', style='is_pareto_optimal',\n",
        "                    s=200, alpha=0.8, palette=PALETTE, hue_order=HUE_ORDER, ax=axes[2])\n",
        "    axes[2].axhline(goals['latency'], ls='--', color='orange')\n",
        "    axes[2].axvline(goals['privacy'], ls='--', color='red')\n",
        "    axes[2].set_title('Latency vs. Privacy', fontsize=16)\n",
        "    axes[2].set_xlabel('Total Privacy Budget Îµ (Lower is Better)', fontsize=12)\n",
        "    axes[2].set_ylabel('P95 End-to-End Latency (s) (Lower is Better)', fontsize=12)\n",
        "    axes[2].legend(title='Optimal', loc='upper left')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.savefig(os.path.join(output_dir, 'pareto_frontier_analysis.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_training_performance(history_df, output_dir, reward_goal):\n",
        "    if history_df.empty: return\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.lineplot(data=history_df, x='round', y='avg_reward', hue='mode',\n",
        "                 hue_order=HUE_ORDER, palette=PALETTE,\n",
        "                 errorbar='sd', linewidth=2.5, marker='o', markersize=6)\n",
        "    plt.axhline(y=reward_goal, color='green', linestyle='--', linewidth=2, label=f'Reward Target ({reward_goal})')\n",
        "    plt.title('Federated RL Training Performance (with Std Dev)', fontsize=18)\n",
        "    plt.xlabel('Communication Round', fontsize=14)\n",
        "    plt.ylabel('Average Episodic Reward', fontsize=14)\n",
        "    plt.legend(title='FL Mode', loc='lower right', fontsize=12)\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'training_performance.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_latency_breakdown(latency_df, output_dir, latency_goal):\n",
        "    if latency_df.empty: return\n",
        "    components = ['client_compute_time', 'server_aggregation_time', 'client_to_server_comm', 'server_to_client_comm']\n",
        "    if not all(c in latency_df.columns for c in components):\n",
        "        logger.warning(\"å»¶é²æ•¸æ“šç¼ºå°‘çµ„ä»¶æ¬„ä½ï¼Œç„¡æ³•ç”Ÿæˆåˆ†è§£åœ–ã€‚\")\n",
        "        return\n",
        "    latency_df['communication_time'] = latency_df['client_to_server_comm'] + latency_df['server_to_client_comm']\n",
        "    avg_latency = latency_df.groupby('mode')[['client_compute_time', 'server_aggregation_time', 'communication_time']].mean()\n",
        "    avg_latency = avg_latency.reindex(HUE_ORDER).dropna()\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "    fig.suptitle('System Latency Analysis', fontsize=20)\n",
        "    avg_latency.plot(kind='bar', stacked=True, ax=axes[0], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "    axes[0].set_title('Average Latency Breakdown per Round', fontsize=16)\n",
        "    axes[0].set_ylabel('Average Time (s)', fontsize=12)\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    axes[0].legend(['Client Compute', 'Server Aggregation', 'Communication'], fontsize=10)\n",
        "    p95_latency = latency_df.groupby(['mode', 'seed'])['wall_time'].quantile(0.95).reset_index()\n",
        "    sns.boxplot(data=p95_latency, x='mode', y='wall_time', order=HUE_ORDER, palette=PALETTE, ax=axes[1])\n",
        "    axes[1].axhline(y=latency_goal, color='r', linestyle='--', label=f'Latency Target ({latency_goal}s)')\n",
        "    axes[1].set_title('P95 End-to-End Latency Distribution', fontsize=16)\n",
        "    axes[1].set_ylabel('P95 Latency (s)', fontsize=12)\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    axes[1].legend()\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.savefig(os.path.join(output_dir, 'latency_breakdown_and_distribution.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_fairness_and_personalization(eval_df, output_dir, reward_goal):\n",
        "    if eval_df.empty: return\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "    fig.suptitle('Client Fairness and Personalization Benefit', fontsize=20)\n",
        "    sns.violinplot(data=eval_df, x='mode', y='reward_pfl_finetuned', order=HUE_ORDER, palette=PALETTE, ax=axes[0], inner='quartile')\n",
        "    axes[0].axhline(y=reward_goal, color='g', linestyle='--', label=f'Reward Target ({reward_goal})')\n",
        "    axes[0].set_title('Client Reward Distribution (Fairness)', fontsize=16)\n",
        "    axes[0].set_xlabel('FL Mode', fontsize=12)\n",
        "    axes[0].set_ylabel('Final PFL Reward', fontsize=12)\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    axes[0].legend()\n",
        "    eval_agg = eval_df.groupby('mode')[['reward_global', 'reward_pfl_finetuned']].mean().reset_index()\n",
        "    eval_melted = eval_agg.melt(id_vars='mode', value_vars=['reward_global', 'reward_pfl_finetuned'],\n",
        "                                var_name='ModelType', value_name='AverageReward')\n",
        "    sns.barplot(data=eval_melted, x='mode', y='AverageReward', hue='ModelType',\n",
        "                order=HUE_ORDER, palette={'reward_global': 'skyblue', 'reward_pfl_finetuned': 'darkorange'}, ax=axes[1])\n",
        "    axes[1].set_title('Personalization Benefit (Global vs. PFL)', fontsize=16)\n",
        "    axes[1].set_xlabel('FL Mode', fontsize=12)\n",
        "    axes[1].set_ylabel('Average Final Reward', fontsize=12)\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.savefig(os.path.join(output_dir, 'fairness_and_personalization.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# --- 3. é©—è­‰èˆ‡ç¸½çµ ---\n",
        "def final_summary_and_validation(results_dict):\n",
        "    \"\"\"\n",
        "    ç”Ÿæˆæœ€çµ‚çš„æ‘˜è¦å ±å‘Šå’Œè‡ªå‹•åŒ–é©—æ”¶æª¢æŸ¥ã€‚\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ‰ æœ€çµ‚å¯¦é©—çµæœæ‘˜è¦èˆ‡é©—æ”¶\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    summary_data = []\n",
        "    pareto_goals = {'reward': 165, 'privacy': 4.0, 'latency': 40.0}\n",
        "\n",
        "    for mode in HUE_ORDER:\n",
        "        mode_eval = results_dict['eval'][results_dict['eval']['mode'] == mode]\n",
        "        mode_privacy = results_dict['privacy'][results_dict['privacy']['mode'] == mode]\n",
        "        mode_latency = results_dict['latency'][results_dict['latency']['mode'] == mode]\n",
        "\n",
        "        if any([df.empty for df in [mode_eval, mode_privacy, mode_latency]]):\n",
        "            continue\n",
        "\n",
        "        avg_reward = mode_eval['reward_pfl_finetuned'].mean()\n",
        "        max_epsilon = mode_privacy.groupby('seed')['cumulative_epsilon'].max().mean()\n",
        "        p95_latency = mode_latency.groupby('seed')['wall_time'].quantile(0.95).mean()\n",
        "\n",
        "        reward_ok = avg_reward > pareto_goals['reward']\n",
        "        privacy_ok = max_epsilon < pareto_goals['privacy']\n",
        "        latency_ok = p95_latency < pareto_goals['latency']\n",
        "\n",
        "        summary_data.append({\n",
        "            'Mode': mode,\n",
        "            'Avg Reward': f\"{avg_reward:.2f}\",\n",
        "            'Privacy Îµ': f\"{max_epsilon:.2f}\",\n",
        "            'P95 Latency (s)': f\"{p95_latency:.2f}\",\n",
        "            'Reward OK': 'âœ…' if reward_ok else 'âŒ',\n",
        "            'Privacy OK': 'âœ…' if privacy_ok else 'âŒ',\n",
        "            'Latency OK': 'âœ…' if latency_ok else 'âŒ',\n",
        "            'Pareto Optimal': 'ğŸ†' if all([reward_ok, privacy_ok, latency_ok]) else ''\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(summary_df.to_string())\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SUCCESS: åˆ†æèˆ‡è¦–è¦ºåŒ–æµç¨‹åŸ·è¡Œå®Œç•¢ã€‚\")\n",
        "    print(f\"ğŸ“ æ‰€æœ‰åœ–è¡¨å·²ä¿å­˜è‡³: {FIGURES_OUTPUT_DIR}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# --- ä¸»åŸ·è¡Œæµç¨‹ ---\n",
        "try:\n",
        "    # è¼‰å…¥æ‰€æœ‰æ•¸æ“šä¸¦å‘½åç‚º results_dict ä¾†å¼·èª¿å…¶çµæ§‹\n",
        "    results_dict = load_all_results_as_dict(BASE_OUTPUT_DIR)\n",
        "\n",
        "    # *** éŒ¯èª¤ä¿®æ­£ ***ï¼š\n",
        "    # æª¢æŸ¥ results_dict.values() ä¸­çš„æ‰€æœ‰ DataFrame æ˜¯å¦ç‚ºç©º\n",
        "    # é€™æ¨£å¯ä»¥æ­£ç¢ºè™•ç†å­—å…¸ï¼Œé¿å… 'tuple' object has no attribute 'values' éŒ¯èª¤\n",
        "    if not results_dict or all(df.empty for df in results_dict.values()):\n",
        "        raise FileNotFoundError(\"åœ¨æŒ‡å®šçš„è¼¸å‡ºç›®éŒ„ä¸‹æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ CSV çµæœæ–‡ä»¶ã€‚\")\n",
        "\n",
        "    # å®šç¾© Pareto ç›®æ¨™\n",
        "    pareto_goals = {'reward': 165, 'privacy': 4.0, 'latency': 40.0}\n",
        "\n",
        "    # åŸ·è¡Œè¦–è¦ºåŒ–\n",
        "    print(\"ğŸ“Š æ­£åœ¨ç”Ÿæˆ Pareto å‰æ²¿åˆ†æåœ–...\")\n",
        "    plot_pareto_frontier_analysis(results_dict['eval'], results_dict['privacy'], results_dict['latency'], FIGURES_OUTPUT_DIR, pareto_goals)\n",
        "\n",
        "    print(\"ğŸ“ˆ æ­£åœ¨ç”Ÿæˆè¨“ç·´æ•ˆèƒ½è¶¨å‹¢åœ–...\")\n",
        "    plot_training_performance(results_dict['history'], FIGURES_OUTPUT_DIR, pareto_goals['reward'])\n",
        "\n",
        "    print(\"â±ï¸ æ­£åœ¨ç”Ÿæˆå»¶é²åˆ†æåœ–...\")\n",
        "    plot_latency_breakdown(results_dict['latency'], FIGURES_OUTPUT_DIR, pareto_goals['latency'])\n",
        "\n",
        "    print(\"ğŸ¤ æ­£åœ¨ç”Ÿæˆå…¬å¹³æ€§èˆ‡å€‹äººåŒ–æ•ˆç›Šåœ–...\")\n",
        "    plot_fairness_and_personalization(results_dict['eval'], FIGURES_OUTPUT_DIR, pareto_goals['reward'])\n",
        "\n",
        "    # è¼¸å‡ºæœ€çµ‚ç¸½çµ\n",
        "    final_summary_and_validation(results_dict)\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    logger.error(f\"åŸ·è¡Œå¤±æ•—: {e}\")\n",
        "    print(f\"\\nâŒ éŒ¯èª¤: {e}ã€‚è«‹æª¢æŸ¥ BASE_OUTPUT_DIR ('{BASE_OUTPUT_DIR}') æ˜¯å¦æ­£ç¢ºï¼Œä»¥åŠ Cell 8 çš„å¯¦é©—æ˜¯å¦å·²æˆåŠŸåŸ·è¡Œä¸¦ç”¢ç”Ÿäº† .csv çµæœæ–‡ä»¶ã€‚\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"åˆ†æéç¨‹ä¸­ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}\")\n",
        "    traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ecd8a7f8456e4c35b5df0a9af5723473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9dfcef1cb9bf477a909acb4dbd32b876",
              "IPY_MODEL_9b86bbe8f7a147fa8efceae43a87cace",
              "IPY_MODEL_c51e06324eeb4961bea60f024ce51caa"
            ],
            "layout": "IPY_MODEL_2b7d7255fcbb4d79991aceaa6725f029"
          }
        },
        "9dfcef1cb9bf477a909acb4dbd32b876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29aaee57e48a4e45997aa13c19173434",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c387532806ac4822b14426fc28fa224c",
            "value": "ClusteredFLâ€‡Training:â€‡100%"
          }
        },
        "9b86bbe8f7a147fa8efceae43a87cace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c0b889c882940a7a89480155af8fef2",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed458669f4794e1fa569d339786c00b8",
            "value": 5
          }
        },
        "c51e06324eeb4961bea60f024ce51caa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d73e469a9f1b4a2d9bfb52857c55f674",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ef0f31b3d8d44eac9d40840cb446b896",
            "value": "â€‡5/5â€‡[06:03&lt;00:00,â€‡92.13s/it,â€‡reward=164.95,â€‡loss=56597422.7409]"
          }
        },
        "2b7d7255fcbb4d79991aceaa6725f029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29aaee57e48a4e45997aa13c19173434": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c387532806ac4822b14426fc28fa224c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c0b889c882940a7a89480155af8fef2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed458669f4794e1fa569d339786c00b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d73e469a9f1b4a2d9bfb52857c55f674": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef0f31b3d8d44eac9d40840cb446b896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "557ac0fd04a74f1685f314bed8c9e51f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c40456ba6dcd4547893223fbe4f1bb57",
              "IPY_MODEL_2d1ce7facd044f7783bed3e1fa32f771",
              "IPY_MODEL_6b6b42aaa43c43648f9e19adabb106e9"
            ],
            "layout": "IPY_MODEL_dac32e58447844c5a1e8edc7eb8267c1"
          }
        },
        "c40456ba6dcd4547893223fbe4f1bb57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40871663e605410981e68717c34f858c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_76db7a64f9344d9c964010767b1155ac",
            "value": "æœ€çµ‚è©•ä¼°:â€‡100%"
          }
        },
        "2d1ce7facd044f7783bed3e1fa32f771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cd1140d0ba14c4182f2a66f283dbd83",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe74e6ad794e4389b360f2416c4d7e3c",
            "value": 10
          }
        },
        "6b6b42aaa43c43648f9e19adabb106e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd99acf48aed4827851998254dbe3854",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_959c2fdc4b4f4f30acbe523955f32394",
            "value": "â€‡10/10â€‡[10:43&lt;00:00,â€‡65.99s/it]"
          }
        },
        "dac32e58447844c5a1e8edc7eb8267c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40871663e605410981e68717c34f858c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76db7a64f9344d9c964010767b1155ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cd1140d0ba14c4182f2a66f283dbd83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe74e6ad794e4389b360f2416c4d7e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd99acf48aed4827851998254dbe3854": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "959c2fdc4b4f4f30acbe523955f32394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0bf8e89fa08f47eaaa8a9bde801ba958": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a1d2e58a88e40599782c74db5eb4311",
              "IPY_MODEL_66d5d9e6790e49efa4b905bb13afd4c2",
              "IPY_MODEL_e1fababc1fdd425fa780d5d2001f9266"
            ],
            "layout": "IPY_MODEL_bb7a30d997d841d1ab73a7bfeabf5ac6"
          }
        },
        "2a1d2e58a88e40599782c74db5eb4311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5943860aa8a470eb1576ed0a0154231",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a1df46e11b174ad4be8bfa6cd2727837",
            "value": "FedProxâ€‡Training:â€‡â€‡80%"
          }
        },
        "66d5d9e6790e49efa4b905bb13afd4c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_638d012626564f66b157b8a7b9bd6556",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9726ca0064647d8a334f77644fe80d4",
            "value": 4
          }
        },
        "e1fababc1fdd425fa780d5d2001f9266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d42d62f5ecf4e1f95bb269a7f0b8164",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dcaf1b36f7f1474f9fc22a7c171601bb",
            "value": "â€‡4/5â€‡[03:24&lt;00:59,â€‡59.97s/it,â€‡reward=160.55,â€‡loss=45489740.2138]"
          }
        },
        "bb7a30d997d841d1ab73a7bfeabf5ac6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5943860aa8a470eb1576ed0a0154231": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1df46e11b174ad4be8bfa6cd2727837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "638d012626564f66b157b8a7b9bd6556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9726ca0064647d8a334f77644fe80d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d42d62f5ecf4e1f95bb269a7f0b8164": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcaf1b36f7f1474f9fc22a7c171601bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "360e07cb76354723abe6d1675a5e50b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6dbc3bfb108e4afd84fe2b979dee838a",
              "IPY_MODEL_dbe2602ffdb2419b86ea5ca44281bfcf",
              "IPY_MODEL_3e59a9c2ba944826ac7093a55636fb1e"
            ],
            "layout": "IPY_MODEL_e52325c596e8445a82b7d5e77a5e3963"
          }
        },
        "6dbc3bfb108e4afd84fe2b979dee838a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56b3251d52e8468ea61f25e9d409ca98",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_aab2787947f04c62b1709cb623a7730f",
            "value": "æœ€çµ‚è©•ä¼°:â€‡100%"
          }
        },
        "dbe2602ffdb2419b86ea5ca44281bfcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38f0f12acfd9497e90c0782e0b16b95a",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54427b901439429c87acf9732f9e39ba",
            "value": 10
          }
        },
        "3e59a9c2ba944826ac7093a55636fb1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc50fec87b4f4afbbd5acf9d1494354c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_04f4521bcb2d488f99f2950193a61341",
            "value": "â€‡10/10â€‡[10:48&lt;00:00,â€‡66.76s/it]"
          }
        },
        "e52325c596e8445a82b7d5e77a5e3963": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56b3251d52e8468ea61f25e9d409ca98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aab2787947f04c62b1709cb623a7730f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38f0f12acfd9497e90c0782e0b16b95a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54427b901439429c87acf9732f9e39ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc50fec87b4f4afbbd5acf9d1494354c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04f4521bcb2d488f99f2950193a61341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54e3f5c77e3b4d848bc7d032ea4eb83e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f06c7a6c9664d30ae2d24d677dcf92f",
              "IPY_MODEL_949195bc442847a4aee1127841e90840",
              "IPY_MODEL_8f0caef3d10b47c184350af8da00ea2d"
            ],
            "layout": "IPY_MODEL_58264721e6794bf69b9a364436eb0d49"
          }
        },
        "9f06c7a6c9664d30ae2d24d677dcf92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47dd5f5856a247588aff1b57ac524f45",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f370dd06597245db8ec52b632f04dc6e",
            "value": "FedAvgâ€‡Training:â€‡â€‡80%"
          }
        },
        "949195bc442847a4aee1127841e90840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2da9c207cce3490baf873d1057aea9d8",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf54d44f3c4449d2949d9b919d72245d",
            "value": 4
          }
        },
        "8f0caef3d10b47c184350af8da00ea2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7b3770ddd504f8781958bee42fa689f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fb8f33b04b2d4d35b8097398416980e4",
            "value": "â€‡4/5â€‡[03:14&lt;00:57,â€‡57.03s/it,â€‡reward=160.55,â€‡loss=45505907.5371]"
          }
        },
        "58264721e6794bf69b9a364436eb0d49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47dd5f5856a247588aff1b57ac524f45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f370dd06597245db8ec52b632f04dc6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2da9c207cce3490baf873d1057aea9d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf54d44f3c4449d2949d9b919d72245d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7b3770ddd504f8781958bee42fa689f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb8f33b04b2d4d35b8097398416980e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f5b5014fa934425aafbce2b61300ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e747bdf56b6648e19c29b8edd23b0a3e",
              "IPY_MODEL_be5d555a430548179c0fff086edd9851",
              "IPY_MODEL_656ee897650f41b3afa90f974d426faf"
            ],
            "layout": "IPY_MODEL_bd1f8bbf241942cd8fa1ec6983949c9f"
          }
        },
        "e747bdf56b6648e19c29b8edd23b0a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a341d94632c4de289e6f7637c0fff19",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_30c4303ff73c46b9bddedfe95ad6dd24",
            "value": "æœ€çµ‚è©•ä¼°:â€‡100%"
          }
        },
        "be5d555a430548179c0fff086edd9851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6c073e64fdf41d8bb73d70ba83b7712",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5d983736bcd4c39b8c4d496e7fd1ba4",
            "value": 10
          }
        },
        "656ee897650f41b3afa90f974d426faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ac69829d08a46ed98b3d93525f41b19",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8d4cb6b47d0041248b799da32c9da160",
            "value": "â€‡10/10â€‡[10:47&lt;00:00,â€‡66.23s/it]"
          }
        },
        "bd1f8bbf241942cd8fa1ec6983949c9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a341d94632c4de289e6f7637c0fff19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30c4303ff73c46b9bddedfe95ad6dd24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6c073e64fdf41d8bb73d70ba83b7712": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5d983736bcd4c39b8c4d496e7fd1ba4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ac69829d08a46ed98b3d93525f41b19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d4cb6b47d0041248b799da32c9da160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fbb01d1a665496c90059b6eacb47968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_191a3cf032644c749a94149c3f290c83",
              "IPY_MODEL_c5164b185b6e423fbaedd9ea79acd8aa",
              "IPY_MODEL_c1ae23b61d4e432fba17a77d164f1fb1"
            ],
            "layout": "IPY_MODEL_9d52cc4eb4564c8cb4d77802c7e4ac75"
          }
        },
        "191a3cf032644c749a94149c3f290c83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_781805d4fab242278684066c2ce35202",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7199d542a1ca44f19ddb80921861f7dc",
            "value": "CQLâ€‡Training:â€‡â€‡60%"
          }
        },
        "c5164b185b6e423fbaedd9ea79acd8aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1588fa5a55f94c26841b6835a98199d3",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f620c927583f403389d4f5464e2f6246",
            "value": 3
          }
        },
        "c1ae23b61d4e432fba17a77d164f1fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_945cb2b898ba433a8309805168a834c3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_68eb3a9e25e548fe81b54a4c35befaec",
            "value": "â€‡3/5â€‡[01:45&lt;01:17,â€‡38.83s/it,â€‡reward=162.97,â€‡loss=0.0000]"
          }
        },
        "9d52cc4eb4564c8cb4d77802c7e4ac75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "781805d4fab242278684066c2ce35202": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7199d542a1ca44f19ddb80921861f7dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1588fa5a55f94c26841b6835a98199d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f620c927583f403389d4f5464e2f6246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "945cb2b898ba433a8309805168a834c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68eb3a9e25e548fe81b54a4c35befaec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}